{"traceEvents": [{"ph": "M", "pid": 30327, "tid": 30327, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 30327, "tid": 30327, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 30327, "tid": 30327, "ts": 45578246651.666, "ph": "X", "cat": "fee", "dur": 0.492, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578246653.959, "ph": "X", "cat": "fee", "dur": 1.789, "name": "Runner.__init__ (/usr/lib/python3.13/asyncio/runners.py:48)"}, {"pid": 30327, "tid": 30327, "ts": 45578246663.459, "ph": "X", "cat": "fee", "dur": 0.462, "name": "str.rpartition"}, {"pid": 30327, "tid": 30327, "ts": 45578246663.145, "ph": "X", "cat": "fee", "dur": 1.139, "name": "ModuleSpec.parent (<frozen importlib._bootstrap>:645)"}, {"pid": 30327, "tid": 30327, "ts": 45578246666.908, "ph": "X", "cat": "fee", "dur": 0.164, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246667.406, "ph": "X", "cat": "fee", "dur": 0.262, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578246666.383, "ph": "X", "cat": "fee", "dur": 1.469, "name": "_handle_fromlist (<frozen importlib._bootstrap>:1390)"}, {"pid": 30327, "tid": 30327, "ts": 45578246672.241, "ph": "X", "cat": "fee", "dur": 1.931, "name": "BaseDefaultEventLoopPolicy.__init__ (/usr/lib/python3.13/asyncio/events.py:685)"}, {"pid": 30327, "tid": 30327, "ts": 45578246669.065, "ph": "X", "cat": "fee", "dur": 5.328, "name": "_UnixDefaultEventLoopPolicy.__init__ (/usr/lib/python3.13/asyncio/unix_events.py:1481)"}, {"pid": 30327, "tid": 30327, "ts": 45578246674.654, "ph": "X", "cat": "fee", "dur": 0.28, "name": "_thread.lock.__exit__"}, {"pid": 30327, "tid": 30327, "ts": 45578246660.418, "ph": "X", "cat": "fee", "dur": 14.671, "name": "_init_event_loop_policy (/usr/lib/python3.13/asyncio/events.py:789)"}, {"pid": 30327, "tid": 30327, "ts": 45578246659.748, "ph": "X", "cat": "fee", "dur": 15.474, "name": "get_event_loop_policy (/usr/lib/python3.13/asyncio/events.py:797)"}, {"pid": 30327, "tid": 30327, "ts": 45578246681.778, "ph": "X", "cat": "fee", "dur": 9.217, "name": "time.get_clock_info"}, {"pid": 30327, "tid": 30327, "ts": 45578246698.142, "ph": "X", "cat": "fee", "dur": 0.121, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246698.607, "ph": "X", "cat": "fee", "dur": 0.426, "name": "str.encode"}, {"pid": 30327, "tid": 30327, "ts": 45578246697.75, "ph": "X", "cat": "fee", "dur": 1.466, "name": "_createenviron.<locals>.encode (<frozen os>:793)"}, {"pid": 30327, "tid": 30327, "ts": 45578246696.987, "ph": "X", "cat": "fee", "dur": 4.087, "name": "_Environ.__getitem__ (<frozen os>:711)"}, {"pid": 30327, "tid": 30327, "ts": 45578246696.316, "ph": "X", "cat": "fee", "dur": 5.968, "name": "Mapping.get (<frozen _collections_abc>:808)"}, {"pid": 30327, "tid": 30327, "ts": 45578246692.964, "ph": "X", "cat": "fee", "dur": 9.564, "name": "_is_debug_mode (/usr/lib/python3.13/asyncio/coroutines.py:10)"}, {"pid": 30327, "tid": 30327, "ts": 45578246703.716, "ph": "X", "cat": "fee", "dur": 0.238, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578246703.136, "ph": "X", "cat": "fee", "dur": 0.932, "name": "BaseEventLoop.set_debug (/usr/lib/python3.13/asyncio/base_events.py:2048)"}, {"pid": 30327, "tid": 30327, "ts": 45578246706.26, "ph": "X", "cat": "fee", "dur": 1.585, "name": "WeakSet.__init__ (/usr/lib/python3.13/_weakrefset.py:37)"}, {"pid": 30327, "tid": 30327, "ts": 45578246678.757, "ph": "X", "cat": "fee", "dur": 29.845, "name": "BaseEventLoop.__init__ (/usr/lib/python3.13/asyncio/base_events.py:420)"}, {"pid": 30327, "tid": 30327, "ts": 45578246713.318, "ph": "X", "cat": "fee", "dur": 0.447, "name": "_SelectorMapping.__init__ (/usr/lib/python3.13/selectors.py:63)"}, {"pid": 30327, "tid": 30327, "ts": 45578246712.002, "ph": "X", "cat": "fee", "dur": 2.171, "name": "_BaseSelectorImpl.__init__ (/usr/lib/python3.13/selectors.py:213)"}, {"pid": 30327, "tid": 30327, "ts": 45578246710.973, "ph": "X", "cat": "fee", "dur": 7.6, "name": "_PollLikeSelector.__init__ (/usr/lib/python3.13/selectors.py:336)"}, {"pid": 30327, "tid": 30327, "ts": 45578246725.19, "ph": "X", "cat": "fee", "dur": 0.45, "name": "Manager.disable (/usr/lib/python3.13/logging/__init__.py:1353)"}, {"pid": 30327, "tid": 30327, "ts": 45578246726.452, "ph": "X", "cat": "fee", "dur": 1.226, "name": "Logger.getEffectiveLevel (/usr/lib/python3.13/logging/__init__.py:1750)"}, {"pid": 30327, "tid": 30327, "ts": 45578246728.13, "ph": "X", "cat": "fee", "dur": 0.319, "name": "_thread.RLock.__exit__"}, {"pid": 30327, "tid": 30327, "ts": 45578246721.915, "ph": "X", "cat": "fee", "dur": 6.824, "name": "Logger.isEnabledFor (/usr/lib/python3.13/logging/__init__.py:1764)"}, {"pid": 30327, "tid": 30327, "ts": 45578246720.515, "ph": "X", "cat": "fee", "dur": 8.424, "name": "Logger.debug (/usr/lib/python3.13/logging/__init__.py:1497)"}, {"pid": 30327, "tid": 30327, "ts": 45578246732.738, "ph": "X", "cat": "fee", "dur": 6.509, "name": "_socket.socketpair"}, {"pid": 30327, "tid": 30327, "ts": 45578246740.796, "ph": "X", "cat": "fee", "dur": 0.219, "name": "_socket.socket.detach"}, {"pid": 30327, "tid": 30327, "ts": 45578246742.245, "ph": "X", "cat": "fee", "dur": 4.355, "name": "socket.__init__ (/usr/lib/python3.13/socket.py:221)"}, {"pid": 30327, "tid": 30327, "ts": 45578246747.049, "ph": "X", "cat": "fee", "dur": 0.074, "name": "_socket.socket.detach"}, {"pid": 30327, "tid": 30327, "ts": 45578246747.454, "ph": "X", "cat": "fee", "dur": 1.527, "name": "socket.__init__ (/usr/lib/python3.13/socket.py:221)"}, {"pid": 30327, "tid": 30327, "ts": 45578246731.253, "ph": "X", "cat": "fee", "dur": 17.931, "name": "socketpair (/usr/lib/python3.13/socket.py:653)"}, {"pid": 30327, "tid": 30327, "ts": 45578246750.088, "ph": "X", "cat": "fee", "dur": 0.799, "name": "socket.setblocking"}, {"pid": 30327, "tid": 30327, "ts": 45578246751.069, "ph": "X", "cat": "fee", "dur": 0.213, "name": "socket.setblocking"}, {"pid": 30327, "tid": 30327, "ts": 45578246752.053, "ph": "X", "cat": "fee", "dur": 0.221, "name": "socket.fileno"}, {"pid": 30327, "tid": 30327, "ts": 45578246754.086, "ph": "X", "cat": "fee", "dur": 0.189, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578246756.069, "ph": "X", "cat": "fee", "dur": 0.629, "name": "_contextvars.copy_context"}, {"pid": 30327, "tid": 30327, "ts": 45578246758.543, "ph": "X", "cat": "fee", "dur": 0.134, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578246755.627, "ph": "X", "cat": "fee", "dur": 3.231, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578246759.621, "ph": "X", "cat": "fee", "dur": 0.145, "name": "_BaseSelectorImpl.get_map (/usr/lib/python3.13/selectors.py:276)"}, {"pid": 30327, "tid": 30327, "ts": 45578246762.381, "ph": "X", "cat": "fee", "dur": 0.13, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246762.119, "ph": "X", "cat": "fee", "dur": 0.622, "name": "_fileobj_to_fd (/usr/lib/python3.13/selectors.py:21)"}, {"pid": 30327, "tid": 30327, "ts": 45578246761.272, "ph": "X", "cat": "fee", "dur": 2.964, "name": "_BaseSelectorImpl._fileobj_lookup (/usr/lib/python3.13/selectors.py:219)"}, {"pid": 30327, "tid": 30327, "ts": 45578246764.574, "ph": "X", "cat": "fee", "dur": 0.213, "name": "dict.get"}, {"pid": 30327, "tid": 30327, "ts": 45578246760.382, "ph": "X", "cat": "fee", "dur": 4.553, "name": "_SelectorMapping.get (/usr/lib/python3.13/selectors.py:69)"}, {"pid": 30327, "tid": 30327, "ts": 45578246769.302, "ph": "X", "cat": "fee", "dur": 0.048, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246768.973, "ph": "X", "cat": "fee", "dur": 0.604, "name": "_fileobj_to_fd (/usr/lib/python3.13/selectors.py:21)"}, {"pid": 30327, "tid": 30327, "ts": 45578246768.775, "ph": "X", "cat": "fee", "dur": 0.875, "name": "_BaseSelectorImpl._fileobj_lookup (/usr/lib/python3.13/selectors.py:219)"}, {"pid": 30327, "tid": 30327, "ts": 45578246770.935, "ph": "X", "cat": "fee", "dur": 0.908, "name": "type.__new__"}, {"pid": 30327, "tid": 30327, "ts": 45578246770.591, "ph": "X", "cat": "fee", "dur": 1.39, "name": "<lambda> (<string>:1)"}, {"pid": 30327, "tid": 30327, "ts": 45578246767.853, "ph": "X", "cat": "fee", "dur": 5.092, "name": "_BaseSelectorImpl.register (/usr/lib/python3.13/selectors.py:238)"}, {"pid": 30327, "tid": 30327, "ts": 45578246774.124, "ph": "X", "cat": "fee", "dur": 2.832, "name": "select.epoll.register"}, {"pid": 30327, "tid": 30327, "ts": 45578246766.387, "ph": "X", "cat": "fee", "dur": 10.697, "name": "_PollLikeSelector.register (/usr/lib/python3.13/selectors.py:340)"}, {"pid": 30327, "tid": 30327, "ts": 45578246753.595, "ph": "X", "cat": "fee", "dur": 23.676, "name": "BaseSelectorEventLoop._add_reader (/usr/lib/python3.13/asyncio/selector_events.py:274)"}, {"pid": 30327, "tid": 30327, "ts": 45578246729.853, "ph": "X", "cat": "fee", "dur": 47.57, "name": "BaseSelectorEventLoop._make_self_pipe (/usr/lib/python3.13/asyncio/selector_events.py:118)"}, {"pid": 30327, "tid": 30327, "ts": 45578246782.013, "ph": "X", "cat": "fee", "dur": 0.261, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578246783.183, "ph": "X", "cat": "fee", "dur": 0.158, "name": "dict.items"}, {"pid": 30327, "tid": 30327, "ts": 45578246783.798, "ph": "X", "cat": "fee", "dur": 0.108, "name": "dict.items"}, {"pid": 30327, "tid": 30327, "ts": 45578246781.695, "ph": "X", "cat": "fee", "dur": 2.391, "name": "WeakValueDictionary.update (/usr/lib/python3.13/weakref.py:289)"}, {"pid": 30327, "tid": 30327, "ts": 45578246778.775, "ph": "X", "cat": "fee", "dur": 5.629, "name": "WeakValueDictionary.__init__ (/usr/lib/python3.13/weakref.py:104)"}, {"pid": 30327, "tid": 30327, "ts": 45578246677.633, "ph": "X", "cat": "fee", "dur": 107.192, "name": "BaseSelectorEventLoop.__init__ (/usr/lib/python3.13/asyncio/selector_events.py:59)"}, {"pid": 30327, "tid": 30327, "ts": 45578246676.638, "ph": "X", "cat": "fee", "dur": 108.673, "name": "_UnixSelectorEventLoop.__init__ (/usr/lib/python3.13/asyncio/unix_events.py:64)"}, {"pid": 30327, "tid": 30327, "ts": 45578246675.808, "ph": "X", "cat": "fee", "dur": 109.726, "name": "BaseDefaultEventLoopPolicy.new_event_loop (/usr/lib/python3.13/asyncio/events.py:728)"}, {"pid": 30327, "tid": 30327, "ts": 45578246659.149, "ph": "X", "cat": "fee", "dur": 126.482, "name": "new_event_loop (/usr/lib/python3.13/asyncio/events.py:835)"}, {"pid": 30327, "tid": 30327, "ts": 45578246787.389, "ph": "X", "cat": "fee", "dur": 0.193, "name": "get_event_loop_policy (/usr/lib/python3.13/asyncio/events.py:797)"}, {"pid": 30327, "tid": 30327, "ts": 45578246790.525, "ph": "X", "cat": "fee", "dur": 0.154, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246789.267, "ph": "X", "cat": "fee", "dur": 1.744, "name": "BaseDefaultEventLoopPolicy.set_event_loop (/usr/lib/python3.13/asyncio/events.py:721)"}, {"pid": 30327, "tid": 30327, "ts": 45578246788.318, "ph": "X", "cat": "fee", "dur": 2.836, "name": "_UnixDefaultEventLoopPolicy.set_event_loop (/usr/lib/python3.13/asyncio/unix_events.py:1493)"}, {"pid": 30327, "tid": 30327, "ts": 45578246786.683, "ph": "X", "cat": "fee", "dur": 4.588, "name": "set_event_loop (/usr/lib/python3.13/asyncio/events.py:830)"}, {"pid": 30327, "tid": 30327, "ts": 45578246791.624, "ph": "X", "cat": "fee", "dur": 0.229, "name": "_contextvars.copy_context"}, {"pid": 30327, "tid": 30327, "ts": 45578246657.957, "ph": "X", "cat": "fee", "dur": 134.369, "name": "Runner._lazy_init (/usr/lib/python3.13/asyncio/runners.py:131)"}, {"pid": 30327, "tid": 30327, "ts": 45578246656.905, "ph": "X", "cat": "fee", "dur": 135.524, "name": "Runner.__enter__ (/usr/lib/python3.13/asyncio/runners.py:57)"}, {"pid": 30327, "tid": 30327, "ts": 45578246796.291, "ph": "X", "cat": "fee", "dur": 0.085, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246796.534, "ph": "X", "cat": "fee", "dur": 0.214, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246797.216, "ph": "X", "cat": "fee", "dur": 0.143, "name": "set.add"}, {"pid": 30327, "tid": 30327, "ts": 45578246795.728, "ph": "X", "cat": "fee", "dur": 1.721, "name": "iscoroutine (/usr/lib/python3.13/asyncio/coroutines.py:32)"}, {"pid": 30327, "tid": 30327, "ts": 45578246797.657, "ph": "X", "cat": "fee", "dur": 0.158, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578246798.132, "ph": "X", "cat": "fee", "dur": 0.435, "name": "Runner._lazy_init (/usr/lib/python3.13/asyncio/runners.py:131)"}, {"pid": 30327, "tid": 30327, "ts": 45578246800.08, "ph": "X", "cat": "fee", "dur": 0.113, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578246803.505, "ph": "X", "cat": "fee", "dur": 0.119, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578246805.317, "ph": "X", "cat": "fee", "dur": 0.053, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578246807.517, "ph": "X", "cat": "fee", "dur": 0.064, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578246806.772, "ph": "X", "cat": "fee", "dur": 0.965, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578246808.568, "ph": "X", "cat": "fee", "dur": 0.236, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578246806.209, "ph": "X", "cat": "fee", "dur": 2.686, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578246805.144, "ph": "X", "cat": "fee", "dur": 3.912, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578246810.351, "ph": "X", "cat": "fee", "dur": 0.17, "name": "set.add"}, {"pid": 30327, "tid": 30327, "ts": 45578246809.883, "ph": "X", "cat": "fee", "dur": 0.715, "name": "WeakSet.add (/usr/lib/python3.13/_weakrefset.py:85)"}, {"pid": 30327, "tid": 30327, "ts": 45578246799.897, "ph": "X", "cat": "fee", "dur": 12.519, "name": "BaseEventLoop.create_task (/usr/lib/python3.13/asyncio/base_events.py:462)"}, {"pid": 30327, "tid": 30327, "ts": 45578246813.474, "ph": "X", "cat": "fee", "dur": 0.152, "name": "_thread.get_ident"}, {"pid": 30327, "tid": 30327, "ts": 45578246813.243, "ph": "X", "cat": "fee", "dur": 0.775, "name": "current_thread (/usr/lib/python3.13/threading.py:1427)"}, {"pid": 30327, "tid": 30327, "ts": 45578246814.61, "ph": "X", "cat": "fee", "dur": 0.183, "name": "main_thread (/usr/lib/python3.13/threading.py:1543)"}, {"pid": 30327, "tid": 30327, "ts": 45578246816.107, "ph": "X", "cat": "fee", "dur": 0.34, "name": "_signal.getsignal"}, {"pid": 30327, "tid": 30327, "ts": 45578246817.301, "ph": "X", "cat": "fee", "dur": 0.457, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246817.043, "ph": "X", "cat": "fee", "dur": 0.811, "name": "_int_to_enum (/usr/lib/python3.13/signal.py:24)"}, {"pid": 30327, "tid": 30327, "ts": 45578246815.814, "ph": "X", "cat": "fee", "dur": 2.099, "name": "getsignal (/usr/lib/python3.13/signal.py:62)"}, {"pid": 30327, "tid": 30327, "ts": 45578246820.745, "ph": "X", "cat": "fee", "dur": 0.238, "name": "_enum_to_int (/usr/lib/python3.13/signal.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578246821.15, "ph": "X", "cat": "fee", "dur": 2.819, "name": "_enum_to_int (/usr/lib/python3.13/signal.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578246824.099, "ph": "X", "cat": "fee", "dur": 0.732, "name": "_signal.signal"}, {"pid": 30327, "tid": 30327, "ts": 45578246825.209, "ph": "X", "cat": "fee", "dur": 0.088, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578246825.122, "ph": "X", "cat": "fee", "dur": 0.26, "name": "_int_to_enum (/usr/lib/python3.13/signal.py:24)"}, {"pid": 30327, "tid": 30327, "ts": 45578246820.002, "ph": "X", "cat": "fee", "dur": 5.442, "name": "signal (/usr/lib/python3.13/signal.py:56)"}, {"pid": 30327, "tid": 30327, "ts": 45578246827.734, "ph": "X", "cat": "fee", "dur": 0.088, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578246828.532, "ph": "X", "cat": "fee", "dur": 0.181, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578246828.969, "ph": "X", "cat": "fee", "dur": 0.109, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578246828.395, "ph": "X", "cat": "fee", "dur": 0.737, "name": "BaseEventLoop._check_running (/usr/lib/python3.13/asyncio/base_events.py:631)"}, {"pid": 30327, "tid": 30327, "ts": 45578246830.336, "ph": "X", "cat": "fee", "dur": 0.359, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578246829.867, "ph": "X", "cat": "fee", "dur": 1.052, "name": "isfuture (/usr/lib/python3.13/asyncio/base_futures.py:13)"}, {"pid": 30327, "tid": 30327, "ts": 45578246832.827, "ph": "X", "cat": "fee", "dur": 0.134, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578246832.426, "ph": "X", "cat": "fee", "dur": 0.743, "name": "isfuture (/usr/lib/python3.13/asyncio/base_futures.py:13)"}, {"pid": 30327, "tid": 30327, "ts": 45578246834.69, "ph": "X", "cat": "fee", "dur": 0.092, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578246833.845, "ph": "X", "cat": "fee", "dur": 1.008, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578246832.219, "ph": "X", "cat": "fee", "dur": 2.788, "name": "ensure_future (/usr/lib/python3.13/asyncio/tasks.py:723)"}, {"pid": 30327, "tid": 30327, "ts": 45578246835.474, "ph": "X", "cat": "fee", "dur": 0.435, "name": "_asyncio.Task.add_done_callback"}, {"pid": 30327, "tid": 30327, "ts": 45578246837.844, "ph": "X", "cat": "fee", "dur": 0.059, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578246838.231, "ph": "X", "cat": "fee", "dur": 0.079, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578246838.606, "ph": "X", "cat": "fee", "dur": 0.072, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578246838.041, "ph": "X", "cat": "fee", "dur": 0.694, "name": "BaseEventLoop._check_running (/usr/lib/python3.13/asyncio/base_events.py:631)"}, {"pid": 30327, "tid": 30327, "ts": 45578246839.541, "ph": "X", "cat": "fee", "dur": 0.488, "name": "BaseEventLoop._set_coroutine_origin_tracking (/usr/lib/python3.13/asyncio/base_events.py:2030)"}, {"pid": 30327, "tid": 30327, "ts": 45578246840.402, "ph": "X", "cat": "fee", "dur": 0.768, "name": "sys.get_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578246841.526, "ph": "X", "cat": "fee", "dur": 0.147, "name": "_thread.get_ident"}, {"pid": 30327, "tid": 30327, "ts": 45578246843.484, "ph": "X", "cat": "fee", "dur": 2.359, "name": "sys.set_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578246846.092, "ph": "X", "cat": "fee", "dur": 0.164, "name": "_asyncio._set_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578246837.647, "ph": "X", "cat": "fee", "dur": 8.713, "name": "BaseEventLoop._run_forever_setup (/usr/lib/python3.13/asyncio/base_events.py:638)"}, {"pid": 30327, "tid": 30327, "ts": 45578246849.985, "ph": "X", "cat": "fee", "dur": 0.131, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246852.465, "ph": "X", "cat": "fee", "dur": 0.107, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246853.046, "ph": "X", "cat": "fee", "dur": 2.365, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578246852.153, "ph": "X", "cat": "fee", "dur": 3.615, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578246856.974, "ph": "X", "cat": "fee", "dur": 0.141, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578246858.151, "ph": "X", "cat": "fee", "dur": 0.354, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578246857.77, "ph": "X", "cat": "fee", "dur": 0.79, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578246859.134, "ph": "X", "cat": "fee", "dur": 0.115, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246860.123, "ph": "X", "cat": "fee", "dur": 0.166, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16328928, "ts": 45578246867.601, "ph": "X", "cat": "fee", "dur": 0.096, "name": "_asyncio.get_running_loop"}, {"pid": 30327, "tid": 16328928, "ts": 45578246868.402, "ph": "X", "cat": "fee", "dur": 0.104, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246870.718, "ph": "X", "cat": "fee", "dur": 0.057, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246871.724, "ph": "X", "cat": "fee", "dur": 0.061, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246872.874, "ph": "X", "cat": "fee", "dur": 0.046, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246872.593, "ph": "X", "cat": "fee", "dur": 0.467, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246873.824, "ph": "X", "cat": "fee", "dur": 0.156, "name": "collections.deque.append"}, {"pid": 30327, "tid": 16328928, "ts": 45578246872.115, "ph": "X", "cat": "fee", "dur": 1.998, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246871.574, "ph": "X", "cat": "fee", "dur": 2.75, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246875.152, "ph": "X", "cat": "fee", "dur": 0.114, "name": "set.add"}, {"pid": 30327, "tid": 16328928, "ts": 45578246874.719, "ph": "X", "cat": "fee", "dur": 0.632, "name": "WeakSet.add (/usr/lib/python3.13/_weakrefset.py:85)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246868.202, "ph": "X", "cat": "fee", "dur": 7.83, "name": "BaseEventLoop.create_task (/usr/lib/python3.13/asyncio/base_events.py:462)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246867.433, "ph": "X", "cat": "fee", "dur": 8.744, "name": "create_task (/usr/lib/python3.13/asyncio/tasks.py:402)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246876.811, "ph": "X", "cat": "fee", "dur": 0.08, "name": "_asyncio.get_running_loop"}, {"pid": 30327, "tid": 16328928, "ts": 45578246877.324, "ph": "X", "cat": "fee", "dur": 0.045, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246877.831, "ph": "X", "cat": "fee", "dur": 0.047, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246878.331, "ph": "X", "cat": "fee", "dur": 0.051, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246879.072, "ph": "X", "cat": "fee", "dur": 0.047, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246878.852, "ph": "X", "cat": "fee", "dur": 0.35, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246879.441, "ph": "X", "cat": "fee", "dur": 0.089, "name": "collections.deque.append"}, {"pid": 30327, "tid": 16328928, "ts": 45578246878.605, "ph": "X", "cat": "fee", "dur": 1.024, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246878.217, "ph": "X", "cat": "fee", "dur": 1.537, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246880.109, "ph": "X", "cat": "fee", "dur": 0.088, "name": "set.add"}, {"pid": 30327, "tid": 16328928, "ts": 45578246879.895, "ph": "X", "cat": "fee", "dur": 0.367, "name": "WeakSet.add (/usr/lib/python3.13/_weakrefset.py:85)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246877.201, "ph": "X", "cat": "fee", "dur": 3.337, "name": "BaseEventLoop.create_task (/usr/lib/python3.13/asyncio/base_events.py:462)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246876.698, "ph": "X", "cat": "fee", "dur": 3.942, "name": "create_task (/usr/lib/python3.13/asyncio/tasks.py:402)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246881.048, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_asyncio.get_running_loop"}, {"pid": 30327, "tid": 16328928, "ts": 45578246881.382, "ph": "X", "cat": "fee", "dur": 0.047, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246881.83, "ph": "X", "cat": "fee", "dur": 0.046, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246882.224, "ph": "X", "cat": "fee", "dur": 0.044, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246882.79, "ph": "X", "cat": "fee", "dur": 0.042, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246882.604, "ph": "X", "cat": "fee", "dur": 0.318, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246883.119, "ph": "X", "cat": "fee", "dur": 0.08, "name": "collections.deque.append"}, {"pid": 30327, "tid": 16328928, "ts": 45578246882.448, "ph": "X", "cat": "fee", "dur": 0.841, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246882.119, "ph": "X", "cat": "fee", "dur": 1.255, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246883.697, "ph": "X", "cat": "fee", "dur": 0.084, "name": "set.add"}, {"pid": 30327, "tid": 16328928, "ts": 45578246883.517, "ph": "X", "cat": "fee", "dur": 0.329, "name": "WeakSet.add (/usr/lib/python3.13/_weakrefset.py:85)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246881.28, "ph": "X", "cat": "fee", "dur": 2.767, "name": "BaseEventLoop.create_task (/usr/lib/python3.13/asyncio/base_events.py:462)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246880.964, "ph": "X", "cat": "fee", "dur": 3.176, "name": "create_task (/usr/lib/python3.13/asyncio/tasks.py:402)"}, {"pid": 30327, "tid": 16328928, "ts": 45578246866.244, "ph": "X", "cat": "fee", "dur": 18.472, "name": "main (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:8)"}, {"pid": 30327, "tid": 30327, "ts": 45578246862.778, "ph": "X", "cat": "fee", "dur": 22.653, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578246861.579, "ph": "X", "cat": "fee", "dur": 24.058, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578246849.358, "ph": "X", "cat": "fee", "dur": 36.939, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578246887.188, "ph": "X", "cat": "fee", "dur": 0.101, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246888.376, "ph": "X", "cat": "fee", "dur": 0.104, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246888.824, "ph": "X", "cat": "fee", "dur": 0.785, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578246888.098, "ph": "X", "cat": "fee", "dur": 1.842, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578246890.339, "ph": "X", "cat": "fee", "dur": 0.158, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578246891.113, "ph": "X", "cat": "fee", "dur": 0.166, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578246890.767, "ph": "X", "cat": "fee", "dur": 1.423, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578246892.675, "ph": "X", "cat": "fee", "dur": 0.07, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246893.288, "ph": "X", "cat": "fee", "dur": 0.103, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329104, "ts": 45578246897.607, "ph": "X", "cat": "fee", "dur": 0.258, "name": "math.isnan"}, {"pid": 30327, "tid": 16329104, "ts": 45578246898.06, "ph": "X", "cat": "fee", "dur": 0.071, "name": "_asyncio.get_running_loop"}, {"pid": 30327, "tid": 16329104, "ts": 45578246899.645, "ph": "X", "cat": "fee", "dur": 0.062, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246898.637, "ph": "X", "cat": "fee", "dur": 1.317, "name": "BaseEventLoop.create_future (/usr/lib/python3.13/asyncio/base_events.py:458)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246901.624, "ph": "X", "cat": "fee", "dur": 0.13, "name": "time.monotonic"}, {"pid": 30327, "tid": 16329104, "ts": 45578246901.519, "ph": "X", "cat": "fee", "dur": 0.295, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246903.708, "ph": "X", "cat": "fee", "dur": 0.088, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246906.607, "ph": "X", "cat": "fee", "dur": 0.187, "name": "_contextvars.copy_context"}, {"pid": 30327, "tid": 16329104, "ts": 45578246907.783, "ph": "X", "cat": "fee", "dur": 0.056, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246906.361, "ph": "X", "cat": "fee", "dur": 1.724, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246905.645, "ph": "X", "cat": "fee", "dur": 2.81, "name": "TimerHandle.__init__ (/usr/lib/python3.13/asyncio/events.py:113)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246909.222, "ph": "X", "cat": "fee", "dur": 0.551, "name": "_heapq.heappush"}, {"pid": 30327, "tid": 16329104, "ts": 45578246903.506, "ph": "X", "cat": "fee", "dur": 6.504, "name": "BaseEventLoop.call_at (/usr/lib/python3.13/asyncio/base_events.py:801)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246901.027, "ph": "X", "cat": "fee", "dur": 9.482, "name": "BaseEventLoop.call_later (/usr/lib/python3.13/asyncio/base_events.py:777)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246896.828, "ph": "X", "cat": "fee", "dur": 14.189, "name": "sleep (/usr/lib/python3.13/asyncio/tasks.py:703)"}, {"pid": 30327, "tid": 16329104, "ts": 45578246895.412, "ph": "X", "cat": "fee", "dur": 15.716, "name": "io_task (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:4)"}, {"pid": 30327, "tid": 30327, "ts": 45578246894.443, "ph": "X", "cat": "fee", "dur": 18.404, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578246893.832, "ph": "X", "cat": "fee", "dur": 19.188, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578246913.287, "ph": "X", "cat": "fee", "dur": 0.132, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329280, "ts": 45578246915.784, "ph": "X", "cat": "fee", "dur": 0.067, "name": "math.isnan"}, {"pid": 30327, "tid": 16329280, "ts": 45578246916.041, "ph": "X", "cat": "fee", "dur": 0.067, "name": "_asyncio.get_running_loop"}, {"pid": 30327, "tid": 16329280, "ts": 45578246916.835, "ph": "X", "cat": "fee", "dur": 0.063, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246916.33, "ph": "X", "cat": "fee", "dur": 0.713, "name": "BaseEventLoop.create_future (/usr/lib/python3.13/asyncio/base_events.py:458)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246917.774, "ph": "X", "cat": "fee", "dur": 0.15, "name": "time.monotonic"}, {"pid": 30327, "tid": 16329280, "ts": 45578246917.691, "ph": "X", "cat": "fee", "dur": 0.287, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246918.752, "ph": "X", "cat": "fee", "dur": 0.054, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246919.936, "ph": "X", "cat": "fee", "dur": 0.114, "name": "_contextvars.copy_context"}, {"pid": 30327, "tid": 16329280, "ts": 45578246920.503, "ph": "X", "cat": "fee", "dur": 0.053, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246919.831, "ph": "X", "cat": "fee", "dur": 0.85, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246919.289, "ph": "X", "cat": "fee", "dur": 1.867, "name": "TimerHandle.__init__ (/usr/lib/python3.13/asyncio/events.py:113)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246922.596, "ph": "X", "cat": "fee", "dur": 0.118, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 16329280, "ts": 45578246922.434, "ph": "X", "cat": "fee", "dur": 0.582, "name": "TimerHandle.__lt__ (/usr/lib/python3.13/asyncio/events.py:129)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246921.569, "ph": "X", "cat": "fee", "dur": 1.597, "name": "_heapq.heappush"}, {"pid": 30327, "tid": 16329280, "ts": 45578246918.603, "ph": "X", "cat": "fee", "dur": 4.712, "name": "BaseEventLoop.call_at (/usr/lib/python3.13/asyncio/base_events.py:801)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246917.387, "ph": "X", "cat": "fee", "dur": 6.406, "name": "BaseEventLoop.call_later (/usr/lib/python3.13/asyncio/base_events.py:777)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246915.342, "ph": "X", "cat": "fee", "dur": 8.768, "name": "sleep (/usr/lib/python3.13/asyncio/tasks.py:703)"}, {"pid": 30327, "tid": 16329280, "ts": 45578246914.796, "ph": "X", "cat": "fee", "dur": 9.402, "name": "io_task (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:4)"}, {"pid": 30327, "tid": 30327, "ts": 45578246914.414, "ph": "X", "cat": "fee", "dur": 10.113, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578246914.077, "ph": "X", "cat": "fee", "dur": 10.567, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578246924.802, "ph": "X", "cat": "fee", "dur": 0.089, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329456, "ts": 45578246926.182, "ph": "X", "cat": "fee", "dur": 0.068, "name": "math.isnan"}, {"pid": 30327, "tid": 16329456, "ts": 45578246926.362, "ph": "X", "cat": "fee", "dur": 0.077, "name": "_asyncio.get_running_loop"}, {"pid": 30327, "tid": 16329456, "ts": 45578246926.88, "ph": "X", "cat": "fee", "dur": 0.054, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246926.559, "ph": "X", "cat": "fee", "dur": 1.852, "name": "BaseEventLoop.create_future (/usr/lib/python3.13/asyncio/base_events.py:458)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246929.053, "ph": "X", "cat": "fee", "dur": 0.147, "name": "time.monotonic"}, {"pid": 30327, "tid": 16329456, "ts": 45578246928.968, "ph": "X", "cat": "fee", "dur": 0.281, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246929.92, "ph": "X", "cat": "fee", "dur": 0.069, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246930.789, "ph": "X", "cat": "fee", "dur": 0.109, "name": "_contextvars.copy_context"}, {"pid": 30327, "tid": 16329456, "ts": 45578246931.228, "ph": "X", "cat": "fee", "dur": 0.054, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246930.71, "ph": "X", "cat": "fee", "dur": 0.698, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246930.334, "ph": "X", "cat": "fee", "dur": 1.35, "name": "TimerHandle.__init__ (/usr/lib/python3.13/asyncio/events.py:113)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246933.233, "ph": "X", "cat": "fee", "dur": 0.07, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 16329456, "ts": 45578246933.031, "ph": "X", "cat": "fee", "dur": 0.531, "name": "TimerHandle.__lt__ (/usr/lib/python3.13/asyncio/events.py:129)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246932.808, "ph": "X", "cat": "fee", "dur": 0.869, "name": "_heapq.heappush"}, {"pid": 30327, "tid": 16329456, "ts": 45578246929.803, "ph": "X", "cat": "fee", "dur": 4.002, "name": "BaseEventLoop.call_at (/usr/lib/python3.13/asyncio/base_events.py:801)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246928.727, "ph": "X", "cat": "fee", "dur": 5.396, "name": "BaseEventLoop.call_later (/usr/lib/python3.13/asyncio/base_events.py:777)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246926.012, "ph": "X", "cat": "fee", "dur": 8.434, "name": "sleep (/usr/lib/python3.13/asyncio/tasks.py:703)"}, {"pid": 30327, "tid": 16329456, "ts": 45578246925.739, "ph": "X", "cat": "fee", "dur": 8.792, "name": "io_task (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:4)"}, {"pid": 30327, "tid": 30327, "ts": 45578246925.491, "ph": "X", "cat": "fee", "dur": 9.367, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578246925.297, "ph": "X", "cat": "fee", "dur": 9.656, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578246886.926, "ph": "X", "cat": "fee", "dur": 48.298, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578246935.797, "ph": "X", "cat": "fee", "dur": 0.131, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246937.133, "ph": "X", "cat": "fee", "dur": 0.128, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578246937.058, "ph": "X", "cat": "fee", "dur": 0.248, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578246938.593, "ph": "X", "cat": "fee", "dur": 1.756, "name": "math.ceil"}, {"pid": 30327, "tid": 30327, "ts": 45578246940.668, "ph": "X", "cat": "fee", "dur": 0.113, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578246941.034, "ph": "X", "cat": "fee", "dur": 10306.091, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578246938.048, "ph": "X", "cat": "fee", "dur": 10313.412, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257255.821, "ph": "X", "cat": "fee", "dur": 0.776, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257258.553, "ph": "X", "cat": "fee", "dur": 1.837, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257257.348, "ph": "X", "cat": "fee", "dur": 3.349, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257269.962, "ph": "X", "cat": "fee", "dur": 0.601, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257269.347, "ph": "X", "cat": "fee", "dur": 1.654, "name": "TimerHandle.__lt__ (/usr/lib/python3.13/asyncio/events.py:129)"}, {"pid": 30327, "tid": 30327, "ts": 45578257266.859, "ph": "X", "cat": "fee", "dur": 4.406, "name": "_heapq.heappop"}, {"pid": 30327, "tid": 30327, "ts": 45578257273.597, "ph": "X", "cat": "fee", "dur": 0.53, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257276.226, "ph": "X", "cat": "fee", "dur": 0.197, "name": "_heapq.heappop"}, {"pid": 30327, "tid": 30327, "ts": 45578257277.175, "ph": "X", "cat": "fee", "dur": 0.091, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257277.743, "ph": "X", "cat": "fee", "dur": 0.534, "name": "_heapq.heappop"}, {"pid": 30327, "tid": 30327, "ts": 45578257278.414, "ph": "X", "cat": "fee", "dur": 0.075, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257278.677, "ph": "X", "cat": "fee", "dur": 0.396, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257281.234, "ph": "X", "cat": "fee", "dur": 0.282, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 30327, "ts": 45578257289.885, "ph": "X", "cat": "fee", "dur": 0.492, "name": "_asyncio.Future.cancelled"}, {"pid": 30327, "tid": 30327, "ts": 45578257294.522, "ph": "X", "cat": "fee", "dur": 0.134, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257297.568, "ph": "X", "cat": "fee", "dur": 0.06, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257297.017, "ph": "X", "cat": "fee", "dur": 0.847, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257298.433, "ph": "X", "cat": "fee", "dur": 0.21, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257295.595, "ph": "X", "cat": "fee", "dur": 3.151, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257294.01, "ph": "X", "cat": "fee", "dur": 4.857, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257291.721, "ph": "X", "cat": "fee", "dur": 7.408, "name": "_asyncio.Future.set_result"}, {"pid": 30327, "tid": 30327, "ts": 45578257289.259, "ph": "X", "cat": "fee", "dur": 10.089, "name": "_set_result_unless_cancelled (/usr/lib/python3.13/asyncio/futures.py:312)"}, {"pid": 30327, "tid": 30327, "ts": 45578257286.508, "ph": "X", "cat": "fee", "dur": 18.502, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257283.383, "ph": "X", "cat": "fee", "dur": 21.943, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257305.605, "ph": "X", "cat": "fee", "dur": 0.134, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 30327, "ts": 45578257307.161, "ph": "X", "cat": "fee", "dur": 0.098, "name": "_asyncio.Future.cancelled"}, {"pid": 30327, "tid": 30327, "ts": 45578257307.95, "ph": "X", "cat": "fee", "dur": 0.071, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257308.647, "ph": "X", "cat": "fee", "dur": 0.054, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257308.468, "ph": "X", "cat": "fee", "dur": 0.328, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257308.989, "ph": "X", "cat": "fee", "dur": 0.091, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257308.186, "ph": "X", "cat": "fee", "dur": 0.968, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257307.833, "ph": "X", "cat": "fee", "dur": 1.408, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257307.491, "ph": "X", "cat": "fee", "dur": 1.885, "name": "_asyncio.Future.set_result"}, {"pid": 30327, "tid": 30327, "ts": 45578257306.865, "ph": "X", "cat": "fee", "dur": 2.631, "name": "_set_result_unless_cancelled (/usr/lib/python3.13/asyncio/futures.py:312)"}, {"pid": 30327, "tid": 30327, "ts": 45578257306.722, "ph": "X", "cat": "fee", "dur": 2.886, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257306.171, "ph": "X", "cat": "fee", "dur": 3.532, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257309.836, "ph": "X", "cat": "fee", "dur": 0.094, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 30327, "ts": 45578257311.207, "ph": "X", "cat": "fee", "dur": 0.091, "name": "_asyncio.Future.cancelled"}, {"pid": 30327, "tid": 30327, "ts": 45578257311.813, "ph": "X", "cat": "fee", "dur": 0.058, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257312.352, "ph": "X", "cat": "fee", "dur": 0.049, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257312.211, "ph": "X", "cat": "fee", "dur": 0.273, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257312.633, "ph": "X", "cat": "fee", "dur": 0.097, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257311.993, "ph": "X", "cat": "fee", "dur": 0.813, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257311.707, "ph": "X", "cat": "fee", "dur": 1.166, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257311.434, "ph": "X", "cat": "fee", "dur": 1.586, "name": "_asyncio.Future.set_result"}, {"pid": 30327, "tid": 30327, "ts": 45578257311.114, "ph": "X", "cat": "fee", "dur": 1.962, "name": "_set_result_unless_cancelled (/usr/lib/python3.13/asyncio/futures.py:312)"}, {"pid": 30327, "tid": 30327, "ts": 45578257311.048, "ph": "X", "cat": "fee", "dur": 2.143, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257310.184, "ph": "X", "cat": "fee", "dur": 3.067, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578246935.698, "ph": "X", "cat": "fee", "dur": 10377.736, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257314.221, "ph": "X", "cat": "fee", "dur": 0.117, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257315.705, "ph": "X", "cat": "fee", "dur": 0.252, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257316.416, "ph": "X", "cat": "fee", "dur": 3.222, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578257315.129, "ph": "X", "cat": "fee", "dur": 4.797, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257320.255, "ph": "X", "cat": "fee", "dur": 0.108, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257320.732, "ph": "X", "cat": "fee", "dur": 0.14, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257320.58, "ph": "X", "cat": "fee", "dur": 0.385, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257321.132, "ph": "X", "cat": "fee", "dur": 0.094, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257321.571, "ph": "X", "cat": "fee", "dur": 0.096, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329104, "ts": 45578257332.045, "ph": "X", "cat": "fee", "dur": 0.224, "name": "BaseEventLoop._timer_handle_cancelled (/usr/lib/python3.13/asyncio/base_events.py:1942)"}, {"pid": 30327, "tid": 16329104, "ts": 45578257336.836, "ph": "X", "cat": "fee", "dur": 0.086, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329104, "ts": 45578257336.045, "ph": "X", "cat": "fee", "dur": 1.342, "name": "Handle.cancel (/usr/lib/python3.13/asyncio/events.py:73)"}, {"pid": 30327, "tid": 16329104, "ts": 45578257330.007, "ph": "X", "cat": "fee", "dur": 7.559, "name": "TimerHandle.cancel (/usr/lib/python3.13/asyncio/events.py:157)"}, {"pid": 30327, "tid": 16329104, "ts": 45578257326.98, "ph": "X", "cat": "fee", "dur": 10.704, "name": "sleep (/usr/lib/python3.13/asyncio/tasks.py:703)"}, {"pid": 30327, "tid": 16329104, "ts": 45578257325.834, "ph": "X", "cat": "fee", "dur": 13.04, "name": "io_task (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:4)"}, {"pid": 30327, "tid": 30327, "ts": 45578257339.569, "ph": "X", "cat": "fee", "dur": 0.06, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257340.245, "ph": "X", "cat": "fee", "dur": 0.05, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257340.074, "ph": "X", "cat": "fee", "dur": 0.319, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257340.584, "ph": "X", "cat": "fee", "dur": 0.138, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257339.829, "ph": "X", "cat": "fee", "dur": 2.078, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257339.376, "ph": "X", "cat": "fee", "dur": 2.621, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257322.343, "ph": "X", "cat": "fee", "dur": 20.61, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257321.985, "ph": "X", "cat": "fee", "dur": 21.115, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257343.284, "ph": "X", "cat": "fee", "dur": 0.07, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329280, "ts": 45578257346.977, "ph": "X", "cat": "fee", "dur": 0.152, "name": "BaseEventLoop._timer_handle_cancelled (/usr/lib/python3.13/asyncio/base_events.py:1942)"}, {"pid": 30327, "tid": 16329280, "ts": 45578257348.3, "ph": "X", "cat": "fee", "dur": 0.051, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329280, "ts": 45578257347.842, "ph": "X", "cat": "fee", "dur": 0.873, "name": "Handle.cancel (/usr/lib/python3.13/asyncio/events.py:73)"}, {"pid": 30327, "tid": 16329280, "ts": 45578257346.404, "ph": "X", "cat": "fee", "dur": 2.387, "name": "TimerHandle.cancel (/usr/lib/python3.13/asyncio/events.py:157)"}, {"pid": 30327, "tid": 16329280, "ts": 45578257345.776, "ph": "X", "cat": "fee", "dur": 3.107, "name": "sleep (/usr/lib/python3.13/asyncio/tasks.py:703)"}, {"pid": 30327, "tid": 16329280, "ts": 45578257345.594, "ph": "X", "cat": "fee", "dur": 3.6, "name": "io_task (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:4)"}, {"pid": 30327, "tid": 30327, "ts": 45578257344.858, "ph": "X", "cat": "fee", "dur": 4.599, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257344.582, "ph": "X", "cat": "fee", "dur": 4.965, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257350.133, "ph": "X", "cat": "fee", "dur": 0.095, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329456, "ts": 45578257351.764, "ph": "X", "cat": "fee", "dur": 0.073, "name": "BaseEventLoop._timer_handle_cancelled (/usr/lib/python3.13/asyncio/base_events.py:1942)"}, {"pid": 30327, "tid": 16329456, "ts": 45578257352.473, "ph": "X", "cat": "fee", "dur": 0.047, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 16329456, "ts": 45578257352.25, "ph": "X", "cat": "fee", "dur": 0.453, "name": "Handle.cancel (/usr/lib/python3.13/asyncio/events.py:73)"}, {"pid": 30327, "tid": 16329456, "ts": 45578257351.585, "ph": "X", "cat": "fee", "dur": 1.183, "name": "TimerHandle.cancel (/usr/lib/python3.13/asyncio/events.py:157)"}, {"pid": 30327, "tid": 16329456, "ts": 45578257351.177, "ph": "X", "cat": "fee", "dur": 1.68, "name": "sleep (/usr/lib/python3.13/asyncio/tasks.py:703)"}, {"pid": 30327, "tid": 16329456, "ts": 45578257351.121, "ph": "X", "cat": "fee", "dur": 1.897, "name": "io_task (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:4)"}, {"pid": 30327, "tid": 30327, "ts": 45578257350.885, "ph": "X", "cat": "fee", "dur": 2.358, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257350.65, "ph": "X", "cat": "fee", "dur": 2.652, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257314.096, "ph": "X", "cat": "fee", "dur": 39.496, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257353.981, "ph": "X", "cat": "fee", "dur": 0.113, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257354.702, "ph": "X", "cat": "fee", "dur": 0.098, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257354.992, "ph": "X", "cat": "fee", "dur": 0.799, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578257354.574, "ph": "X", "cat": "fee", "dur": 1.429, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257356.318, "ph": "X", "cat": "fee", "dur": 0.093, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257356.793, "ph": "X", "cat": "fee", "dur": 0.162, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257356.644, "ph": "X", "cat": "fee", "dur": 0.359, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257357.145, "ph": "X", "cat": "fee", "dur": 0.068, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257357.559, "ph": "X", "cat": "fee", "dur": 0.065, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16328928, "ts": 45578257358.755, "ph": "X", "cat": "fee", "dur": 1.436, "name": "main (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:8)"}, {"pid": 30327, "tid": 30327, "ts": 45578257360.845, "ph": "X", "cat": "fee", "dur": 0.079, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257361.476, "ph": "X", "cat": "fee", "dur": 0.049, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257361.295, "ph": "X", "cat": "fee", "dur": 0.309, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257361.767, "ph": "X", "cat": "fee", "dur": 0.092, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257361.077, "ph": "X", "cat": "fee", "dur": 0.872, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257360.718, "ph": "X", "cat": "fee", "dur": 1.295, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257358.113, "ph": "X", "cat": "fee", "dur": 4.139, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257357.842, "ph": "X", "cat": "fee", "dur": 4.483, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257353.895, "ph": "X", "cat": "fee", "dur": 8.623, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257362.827, "ph": "X", "cat": "fee", "dur": 0.069, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257363.275, "ph": "X", "cat": "fee", "dur": 0.074, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257363.484, "ph": "X", "cat": "fee", "dur": 0.454, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578257363.171, "ph": "X", "cat": "fee", "dur": 0.932, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257364.286, "ph": "X", "cat": "fee", "dur": 0.091, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257367.341, "ph": "X", "cat": "fee", "dur": 0.101, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257367.249, "ph": "X", "cat": "fee", "dur": 0.23, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257367.608, "ph": "X", "cat": "fee", "dur": 0.07, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257367.909, "ph": "X", "cat": "fee", "dur": 0.067, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 30327, "ts": 45578257369.815, "ph": "X", "cat": "fee", "dur": 0.151, "name": "_asyncio.Task.cancelled"}, {"pid": 30327, "tid": 30327, "ts": 45578257370.457, "ph": "X", "cat": "fee", "dur": 0.171, "name": "_asyncio.Task.exception"}, {"pid": 30327, "tid": 30327, "ts": 45578257371.734, "ph": "X", "cat": "fee", "dur": 2.013, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257375.183, "ph": "X", "cat": "fee", "dur": 0.189, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257374.721, "ph": "X", "cat": "fee", "dur": 0.741, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578257376.276, "ph": "X", "cat": "fee", "dur": 0.248, "name": "BaseEventLoop.stop (/usr/lib/python3.13/asyncio/base_events.py:723)"}, {"pid": 30327, "tid": 30327, "ts": 45578257369.352, "ph": "X", "cat": "fee", "dur": 7.321, "name": "_run_until_complete_cb (/usr/lib/python3.13/asyncio/base_events.py:182)"}, {"pid": 30327, "tid": 30327, "ts": 45578257368.489, "ph": "X", "cat": "fee", "dur": 8.281, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257368.167, "ph": "X", "cat": "fee", "dur": 8.686, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257362.769, "ph": "X", "cat": "fee", "dur": 14.381, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257379.2, "ph": "X", "cat": "fee", "dur": 0.163, "name": "_asyncio._set_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257379.79, "ph": "X", "cat": "fee", "dur": 2.115, "name": "BaseEventLoop._set_coroutine_origin_tracking (/usr/lib/python3.13/asyncio/base_events.py:2030)"}, {"pid": 30327, "tid": 30327, "ts": 45578257383.821, "ph": "X", "cat": "fee", "dur": 3.332, "name": "sys.set_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578257378.294, "ph": "X", "cat": "fee", "dur": 9.203, "name": "BaseEventLoop._run_forever_cleanup (/usr/lib/python3.13/asyncio/base_events.py:658)"}, {"pid": 30327, "tid": 30327, "ts": 45578246836.812, "ph": "X", "cat": "fee", "dur": 10550.805, "name": "BaseEventLoop.run_forever (/usr/lib/python3.13/asyncio/base_events.py:674)"}, {"pid": 30327, "tid": 30327, "ts": 45578257388.275, "ph": "X", "cat": "fee", "dur": 0.24, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 30327, "tid": 30327, "ts": 45578257389.546, "ph": "X", "cat": "fee", "dur": 0.088, "name": "_asyncio.Task.done"}, {"pid": 30327, "tid": 30327, "ts": 45578257390.02, "ph": "X", "cat": "fee", "dur": 0.127, "name": "_asyncio.Task.result"}, {"pid": 30327, "tid": 30327, "ts": 45578246827.527, "ph": "X", "cat": "fee", "dur": 10562.71, "name": "BaseEventLoop.run_until_complete (/usr/lib/python3.13/asyncio/base_events.py:685)"}, {"pid": 30327, "tid": 30327, "ts": 45578257393.048, "ph": "X", "cat": "fee", "dur": 0.546, "name": "_signal.getsignal"}, {"pid": 30327, "tid": 30327, "ts": 45578257394.298, "ph": "X", "cat": "fee", "dur": 1.088, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257394.222, "ph": "X", "cat": "fee", "dur": 1.266, "name": "_int_to_enum (/usr/lib/python3.13/signal.py:24)"}, {"pid": 30327, "tid": 30327, "ts": 45578257392.169, "ph": "X", "cat": "fee", "dur": 3.397, "name": "getsignal (/usr/lib/python3.13/signal.py:62)"}, {"pid": 30327, "tid": 30327, "ts": 45578257396.421, "ph": "X", "cat": "fee", "dur": 0.902, "name": "_enum_to_int (/usr/lib/python3.13/signal.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257397.456, "ph": "X", "cat": "fee", "dur": 3.94, "name": "_enum_to_int (/usr/lib/python3.13/signal.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257401.543, "ph": "X", "cat": "fee", "dur": 1.877, "name": "_signal.signal"}, {"pid": 30327, "tid": 30327, "ts": 45578257403.759, "ph": "X", "cat": "fee", "dur": 0.16, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257403.703, "ph": "X", "cat": "fee", "dur": 0.274, "name": "_int_to_enum (/usr/lib/python3.13/signal.py:24)"}, {"pid": 30327, "tid": 30327, "ts": 45578257396.206, "ph": "X", "cat": "fee", "dur": 7.827, "name": "signal (/usr/lib/python3.13/signal.py:56)"}, {"pid": 30327, "tid": 30327, "ts": 45578246794.779, "ph": "X", "cat": "fee", "dur": 10609.365, "name": "Runner.run (/usr/lib/python3.13/asyncio/runners.py:86)"}, {"pid": 30327, "tid": 30327, "ts": 45578257416.725, "ph": "X", "cat": "fee", "dur": 0.227, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257417.338, "ph": "X", "cat": "fee", "dur": 0.07, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257416.091, "ph": "X", "cat": "fee", "dur": 1.832, "name": "WeakSet.__len__ (/usr/lib/python3.13/_weakrefset.py:72)"}, {"pid": 30327, "tid": 30327, "ts": 45578257422.396, "ph": "X", "cat": "fee", "dur": 1.323, "name": "_IterationGuard.__init__ (/usr/lib/python3.13/_weakrefset.py:17)"}, {"pid": 30327, "tid": 30327, "ts": 45578257425.715, "ph": "X", "cat": "fee", "dur": 0.196, "name": "set.add"}, {"pid": 30327, "tid": 30327, "ts": 45578257424.494, "ph": "X", "cat": "fee", "dur": 1.546, "name": "_IterationGuard.__enter__ (/usr/lib/python3.13/_weakrefset.py:21)"}, {"pid": 30327, "tid": 30327, "ts": 45578257418.83, "ph": "X", "cat": "fee", "dur": 7.732, "name": "WeakSet.__iter__ (/usr/lib/python3.13/_weakrefset.py:63)"}, {"pid": 30327, "tid": 30327, "ts": 45578257426.697, "ph": "X", "cat": "fee", "dur": 0.23, "name": "WeakSet.__iter__ (/usr/lib/python3.13/_weakrefset.py:63)"}, {"pid": 30327, "tid": 30327, "ts": 45578257426.99, "ph": "X", "cat": "fee", "dur": 0.168, "name": "WeakSet.__iter__ (/usr/lib/python3.13/_weakrefset.py:63)"}, {"pid": 30327, "tid": 30327, "ts": 45578257427.222, "ph": "X", "cat": "fee", "dur": 0.112, "name": "WeakSet.__iter__ (/usr/lib/python3.13/_weakrefset.py:63)"}, {"pid": 30327, "tid": 30327, "ts": 45578257429.728, "ph": "X", "cat": "fee", "dur": 0.476, "name": "set.remove"}, {"pid": 30327, "tid": 30327, "ts": 45578257431.989, "ph": "X", "cat": "fee", "dur": 0.64, "name": "list.pop"}, {"pid": 30327, "tid": 30327, "ts": 45578257430.99, "ph": "X", "cat": "fee", "dur": 3.082, "name": "WeakSet._commit_removals (/usr/lib/python3.13/_weakrefset.py:53)"}, {"pid": 30327, "tid": 30327, "ts": 45578257428.801, "ph": "X", "cat": "fee", "dur": 5.464, "name": "_IterationGuard.__exit__ (/usr/lib/python3.13/_weakrefset.py:27)"}, {"pid": 30327, "tid": 30327, "ts": 45578257427.389, "ph": "X", "cat": "fee", "dur": 7.309, "name": "WeakSet.__iter__ (/usr/lib/python3.13/_weakrefset.py:63)"}, {"pid": 30327, "tid": 30327, "ts": 45578257438.481, "ph": "X", "cat": "fee", "dur": 0.088, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257438.227, "ph": "X", "cat": "fee", "dur": 0.402, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578257438.885, "ph": "X", "cat": "fee", "dur": 0.113, "name": "_asyncio.Task.done"}, {"pid": 30327, "tid": 30327, "ts": 45578257439.442, "ph": "X", "cat": "fee", "dur": 0.049, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257439.365, "ph": "X", "cat": "fee", "dur": 0.166, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578257439.803, "ph": "X", "cat": "fee", "dur": 0.082, "name": "_asyncio.Task.done"}, {"pid": 30327, "tid": 30327, "ts": 45578257440.215, "ph": "X", "cat": "fee", "dur": 0.053, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257440.087, "ph": "X", "cat": "fee", "dur": 0.218, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578257440.421, "ph": "X", "cat": "fee", "dur": 0.064, "name": "_asyncio.Task.done"}, {"pid": 30327, "tid": 30327, "ts": 45578257440.711, "ph": "X", "cat": "fee", "dur": 0.047, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257440.635, "ph": "X", "cat": "fee", "dur": 0.161, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578257440.89, "ph": "X", "cat": "fee", "dur": 0.063, "name": "_asyncio.Task.done"}, {"pid": 30327, "tid": 30327, "ts": 45578257411.457, "ph": "X", "cat": "fee", "dur": 30.007, "name": "all_tasks (/usr/lib/python3.13/asyncio/tasks.py:44)"}, {"pid": 30327, "tid": 30327, "ts": 45578257409.893, "ph": "X", "cat": "fee", "dur": 31.84, "name": "_cancel_all_tasks (/usr/lib/python3.13/asyncio/runners.py:197)"}, {"pid": 30327, "tid": 30327, "ts": 45578257444.208, "ph": "X", "cat": "fee", "dur": 0.09, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257445.045, "ph": "X", "cat": "fee", "dur": 0.135, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578257445.399, "ph": "X", "cat": "fee", "dur": 0.152, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257444.791, "ph": "X", "cat": "fee", "dur": 0.841, "name": "BaseEventLoop._check_running (/usr/lib/python3.13/asyncio/base_events.py:631)"}, {"pid": 30327, "tid": 30327, "ts": 45578257446.863, "ph": "X", "cat": "fee", "dur": 0.501, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578257446.125, "ph": "X", "cat": "fee", "dur": 1.338, "name": "isfuture (/usr/lib/python3.13/asyncio/base_futures.py:13)"}, {"pid": 30327, "tid": 30327, "ts": 45578257448.816, "ph": "X", "cat": "fee", "dur": 0.113, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578257448.707, "ph": "X", "cat": "fee", "dur": 0.287, "name": "isfuture (/usr/lib/python3.13/asyncio/base_futures.py:13)"}, {"pid": 30327, "tid": 30327, "ts": 45578257449.843, "ph": "X", "cat": "fee", "dur": 0.873, "name": "iscoroutine (/usr/lib/python3.13/asyncio/coroutines.py:32)"}, {"pid": 30327, "tid": 30327, "ts": 45578257452.172, "ph": "X", "cat": "fee", "dur": 0.052, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257453.542, "ph": "X", "cat": "fee", "dur": 0.047, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257454.62, "ph": "X", "cat": "fee", "dur": 0.042, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257455.445, "ph": "X", "cat": "fee", "dur": 0.062, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257455.229, "ph": "X", "cat": "fee", "dur": 0.373, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257455.949, "ph": "X", "cat": "fee", "dur": 0.114, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257454.859, "ph": "X", "cat": "fee", "dur": 1.369, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257454.504, "ph": "X", "cat": "fee", "dur": 1.841, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257457.333, "ph": "X", "cat": "fee", "dur": 0.449, "name": "set.add"}, {"pid": 30327, "tid": 30327, "ts": 45578257456.744, "ph": "X", "cat": "fee", "dur": 1.112, "name": "WeakSet.add (/usr/lib/python3.13/_weakrefset.py:85)"}, {"pid": 30327, "tid": 30327, "ts": 45578257452.05, "ph": "X", "cat": "fee", "dur": 6.658, "name": "BaseEventLoop.create_task (/usr/lib/python3.13/asyncio/base_events.py:462)"}, {"pid": 30327, "tid": 30327, "ts": 45578257448.428, "ph": "X", "cat": "fee", "dur": 10.381, "name": "ensure_future (/usr/lib/python3.13/asyncio/tasks.py:723)"}, {"pid": 30327, "tid": 30327, "ts": 45578257460.201, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_asyncio.Task.add_done_callback"}, {"pid": 30327, "tid": 30327, "ts": 45578257461.269, "ph": "X", "cat": "fee", "dur": 0.057, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257461.558, "ph": "X", "cat": "fee", "dur": 0.061, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578257461.728, "ph": "X", "cat": "fee", "dur": 0.089, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257461.465, "ph": "X", "cat": "fee", "dur": 0.407, "name": "BaseEventLoop._check_running (/usr/lib/python3.13/asyncio/base_events.py:631)"}, {"pid": 30327, "tid": 30327, "ts": 45578257462.219, "ph": "X", "cat": "fee", "dur": 0.523, "name": "BaseEventLoop._set_coroutine_origin_tracking (/usr/lib/python3.13/asyncio/base_events.py:2030)"}, {"pid": 30327, "tid": 30327, "ts": 45578257463.068, "ph": "X", "cat": "fee", "dur": 0.716, "name": "sys.get_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578257464.622, "ph": "X", "cat": "fee", "dur": 0.297, "name": "_thread.get_ident"}, {"pid": 30327, "tid": 30327, "ts": 45578257465.595, "ph": "X", "cat": "fee", "dur": 2.475, "name": "sys.set_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578257469.341, "ph": "X", "cat": "fee", "dur": 0.15, "name": "_asyncio._set_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257461.079, "ph": "X", "cat": "fee", "dur": 8.532, "name": "BaseEventLoop._run_forever_setup (/usr/lib/python3.13/asyncio/base_events.py:638)"}, {"pid": 30327, "tid": 30327, "ts": 45578257470.014, "ph": "X", "cat": "fee", "dur": 0.078, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257470.88, "ph": "X", "cat": "fee", "dur": 0.111, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257471.17, "ph": "X", "cat": "fee", "dur": 0.854, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578257470.697, "ph": "X", "cat": "fee", "dur": 1.57, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257472.554, "ph": "X", "cat": "fee", "dur": 0.096, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257472.989, "ph": "X", "cat": "fee", "dur": 0.135, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257472.876, "ph": "X", "cat": "fee", "dur": 0.311, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257473.364, "ph": "X", "cat": "fee", "dur": 0.087, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257473.878, "ph": "X", "cat": "fee", "dur": 0.106, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329984, "ts": 45578257478.34, "ph": "X", "cat": "fee", "dur": 0.102, "name": "builtins.len"}, {"pid": 30327, "tid": 16329984, "ts": 45578257478.648, "ph": "X", "cat": "fee", "dur": 0.067, "name": "builtins.len"}, {"pid": 30327, "tid": 16329984, "ts": 45578257478.12, "ph": "X", "cat": "fee", "dur": 0.886, "name": "WeakSet.__len__ (/usr/lib/python3.13/_weakrefset.py:72)"}, {"pid": 30327, "tid": 16329984, "ts": 45578257477.792, "ph": "X", "cat": "fee", "dur": 1.424, "name": "builtins.len"}, {"pid": 30327, "tid": 16329984, "ts": 45578257477.065, "ph": "X", "cat": "fee", "dur": 2.335, "name": "BaseEventLoop.shutdown_asyncgens (/usr/lib/python3.13/asyncio/base_events.py:572)"}, {"pid": 30327, "tid": 30327, "ts": 45578257480.048, "ph": "X", "cat": "fee", "dur": 0.061, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257480.815, "ph": "X", "cat": "fee", "dur": 0.05, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257480.594, "ph": "X", "cat": "fee", "dur": 0.358, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257481.179, "ph": "X", "cat": "fee", "dur": 0.105, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257480.268, "ph": "X", "cat": "fee", "dur": 1.097, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257479.925, "ph": "X", "cat": "fee", "dur": 1.529, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257475.235, "ph": "X", "cat": "fee", "dur": 6.568, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257474.249, "ph": "X", "cat": "fee", "dur": 7.711, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257469.912, "ph": "X", "cat": "fee", "dur": 12.51, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257482.742, "ph": "X", "cat": "fee", "dur": 0.045, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257483.141, "ph": "X", "cat": "fee", "dur": 0.075, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257483.359, "ph": "X", "cat": "fee", "dur": 0.422, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578257483.025, "ph": "X", "cat": "fee", "dur": 0.927, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257484.1, "ph": "X", "cat": "fee", "dur": 0.084, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257484.442, "ph": "X", "cat": "fee", "dur": 0.112, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257484.355, "ph": "X", "cat": "fee", "dur": 0.246, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257484.738, "ph": "X", "cat": "fee", "dur": 0.054, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257485.081, "ph": "X", "cat": "fee", "dur": 0.088, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 30327, "ts": 45578257485.937, "ph": "X", "cat": "fee", "dur": 0.103, "name": "_asyncio.Task.cancelled"}, {"pid": 30327, "tid": 30327, "ts": 45578257486.253, "ph": "X", "cat": "fee", "dur": 0.12, "name": "_asyncio.Task.exception"}, {"pid": 30327, "tid": 30327, "ts": 45578257486.808, "ph": "X", "cat": "fee", "dur": 0.237, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257487.562, "ph": "X", "cat": "fee", "dur": 0.051, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257487.442, "ph": "X", "cat": "fee", "dur": 0.236, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578257487.929, "ph": "X", "cat": "fee", "dur": 0.185, "name": "BaseEventLoop.stop (/usr/lib/python3.13/asyncio/base_events.py:723)"}, {"pid": 30327, "tid": 30327, "ts": 45578257485.773, "ph": "X", "cat": "fee", "dur": 2.412, "name": "_run_until_complete_cb (/usr/lib/python3.13/asyncio/base_events.py:182)"}, {"pid": 30327, "tid": 30327, "ts": 45578257485.624, "ph": "X", "cat": "fee", "dur": 2.633, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257485.327, "ph": "X", "cat": "fee", "dur": 3.009, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257482.681, "ph": "X", "cat": "fee", "dur": 5.924, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257489.439, "ph": "X", "cat": "fee", "dur": 0.119, "name": "_asyncio._set_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257489.765, "ph": "X", "cat": "fee", "dur": 0.266, "name": "BaseEventLoop._set_coroutine_origin_tracking (/usr/lib/python3.13/asyncio/base_events.py:2030)"}, {"pid": 30327, "tid": 30327, "ts": 45578257491.627, "ph": "X", "cat": "fee", "dur": 0.708, "name": "sys.set_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578257489.075, "ph": "X", "cat": "fee", "dur": 3.474, "name": "BaseEventLoop._run_forever_cleanup (/usr/lib/python3.13/asyncio/base_events.py:658)"}, {"pid": 30327, "tid": 30327, "ts": 45578257460.749, "ph": "X", "cat": "fee", "dur": 31.86, "name": "BaseEventLoop.run_forever (/usr/lib/python3.13/asyncio/base_events.py:674)"}, {"pid": 30327, "tid": 30327, "ts": 45578257492.877, "ph": "X", "cat": "fee", "dur": 0.147, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 30327, "tid": 30327, "ts": 45578257493.173, "ph": "X", "cat": "fee", "dur": 0.082, "name": "_asyncio.Task.done"}, {"pid": 30327, "tid": 30327, "ts": 45578257493.411, "ph": "X", "cat": "fee", "dur": 0.128, "name": "_asyncio.Task.result"}, {"pid": 30327, "tid": 30327, "ts": 45578257443.255, "ph": "X", "cat": "fee", "dur": 50.382, "name": "BaseEventLoop.run_until_complete (/usr/lib/python3.13/asyncio/base_events.py:685)"}, {"pid": 30327, "tid": 30327, "ts": 45578257495.303, "ph": "X", "cat": "fee", "dur": 0.061, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257495.581, "ph": "X", "cat": "fee", "dur": 0.069, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578257495.773, "ph": "X", "cat": "fee", "dur": 0.101, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257495.484, "ph": "X", "cat": "fee", "dur": 0.453, "name": "BaseEventLoop._check_running (/usr/lib/python3.13/asyncio/base_events.py:631)"}, {"pid": 30327, "tid": 30327, "ts": 45578257496.261, "ph": "X", "cat": "fee", "dur": 0.162, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578257496.118, "ph": "X", "cat": "fee", "dur": 0.415, "name": "isfuture (/usr/lib/python3.13/asyncio/base_futures.py:13)"}, {"pid": 30327, "tid": 30327, "ts": 45578257497.094, "ph": "X", "cat": "fee", "dur": 0.057, "name": "builtins.hasattr"}, {"pid": 30327, "tid": 30327, "ts": 45578257497.017, "ph": "X", "cat": "fee", "dur": 0.185, "name": "isfuture (/usr/lib/python3.13/asyncio/base_futures.py:13)"}, {"pid": 30327, "tid": 30327, "ts": 45578257497.504, "ph": "X", "cat": "fee", "dur": 0.247, "name": "iscoroutine (/usr/lib/python3.13/asyncio/coroutines.py:32)"}, {"pid": 30327, "tid": 30327, "ts": 45578257498.188, "ph": "X", "cat": "fee", "dur": 0.067, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257498.866, "ph": "X", "cat": "fee", "dur": 0.046, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257499.443, "ph": "X", "cat": "fee", "dur": 0.045, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257500.092, "ph": "X", "cat": "fee", "dur": 0.05, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257499.889, "ph": "X", "cat": "fee", "dur": 0.35, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257500.507, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257499.635, "ph": "X", "cat": "fee", "dur": 1.037, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257499.331, "ph": "X", "cat": "fee", "dur": 1.451, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257501.35, "ph": "X", "cat": "fee", "dur": 0.121, "name": "set.add"}, {"pid": 30327, "tid": 30327, "ts": 45578257501.002, "ph": "X", "cat": "fee", "dur": 0.549, "name": "WeakSet.add (/usr/lib/python3.13/_weakrefset.py:85)"}, {"pid": 30327, "tid": 30327, "ts": 45578257498.086, "ph": "X", "cat": "fee", "dur": 3.818, "name": "BaseEventLoop.create_task (/usr/lib/python3.13/asyncio/base_events.py:462)"}, {"pid": 30327, "tid": 30327, "ts": 45578257496.889, "ph": "X", "cat": "fee", "dur": 5.083, "name": "ensure_future (/usr/lib/python3.13/asyncio/tasks.py:723)"}, {"pid": 30327, "tid": 30327, "ts": 45578257502.486, "ph": "X", "cat": "fee", "dur": 0.183, "name": "_asyncio.Task.add_done_callback"}, {"pid": 30327, "tid": 30327, "ts": 45578257503.049, "ph": "X", "cat": "fee", "dur": 0.044, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257503.288, "ph": "X", "cat": "fee", "dur": 0.076, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578257503.469, "ph": "X", "cat": "fee", "dur": 0.068, "name": "_asyncio._get_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257503.203, "ph": "X", "cat": "fee", "dur": 0.384, "name": "BaseEventLoop._check_running (/usr/lib/python3.13/asyncio/base_events.py:631)"}, {"pid": 30327, "tid": 30327, "ts": 45578257503.744, "ph": "X", "cat": "fee", "dur": 0.3, "name": "BaseEventLoop._set_coroutine_origin_tracking (/usr/lib/python3.13/asyncio/base_events.py:2030)"}, {"pid": 30327, "tid": 30327, "ts": 45578257504.166, "ph": "X", "cat": "fee", "dur": 0.227, "name": "sys.get_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578257504.535, "ph": "X", "cat": "fee", "dur": 0.125, "name": "_thread.get_ident"}, {"pid": 30327, "tid": 30327, "ts": 45578257504.934, "ph": "X", "cat": "fee", "dur": 0.862, "name": "sys.set_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578257505.942, "ph": "X", "cat": "fee", "dur": 0.127, "name": "_asyncio._set_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257502.959, "ph": "X", "cat": "fee", "dur": 3.189, "name": "BaseEventLoop._run_forever_setup (/usr/lib/python3.13/asyncio/base_events.py:638)"}, {"pid": 30327, "tid": 30327, "ts": 45578257506.445, "ph": "X", "cat": "fee", "dur": 0.118, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257506.964, "ph": "X", "cat": "fee", "dur": 0.08, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257507.178, "ph": "X", "cat": "fee", "dur": 0.543, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578257506.852, "ph": "X", "cat": "fee", "dur": 1.074, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257508.202, "ph": "X", "cat": "fee", "dur": 0.074, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257508.514, "ph": "X", "cat": "fee", "dur": 0.134, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257508.428, "ph": "X", "cat": "fee", "dur": 0.265, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257508.824, "ph": "X", "cat": "fee", "dur": 0.055, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257510.231, "ph": "X", "cat": "fee", "dur": 0.097, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 16329808, "ts": 45578257512.603, "ph": "X", "cat": "fee", "dur": 0.363, "name": "BaseEventLoop.shutdown_default_executor (/usr/lib/python3.13/asyncio/base_events.py:597)"}, {"pid": 30327, "tid": 30327, "ts": 45578257513.475, "ph": "X", "cat": "fee", "dur": 0.052, "name": "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)"}, {"pid": 30327, "tid": 30327, "ts": 45578257514.078, "ph": "X", "cat": "fee", "dur": 0.059, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257513.915, "ph": "X", "cat": "fee", "dur": 0.313, "name": "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)"}, {"pid": 30327, "tid": 30327, "ts": 45578257514.429, "ph": "X", "cat": "fee", "dur": 0.096, "name": "collections.deque.append"}, {"pid": 30327, "tid": 30327, "ts": 45578257513.675, "ph": "X", "cat": "fee", "dur": 0.958, "name": "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)"}, {"pid": 30327, "tid": 30327, "ts": 45578257513.362, "ph": "X", "cat": "fee", "dur": 1.365, "name": "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)"}, {"pid": 30327, "tid": 30327, "ts": 45578257510.79, "ph": "X", "cat": "fee", "dur": 4.22, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257510.528, "ph": "X", "cat": "fee", "dur": 4.585, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257506.366, "ph": "X", "cat": "fee", "dur": 9.023, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257515.693, "ph": "X", "cat": "fee", "dur": 0.091, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257516.082, "ph": "X", "cat": "fee", "dur": 0.048, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257516.235, "ph": "X", "cat": "fee", "dur": 0.427, "name": "select.epoll.poll"}, {"pid": 30327, "tid": 30327, "ts": 45578257516.015, "ph": "X", "cat": "fee", "dur": 0.801, "name": "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)"}, {"pid": 30327, "tid": 30327, "ts": 45578257516.963, "ph": "X", "cat": "fee", "dur": 0.092, "name": "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)"}, {"pid": 30327, "tid": 30327, "ts": 45578257517.283, "ph": "X", "cat": "fee", "dur": 0.097, "name": "time.monotonic"}, {"pid": 30327, "tid": 30327, "ts": 45578257517.186, "ph": "X", "cat": "fee", "dur": 0.237, "name": "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)"}, {"pid": 30327, "tid": 30327, "ts": 45578257517.535, "ph": "X", "cat": "fee", "dur": 0.059, "name": "builtins.len"}, {"pid": 30327, "tid": 30327, "ts": 45578257517.838, "ph": "X", "cat": "fee", "dur": 0.065, "name": "collections.deque.popleft"}, {"pid": 30327, "tid": 30327, "ts": 45578257518.481, "ph": "X", "cat": "fee", "dur": 0.074, "name": "_asyncio.Task.cancelled"}, {"pid": 30327, "tid": 30327, "ts": 45578257518.723, "ph": "X", "cat": "fee", "dur": 0.079, "name": "_asyncio.Task.exception"}, {"pid": 30327, "tid": 30327, "ts": 45578257518.99, "ph": "X", "cat": "fee", "dur": 0.158, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257519.489, "ph": "X", "cat": "fee", "dur": 0.067, "name": "_asyncio.Task.get_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257519.358, "ph": "X", "cat": "fee", "dur": 0.236, "name": "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)"}, {"pid": 30327, "tid": 30327, "ts": 45578257519.769, "ph": "X", "cat": "fee", "dur": 0.078, "name": "BaseEventLoop.stop (/usr/lib/python3.13/asyncio/base_events.py:723)"}, {"pid": 30327, "tid": 30327, "ts": 45578257518.399, "ph": "X", "cat": "fee", "dur": 1.505, "name": "_run_until_complete_cb (/usr/lib/python3.13/asyncio/base_events.py:182)"}, {"pid": 30327, "tid": 30327, "ts": 45578257518.268, "ph": "X", "cat": "fee", "dur": 1.705, "name": "_contextvars.Context.run"}, {"pid": 30327, "tid": 30327, "ts": 45578257518.043, "ph": "X", "cat": "fee", "dur": 1.993, "name": "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)"}, {"pid": 30327, "tid": 30327, "ts": 45578257515.623, "ph": "X", "cat": "fee", "dur": 4.61, "name": "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)"}, {"pid": 30327, "tid": 30327, "ts": 45578257520.582, "ph": "X", "cat": "fee", "dur": 0.113, "name": "_asyncio._set_running_loop"}, {"pid": 30327, "tid": 30327, "ts": 45578257520.861, "ph": "X", "cat": "fee", "dur": 0.219, "name": "BaseEventLoop._set_coroutine_origin_tracking (/usr/lib/python3.13/asyncio/base_events.py:2030)"}, {"pid": 30327, "tid": 30327, "ts": 45578257521.377, "ph": "X", "cat": "fee", "dur": 0.574, "name": "sys.set_asyncgen_hooks"}, {"pid": 30327, "tid": 30327, "ts": 45578257520.433, "ph": "X", "cat": "fee", "dur": 1.664, "name": "BaseEventLoop._run_forever_cleanup (/usr/lib/python3.13/asyncio/base_events.py:658)"}, {"pid": 30327, "tid": 30327, "ts": 45578257502.852, "ph": "X", "cat": "fee", "dur": 19.301, "name": "BaseEventLoop.run_forever (/usr/lib/python3.13/asyncio/base_events.py:674)"}, {"pid": 30327, "tid": 30327, "ts": 45578257522.313, "ph": "X", "cat": "fee", "dur": 0.148, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 30327, "tid": 30327, "ts": 45578257522.591, "ph": "X", "cat": "fee", "dur": 0.076, "name": "_asyncio.Task.done"}, {"pid": 30327, "tid": 30327, "ts": 45578257522.781, "ph": "X", "cat": "fee", "dur": 0.086, "name": "_asyncio.Task.result"}, {"pid": 30327, "tid": 30327, "ts": 45578257495.186, "ph": "X", "cat": "fee", "dur": 27.769, "name": "BaseEventLoop.run_until_complete (/usr/lib/python3.13/asyncio/base_events.py:685)"}, {"pid": 30327, "tid": 30327, "ts": 45578257524.26, "ph": "X", "cat": "fee", "dur": 0.103, "name": "get_event_loop_policy (/usr/lib/python3.13/asyncio/events.py:797)"}, {"pid": 30327, "tid": 30327, "ts": 45578257526.185, "ph": "X", "cat": "fee", "dur": 2.24, "name": "BaseDefaultEventLoopPolicy.set_event_loop (/usr/lib/python3.13/asyncio/events.py:721)"}, {"pid": 30327, "tid": 30327, "ts": 45578257524.96, "ph": "X", "cat": "fee", "dur": 3.683, "name": "_UnixDefaultEventLoopPolicy.set_event_loop (/usr/lib/python3.13/asyncio/unix_events.py:1493)"}, {"pid": 30327, "tid": 30327, "ts": 45578257523.849, "ph": "X", "cat": "fee", "dur": 4.936, "name": "set_event_loop (/usr/lib/python3.13/asyncio/events.py:830)"}, {"pid": 30327, "tid": 30327, "ts": 45578257532.051, "ph": "X", "cat": "fee", "dur": 0.065, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578257532.656, "ph": "X", "cat": "fee", "dur": 0.222, "name": "BaseEventLoop.is_closed (/usr/lib/python3.13/asyncio/base_events.py:754)"}, {"pid": 30327, "tid": 30327, "ts": 45578257534.639, "ph": "X", "cat": "fee", "dur": 0.254, "name": "socket.fileno"}, {"pid": 30327, "tid": 30327, "ts": 45578257536.999, "ph": "X", "cat": "fee", "dur": 0.082, "name": "BaseEventLoop.is_closed (/usr/lib/python3.13/asyncio/base_events.py:754)"}, {"pid": 30327, "tid": 30327, "ts": 45578257538.257, "ph": "X", "cat": "fee", "dur": 0.291, "name": "_BaseSelectorImpl.get_map (/usr/lib/python3.13/selectors.py:276)"}, {"pid": 30327, "tid": 30327, "ts": 45578257540.126, "ph": "X", "cat": "fee", "dur": 0.116, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257540.028, "ph": "X", "cat": "fee", "dur": 0.387, "name": "_fileobj_to_fd (/usr/lib/python3.13/selectors.py:21)"}, {"pid": 30327, "tid": 30327, "ts": 45578257539.608, "ph": "X", "cat": "fee", "dur": 0.883, "name": "_BaseSelectorImpl._fileobj_lookup (/usr/lib/python3.13/selectors.py:219)"}, {"pid": 30327, "tid": 30327, "ts": 45578257540.931, "ph": "X", "cat": "fee", "dur": 0.401, "name": "dict.get"}, {"pid": 30327, "tid": 30327, "ts": 45578257539.143, "ph": "X", "cat": "fee", "dur": 2.285, "name": "_SelectorMapping.get (/usr/lib/python3.13/selectors.py:69)"}, {"pid": 30327, "tid": 30327, "ts": 45578257548.636, "ph": "X", "cat": "fee", "dur": 0.081, "name": "builtins.isinstance"}, {"pid": 30327, "tid": 30327, "ts": 45578257548.566, "ph": "X", "cat": "fee", "dur": 0.227, "name": "_fileobj_to_fd (/usr/lib/python3.13/selectors.py:21)"}, {"pid": 30327, "tid": 30327, "ts": 45578257548.442, "ph": "X", "cat": "fee", "dur": 0.412, "name": "_BaseSelectorImpl._fileobj_lookup (/usr/lib/python3.13/selectors.py:219)"}, {"pid": 30327, "tid": 30327, "ts": 45578257548.967, "ph": "X", "cat": "fee", "dur": 0.377, "name": "dict.pop"}, {"pid": 30327, "tid": 30327, "ts": 45578257547.726, "ph": "X", "cat": "fee", "dur": 1.733, "name": "_BaseSelectorImpl.unregister (/usr/lib/python3.13/selectors.py:251)"}, {"pid": 30327, "tid": 30327, "ts": 45578257550.114, "ph": "X", "cat": "fee", "dur": 2.738, "name": "select.epoll.unregister"}, {"pid": 30327, "tid": 30327, "ts": 45578257546.325, "ph": "X", "cat": "fee", "dur": 6.635, "name": "_PollLikeSelector.unregister (/usr/lib/python3.13/selectors.py:351)"}, {"pid": 30327, "tid": 30327, "ts": 45578257554.269, "ph": "X", "cat": "fee", "dur": 0.054, "name": "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)"}, {"pid": 30327, "tid": 30327, "ts": 45578257553.69, "ph": "X", "cat": "fee", "dur": 0.933, "name": "Handle.cancel (/usr/lib/python3.13/asyncio/events.py:73)"}, {"pid": 30327, "tid": 30327, "ts": 45578257536.815, "ph": "X", "cat": "fee", "dur": 17.918, "name": "BaseSelectorEventLoop._remove_reader (/usr/lib/python3.13/asyncio/selector_events.py:289)"}, {"pid": 30327, "tid": 30327, "ts": 45578257558.544, "ph": "X", "cat": "fee", "dur": 6.971, "name": "socket.close"}, {"pid": 30327, "tid": 30327, "ts": 45578257557.876, "ph": "X", "cat": "fee", "dur": 7.893, "name": "socket._real_close (/usr/lib/python3.13/socket.py:497)"}, {"pid": 30327, "tid": 30327, "ts": 45578257556.515, "ph": "X", "cat": "fee", "dur": 9.327, "name": "socket.close (/usr/lib/python3.13/socket.py:501)"}, {"pid": 30327, "tid": 30327, "ts": 45578257567.589, "ph": "X", "cat": "fee", "dur": 3.104, "name": "socket.close"}, {"pid": 30327, "tid": 30327, "ts": 45578257567.345, "ph": "X", "cat": "fee", "dur": 3.419, "name": "socket._real_close (/usr/lib/python3.13/socket.py:497)"}, {"pid": 30327, "tid": 30327, "ts": 45578257566.82, "ph": "X", "cat": "fee", "dur": 4.009, "name": "socket.close (/usr/lib/python3.13/socket.py:501)"}, {"pid": 30327, "tid": 30327, "ts": 45578257533.635, "ph": "X", "cat": "fee", "dur": 37.657, "name": "BaseSelectorEventLoop._close_self_pipe (/usr/lib/python3.13/asyncio/selector_events.py:110)"}, {"pid": 30327, "tid": 30327, "ts": 45578257572.984, "ph": "X", "cat": "fee", "dur": 0.074, "name": "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)"}, {"pid": 30327, "tid": 30327, "ts": 45578257573.72, "ph": "X", "cat": "fee", "dur": 0.35, "name": "collections.deque.clear"}, {"pid": 30327, "tid": 30327, "ts": 45578257574.412, "ph": "X", "cat": "fee", "dur": 0.206, "name": "list.clear"}, {"pid": 30327, "tid": 30327, "ts": 45578257572.74, "ph": "X", "cat": "fee", "dur": 2.101, "name": "BaseEventLoop.close (/usr/lib/python3.13/asyncio/base_events.py:731)"}, {"pid": 30327, "tid": 30327, "ts": 45578257576.0, "ph": "X", "cat": "fee", "dur": 1.94, "name": "select.epoll.close"}, {"pid": 30327, "tid": 30327, "ts": 45578257579.137, "ph": "X", "cat": "fee", "dur": 0.328, "name": "dict.clear"}, {"pid": 30327, "tid": 30327, "ts": 45578257578.886, "ph": "X", "cat": "fee", "dur": 1.324, "name": "_BaseSelectorImpl.close (/usr/lib/python3.13/selectors.py:272)"}, {"pid": 30327, "tid": 30327, "ts": 45578257575.681, "ph": "X", "cat": "fee", "dur": 4.649, "name": "EpollSelector.close (/usr/lib/python3.13/selectors.py:465)"}, {"pid": 30327, "tid": 30327, "ts": 45578257531.797, "ph": "X", "cat": "fee", "dur": 49.448, "name": "BaseSelectorEventLoop.close (/usr/lib/python3.13/asyncio/selector_events.py:99)"}, {"pid": 30327, "tid": 30327, "ts": 45578257581.814, "ph": "X", "cat": "fee", "dur": 0.367, "name": "sys.is_finalizing"}, {"pid": 30327, "tid": 30327, "ts": 45578257530.599, "ph": "X", "cat": "fee", "dur": 52.494, "name": "_UnixSelectorEventLoop.close (/usr/lib/python3.13/asyncio/unix_events.py:69)"}, {"pid": 30327, "tid": 30327, "ts": 45578257407.647, "ph": "X", "cat": "fee", "dur": 176.04, "name": "Runner.close (/usr/lib/python3.13/asyncio/runners.py:64)"}, {"pid": 30327, "tid": 30327, "ts": 45578257406.118, "ph": "X", "cat": "fee", "dur": 177.708, "name": "Runner.__exit__ (/usr/lib/python3.13/asyncio/runners.py:61)"}, {"pid": 30327, "tid": 30327, "ts": 45578246651.07, "ph": "X", "cat": "fee", "dur": 10933.09, "name": "run (/usr/lib/python3.13/asyncio/runners.py:160)"}, {"pid": 30327, "tid": 30327, "ts": 45578246637.586, "ph": "X", "cat": "fee", "dur": 10947.195, "name": "<module> (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:1)"}, {"pid": 30327, "tid": 30327, "ts": 45578246635.751, "ph": "X", "cat": "fee", "dur": 10949.872, "name": "builtins.exec"}, {"ph": "M", "pid": 30327, "tid": 16328928, "name": "thread_name", "args": {"name": "Task-1"}}, {"ph": "M", "pid": 30327, "tid": 16329104, "name": "thread_name", "args": {"name": "Task-2"}}, {"ph": "M", "pid": 30327, "tid": 16329280, "name": "thread_name", "args": {"name": "Task-3"}}, {"ph": "M", "pid": 30327, "tid": 16329456, "name": "thread_name", "args": {"name": "Task-4"}}, {"ph": "M", "pid": 30327, "tid": 16329984, "name": "thread_name", "args": {"name": "Task-5"}}, {"ph": "M", "pid": 30327, "tid": 16329808, "name": "thread_name", "args": {"name": "Task-6"}}], "viztracer_metadata": {"overflow": false, "version": "0.17.1"}, "file_info": {"files": {"/usr/lib/python3.13/asyncio/runners.py": ["__all__ = ('Runner', 'run')\n\nimport contextvars\nimport enum\nimport functools\nimport threading\nimport signal\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import tasks\nfrom . import constants\n\nclass _State(enum.Enum):\n    CREATED = \"created\"\n    INITIALIZED = \"initialized\"\n    CLOSED = \"closed\"\n\n\nclass Runner:\n    \"\"\"A context manager that controls event loop life cycle.\n\n    The context manager always creates a new event loop,\n    allows to run async functions inside it,\n    and properly finalizes the loop at the context manager exit.\n\n    If debug is True, the event loop will be run in debug mode.\n    If loop_factory is passed, it is used for new event loop creation.\n\n    asyncio.run(main(), debug=True)\n\n    is a shortcut for\n\n    with asyncio.Runner(debug=True) as runner:\n        runner.run(main())\n\n    The run() method can be called multiple times within the runner's context.\n\n    This can be useful for interactive console (e.g. IPython),\n    unittest runners, console tools, -- everywhere when async code\n    is called from existing sync framework and where the preferred single\n    asyncio.run() call doesn't work.\n\n    \"\"\"\n\n    # Note: the class is final, it is not intended for inheritance.\n\n    def __init__(self, *, debug=None, loop_factory=None):\n        self._state = _State.CREATED\n        self._debug = debug\n        self._loop_factory = loop_factory\n        self._loop = None\n        self._context = None\n        self._interrupt_count = 0\n        self._set_event_loop = False\n\n    def __enter__(self):\n        self._lazy_init()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        \"\"\"Shutdown and close event loop.\"\"\"\n        if self._state is not _State.INITIALIZED:\n            return\n        try:\n            loop = self._loop\n            _cancel_all_tasks(loop)\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            loop.run_until_complete(\n                loop.shutdown_default_executor(constants.THREAD_JOIN_TIMEOUT))\n        finally:\n            if self._set_event_loop:\n                events.set_event_loop(None)\n            loop.close()\n            self._loop = None\n            self._state = _State.CLOSED\n\n    def get_loop(self):\n        \"\"\"Return embedded event loop.\"\"\"\n        self._lazy_init()\n        return self._loop\n\n    def run(self, coro, *, context=None):\n        \"\"\"Run a coroutine inside the embedded event loop.\"\"\"\n        if not coroutines.iscoroutine(coro):\n            raise ValueError(\"a coroutine was expected, got {!r}\".format(coro))\n\n        if events._get_running_loop() is not None:\n            # fail fast with short traceback\n            raise RuntimeError(\n                \"Runner.run() cannot be called from a running event loop\")\n\n        self._lazy_init()\n\n        if context is None:\n            context = self._context\n        task = self._loop.create_task(coro, context=context)\n\n        if (threading.current_thread() is threading.main_thread()\n            and signal.getsignal(signal.SIGINT) is signal.default_int_handler\n        ):\n            sigint_handler = functools.partial(self._on_sigint, main_task=task)\n            try:\n                signal.signal(signal.SIGINT, sigint_handler)\n            except ValueError:\n                # `signal.signal` may throw if `threading.main_thread` does\n                # not support signals (e.g. embedded interpreter with signals\n                # not registered - see gh-91880)\n                sigint_handler = None\n        else:\n            sigint_handler = None\n\n        self._interrupt_count = 0\n        try:\n            return self._loop.run_until_complete(task)\n        except exceptions.CancelledError:\n            if self._interrupt_count > 0:\n                uncancel = getattr(task, \"uncancel\", None)\n                if uncancel is not None and uncancel() == 0:\n                    raise KeyboardInterrupt()\n            raise  # CancelledError\n        finally:\n            if (sigint_handler is not None\n                and signal.getsignal(signal.SIGINT) is sigint_handler\n            ):\n                signal.signal(signal.SIGINT, signal.default_int_handler)\n\n    def _lazy_init(self):\n        if self._state is _State.CLOSED:\n            raise RuntimeError(\"Runner is closed\")\n        if self._state is _State.INITIALIZED:\n            return\n        if self._loop_factory is None:\n            self._loop = events.new_event_loop()\n            if not self._set_event_loop:\n                # Call set_event_loop only once to avoid calling\n                # attach_loop multiple times on child watchers\n                events.set_event_loop(self._loop)\n                self._set_event_loop = True\n        else:\n            self._loop = self._loop_factory()\n        if self._debug is not None:\n            self._loop.set_debug(self._debug)\n        self._context = contextvars.copy_context()\n        self._state = _State.INITIALIZED\n\n    def _on_sigint(self, signum, frame, main_task):\n        self._interrupt_count += 1\n        if self._interrupt_count == 1 and not main_task.done():\n            main_task.cancel()\n            # wakeup loop if it is blocked by select() with long timeout\n            self._loop.call_soon_threadsafe(lambda: None)\n            return\n        raise KeyboardInterrupt()\n\n\ndef run(main, *, debug=None, loop_factory=None):\n    \"\"\"Execute the coroutine and return the result.\n\n    This function runs the passed coroutine, taking care of\n    managing the asyncio event loop, finalizing asynchronous\n    generators and closing the default executor.\n\n    This function cannot be called when another asyncio event loop is\n    running in the same thread.\n\n    If debug is True, the event loop will be run in debug mode.\n\n    This function always creates a new event loop and closes it at the end.\n    It should be used as a main entry point for asyncio programs, and should\n    ideally only be called once.\n\n    The executor is given a timeout duration of 5 minutes to shutdown.\n    If the executor hasn't finished within that duration, a warning is\n    emitted and the executor is closed.\n\n    Example:\n\n        async def main():\n            await asyncio.sleep(1)\n            print('hello')\n\n        asyncio.run(main())\n    \"\"\"\n    if events._get_running_loop() is not None:\n        # fail fast with short traceback\n        raise RuntimeError(\n            \"asyncio.run() cannot be called from a running event loop\")\n\n    with Runner(debug=debug, loop_factory=loop_factory) as runner:\n        return runner.run(main)\n\n\ndef _cancel_all_tasks(loop):\n    to_cancel = tasks.all_tasks(loop)\n    if not to_cancel:\n        return\n\n    for task in to_cancel:\n        task.cancel()\n\n    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\n\n    for task in to_cancel:\n        if task.cancelled():\n            continue\n        if task.exception() is not None:\n            loop.call_exception_handler({\n                'message': 'unhandled exception during asyncio.run() shutdown',\n                'exception': task.exception(),\n                'task': task,\n            })\n", 215], "/usr/lib/python3.13/asyncio/events.py": ["\"\"\"Event loop and event loop policy.\"\"\"\n\n# Contains code from https://github.com/MagicStack/uvloop/tree/v0.16.0\n# SPDX-License-Identifier: PSF-2.0 AND (MIT OR Apache-2.0)\n# SPDX-FileCopyrightText: Copyright (c) 2015-2021 MagicStack Inc.  http://magic.io\n\n__all__ = (\n    'AbstractEventLoopPolicy',\n    'AbstractEventLoop', 'AbstractServer',\n    'Handle', 'TimerHandle',\n    'get_event_loop_policy', 'set_event_loop_policy',\n    'get_event_loop', 'set_event_loop', 'new_event_loop',\n    'get_child_watcher', 'set_child_watcher',\n    '_set_running_loop', 'get_running_loop',\n    '_get_running_loop',\n)\n\nimport contextvars\nimport os\nimport signal\nimport socket\nimport subprocess\nimport sys\nimport threading\n\nfrom . import format_helpers\n\n\nclass Handle:\n    \"\"\"Object returned by callback registration methods.\"\"\"\n\n    __slots__ = ('_callback', '_args', '_cancelled', '_loop',\n                 '_source_traceback', '_repr', '__weakref__',\n                 '_context')\n\n    def __init__(self, callback, args, loop, context=None):\n        if context is None:\n            context = contextvars.copy_context()\n        self._context = context\n        self._loop = loop\n        self._callback = callback\n        self._args = args\n        self._cancelled = False\n        self._repr = None\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n        else:\n            self._source_traceback = None\n\n    def _repr_info(self):\n        info = [self.__class__.__name__]\n        if self._cancelled:\n            info.append('cancelled')\n        if self._callback is not None:\n            info.append(format_helpers._format_callback_source(\n                self._callback, self._args,\n                debug=self._loop.get_debug()))\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            info.append(f'created at {frame[0]}:{frame[1]}')\n        return info\n\n    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n        info = self._repr_info()\n        return '<{}>'.format(' '.join(info))\n\n    def get_context(self):\n        return self._context\n\n    def cancel(self):\n        if not self._cancelled:\n            self._cancelled = True\n            if self._loop.get_debug():\n                # Keep a representation in debug mode to keep callback and\n                # parameters. For example, to log the warning\n                # \"Executing <Handle...> took 2.5 second\"\n                self._repr = repr(self)\n            self._callback = None\n            self._args = None\n\n    def cancelled(self):\n        return self._cancelled\n\n    def _run(self):\n        try:\n            self._context.run(self._callback, *self._args)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            cb = format_helpers._format_callback_source(\n                self._callback, self._args,\n                debug=self._loop.get_debug())\n            msg = f'Exception in callback {cb}'\n            context = {\n                'message': msg,\n                'exception': exc,\n                'handle': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self = None  # Needed to break cycles when an exception occurs.\n\n\nclass TimerHandle(Handle):\n    \"\"\"Object returned by timed callback registration methods.\"\"\"\n\n    __slots__ = ['_scheduled', '_when']\n\n    def __init__(self, when, callback, args, loop, context=None):\n        super().__init__(callback, args, loop, context)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._when = when\n        self._scheduled = False\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        pos = 2 if self._cancelled else 1\n        info.insert(pos, f'when={self._when}')\n        return info\n\n    def __hash__(self):\n        return hash(self._when)\n\n    def __lt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, TimerHandle):\n            return (self._when == other._when and\n                    self._callback == other._callback and\n                    self._args == other._args and\n                    self._cancelled == other._cancelled)\n        return NotImplemented\n\n    def cancel(self):\n        if not self._cancelled:\n            self._loop._timer_handle_cancelled(self)\n        super().cancel()\n\n    def when(self):\n        \"\"\"Return a scheduled callback time.\n\n        The time is an absolute timestamp, using the same time\n        reference as loop.time().\n        \"\"\"\n        return self._when\n\n\nclass AbstractServer:\n    \"\"\"Abstract server returned by create_server().\"\"\"\n\n    def close(self):\n        \"\"\"Stop serving.  This leaves existing connections open.\"\"\"\n        raise NotImplementedError\n\n    def close_clients(self):\n        \"\"\"Close all active connections.\"\"\"\n        raise NotImplementedError\n\n    def abort_clients(self):\n        \"\"\"Close all active connections immediately.\"\"\"\n        raise NotImplementedError\n\n    def get_loop(self):\n        \"\"\"Get the event loop the Server object is attached to.\"\"\"\n        raise NotImplementedError\n\n    def is_serving(self):\n        \"\"\"Return True if the server is accepting connections.\"\"\"\n        raise NotImplementedError\n\n    async def start_serving(self):\n        \"\"\"Start accepting connections.\n\n        This method is idempotent, so it can be called when\n        the server is already being serving.\n        \"\"\"\n        raise NotImplementedError\n\n    async def serve_forever(self):\n        \"\"\"Start accepting connections until the coroutine is cancelled.\n\n        The server is closed when the coroutine is cancelled.\n        \"\"\"\n        raise NotImplementedError\n\n    async def wait_closed(self):\n        \"\"\"Coroutine to wait until service is closed.\"\"\"\n        raise NotImplementedError\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        self.close()\n        await self.wait_closed()\n\n\nclass AbstractEventLoop:\n    \"\"\"Abstract event loop.\"\"\"\n\n    # Running and stopping the event loop.\n\n    def run_forever(self):\n        \"\"\"Run the event loop until stop() is called.\"\"\"\n        raise NotImplementedError\n\n    def run_until_complete(self, future):\n        \"\"\"Run the event loop until a Future is done.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        raise NotImplementedError\n\n    def stop(self):\n        \"\"\"Stop the event loop as soon as reasonable.\n\n        Exactly how soon that is may depend on the implementation, but\n        no more I/O callbacks should be scheduled.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_running(self):\n        \"\"\"Return whether the event loop is currently running.\"\"\"\n        raise NotImplementedError\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the loop.\n\n        The loop should not be running.\n\n        This is idempotent and irreversible.\n\n        No other methods should be called after this one.\n        \"\"\"\n        raise NotImplementedError\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        raise NotImplementedError\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        raise NotImplementedError\n\n    # Methods scheduling callbacks.  All these return Handles.\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        raise NotImplementedError\n\n    def call_soon(self, callback, *args, context=None):\n        return self.call_later(0, callback, *args, context=context)\n\n    def call_later(self, delay, callback, *args, context=None):\n        raise NotImplementedError\n\n    def call_at(self, when, callback, *args, context=None):\n        raise NotImplementedError\n\n    def time(self):\n        raise NotImplementedError\n\n    def create_future(self):\n        raise NotImplementedError\n\n    # Method scheduling a coroutine object: create a task.\n\n    def create_task(self, coro, *, name=None, context=None):\n        raise NotImplementedError\n\n    # Methods for interacting with threads.\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        raise NotImplementedError\n\n    def run_in_executor(self, executor, func, *args):\n        raise NotImplementedError\n\n    def set_default_executor(self, executor):\n        raise NotImplementedError\n\n    # Network I/O methods returning Futures.\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        raise NotImplementedError\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        raise NotImplementedError\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0, proto=0,\n            flags=0, sock=None, local_addr=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        raise NotImplementedError\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *, family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE, sock=None, backlog=100,\n            ssl=None, reuse_address=None, reuse_port=None,\n            keep_alive=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a TCP server bound to host and port.\n\n        The return value is a Server object which can be used to stop\n        the service.\n\n        If host is an empty string or None all interfaces are assumed\n        and a list of multiple sockets will be returned (most likely\n        one for IPv4 and another one for IPv6). The host parameter can also be\n        a sequence (e.g. list) of hosts to bind to.\n\n        family can be set to either AF_INET or AF_INET6 to force the\n        socket to use IPv4 or IPv6. If not set it will be determined\n        from host (defaults to AF_UNSPEC).\n\n        flags is a bitmask for getaddrinfo().\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows.\n\n        keep_alive set to True keeps connections active by enabling the\n        periodic transmission of messages.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL handshake before aborting the\n        connection. Default is 60s.\n\n        ssl_shutdown_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL shutdown procedure\n        before aborting the connection. Default is 30s.\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file through a transport.\n\n        Return an amount of sent bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None,\n                        ssl_shutdown_timeout=None):\n        \"\"\"Upgrade a transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        raise NotImplementedError\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a UNIX Domain Socket server.\n\n        The return value is a Server object, which can be used to stop\n        the service.\n\n        path is a str, representing a file system path to bind the\n        server socket to.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for the SSL handshake to complete (defaults to 60s).\n\n        ssl_shutdown_timeout is the time in seconds that an SSL server\n        will wait for the SSL shutdown to finish (defaults to 30s).\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        \"\"\"Handle an accepted connection.\n\n        This is used by servers that accept connections outside of\n        asyncio, but use asyncio to handle connections.\n\n        This method is a coroutine.  When completed, the coroutine\n        returns a (transport, protocol) pair.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=None, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"A coroutine which creates a datagram endpoint.\n\n        This method will try to establish the endpoint in the background.\n        When successful, the coroutine returns a (transport, protocol) pair.\n\n        protocol_factory must be a callable returning a protocol instance.\n\n        socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on\n        host (or family if specified), socket type SOCK_DGRAM.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified it will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows and some UNIX's. If the\n        :py:data:`~socket.SO_REUSEPORT` constant is not defined then this\n        capability is unsupported.\n\n        allow_broadcast tells the kernel to allow this endpoint to send\n        messages to the broadcast address.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n        \"\"\"\n        raise NotImplementedError\n\n    # Pipes and subprocesses.\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        \"\"\"Register read pipe in event loop. Set the pipe to non-blocking mode.\n\n        protocol_factory should instantiate object with Protocol interface.\n        pipe is a file-like object.\n        Return pair (transport, protocol), where transport supports the\n        ReadTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        \"\"\"Register write pipe in event loop.\n\n        protocol_factory should instantiate object with BaseProtocol interface.\n        Pipe is file-like object already switched to nonblocking.\n        Return pair (transport, protocol), where transport support\n        WriteTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs):\n        raise NotImplementedError\n\n    async def subprocess_exec(self, protocol_factory, *args,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE,\n                              **kwargs):\n        raise NotImplementedError\n\n    # Ready-based callback registration methods.\n    # The add_*() methods return None.\n    # The remove_*() methods return True if something was removed,\n    # False if there was nothing to delete.\n\n    def add_reader(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_reader(self, fd):\n        raise NotImplementedError\n\n    def add_writer(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_writer(self, fd):\n        raise NotImplementedError\n\n    # Completion based I/O methods returning Futures.\n\n    async def sock_recv(self, sock, nbytes):\n        raise NotImplementedError\n\n    async def sock_recv_into(self, sock, buf):\n        raise NotImplementedError\n\n    async def sock_recvfrom(self, sock, bufsize):\n        raise NotImplementedError\n\n    async def sock_recvfrom_into(self, sock, buf, nbytes=0):\n        raise NotImplementedError\n\n    async def sock_sendall(self, sock, data):\n        raise NotImplementedError\n\n    async def sock_sendto(self, sock, data, address):\n        raise NotImplementedError\n\n    async def sock_connect(self, sock, address):\n        raise NotImplementedError\n\n    async def sock_accept(self, sock):\n        raise NotImplementedError\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=None):\n        raise NotImplementedError\n\n    # Signal handling.\n\n    def add_signal_handler(self, sig, callback, *args):\n        raise NotImplementedError\n\n    def remove_signal_handler(self, sig):\n        raise NotImplementedError\n\n    # Task factory.\n\n    def set_task_factory(self, factory):\n        raise NotImplementedError\n\n    def get_task_factory(self):\n        raise NotImplementedError\n\n    # Error handlers.\n\n    def get_exception_handler(self):\n        raise NotImplementedError\n\n    def set_exception_handler(self, handler):\n        raise NotImplementedError\n\n    def default_exception_handler(self, context):\n        raise NotImplementedError\n\n    def call_exception_handler(self, context):\n        raise NotImplementedError\n\n    # Debug flag management.\n\n    def get_debug(self):\n        raise NotImplementedError\n\n    def set_debug(self, enabled):\n        raise NotImplementedError\n\n\nclass AbstractEventLoopPolicy:\n    \"\"\"Abstract policy for accessing the event loop.\"\"\"\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an event loop object implementing the AbstractEventLoop interface,\n        or raises an exception in case no event loop has been set for the\n        current context and the current policy does not specify to create one.\n\n        It should never return None.\"\"\"\n        raise NotImplementedError\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop for the current context to loop.\"\"\"\n        raise NotImplementedError\n\n    def new_event_loop(self):\n        \"\"\"Create and return a new event loop object according to this\n        policy's rules. If there's need to set this loop as the event loop for\n        the current context, set_event_loop must be called explicitly.\"\"\"\n        raise NotImplementedError\n\n    # Child processes handling (Unix only).\n\n    def get_child_watcher(self):\n        \"Get the watcher for child processes.\"\n        raise NotImplementedError\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n        raise NotImplementedError\n\n\nclass BaseDefaultEventLoopPolicy(AbstractEventLoopPolicy):\n    \"\"\"Default policy implementation for accessing the event loop.\n\n    In this policy, each thread has its own event loop.  However, we\n    only automatically create an event loop by default for the main\n    thread; other threads by default have no event loop.\n\n    Other policies may have different rules (e.g. a single global\n    event loop, or automatically creating an event loop per thread, or\n    using some other notion of context to which an event loop is\n    associated).\n    \"\"\"\n\n    _loop_factory = None\n\n    class _Local(threading.local):\n        _loop = None\n        _set_called = False\n\n    def __init__(self):\n        self._local = self._Local()\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an instance of EventLoop or raises an exception.\n        \"\"\"\n        if (self._local._loop is None and\n                not self._local._set_called and\n                threading.current_thread() is threading.main_thread()):\n            stacklevel = 2\n            try:\n                f = sys._getframe(1)\n            except AttributeError:\n                pass\n            else:\n                # Move up the call stack so that the warning is attached\n                # to the line outside asyncio itself.\n                while f:\n                    module = f.f_globals.get('__name__')\n                    if not (module == 'asyncio' or module.startswith('asyncio.')):\n                        break\n                    f = f.f_back\n                    stacklevel += 1\n            import warnings\n            warnings.warn('There is no current event loop',\n                          DeprecationWarning, stacklevel=stacklevel)\n            self.set_event_loop(self.new_event_loop())\n\n        if self._local._loop is None:\n            raise RuntimeError('There is no current event loop in thread %r.'\n                               % threading.current_thread().name)\n\n        return self._local._loop\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\"\"\"\n        self._local._set_called = True\n        if loop is not None and not isinstance(loop, AbstractEventLoop):\n            raise TypeError(f\"loop must be an instance of AbstractEventLoop or None, not '{type(loop).__name__}'\")\n        self._local._loop = loop\n\n    def new_event_loop(self):\n        \"\"\"Create a new event loop.\n\n        You must call set_event_loop() to make this the current event\n        loop.\n        \"\"\"\n        return self._loop_factory()\n\n\n# Event loop policy.  The policy itself is always global, even if the\n# policy's rules say that there is an event loop per thread (or other\n# notion of context).  The default policy is installed by the first\n# call to get_event_loop_policy().\n_event_loop_policy = None\n\n# Lock for protecting the on-the-fly creation of the event loop policy.\n_lock = threading.Lock()\n\n\n# A TLS for the running event loop, used by _get_running_loop.\nclass _RunningLoop(threading.local):\n    loop_pid = (None, None)\n\n\n_running_loop = _RunningLoop()\n\n\ndef get_running_loop():\n    \"\"\"Return the running event loop.  Raise a RuntimeError if there is none.\n\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    loop = _get_running_loop()\n    if loop is None:\n        raise RuntimeError('no running event loop')\n    return loop\n\n\ndef _get_running_loop():\n    \"\"\"Return the running event loop or None.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    running_loop, pid = _running_loop.loop_pid\n    if running_loop is not None and pid == os.getpid():\n        return running_loop\n\n\ndef _set_running_loop(loop):\n    \"\"\"Set the running event loop.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    _running_loop.loop_pid = (loop, os.getpid())\n\n\ndef _init_event_loop_policy():\n    global _event_loop_policy\n    with _lock:\n        if _event_loop_policy is None:  # pragma: no branch\n            from . import DefaultEventLoopPolicy\n            _event_loop_policy = DefaultEventLoopPolicy()\n\n\ndef get_event_loop_policy():\n    \"\"\"Get the current event loop policy.\"\"\"\n    if _event_loop_policy is None:\n        _init_event_loop_policy()\n    return _event_loop_policy\n\n\ndef set_event_loop_policy(policy):\n    \"\"\"Set the current event loop policy.\n\n    If policy is None, the default policy is restored.\"\"\"\n    global _event_loop_policy\n    if policy is not None and not isinstance(policy, AbstractEventLoopPolicy):\n        raise TypeError(f\"policy must be an instance of AbstractEventLoopPolicy or None, not '{type(policy).__name__}'\")\n    _event_loop_policy = policy\n\n\ndef get_event_loop():\n    \"\"\"Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n\n\ndef set_event_loop(loop):\n    \"\"\"Equivalent to calling get_event_loop_policy().set_event_loop(loop).\"\"\"\n    get_event_loop_policy().set_event_loop(loop)\n\n\ndef new_event_loop():\n    \"\"\"Equivalent to calling get_event_loop_policy().new_event_loop().\"\"\"\n    return get_event_loop_policy().new_event_loop()\n\n\ndef get_child_watcher():\n    \"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\n    return get_event_loop_policy().get_child_watcher()\n\n\ndef set_child_watcher(watcher):\n    \"\"\"Equivalent to calling\n    get_event_loop_policy().set_child_watcher(watcher).\"\"\"\n    return get_event_loop_policy().set_child_watcher(watcher)\n\n\n# Alias pure-Python implementations for testing purposes.\n_py__get_running_loop = _get_running_loop\n_py__set_running_loop = _set_running_loop\n_py_get_running_loop = get_running_loop\n_py_get_event_loop = get_event_loop\n\n\ntry:\n    # get_event_loop() is one of the most frequently called\n    # functions in asyncio.  Pure Python implementation is\n    # about 4 times slower than C-accelerated.\n    from _asyncio import (_get_running_loop, _set_running_loop,\n                          get_running_loop, get_event_loop)\nexcept ImportError:\n    pass\nelse:\n    # Alias C implementations for testing purposes.\n    _c__get_running_loop = _get_running_loop\n    _c__set_running_loop = _set_running_loop\n    _c_get_running_loop = get_running_loop\n    _c_get_event_loop = get_event_loop\n\n\nif hasattr(os, 'fork'):\n    def on_fork():\n        # Reset the loop and wakeupfd in the forked child process.\n        if _event_loop_policy is not None:\n            _event_loop_policy._local = BaseDefaultEventLoopPolicy._Local()\n        _set_running_loop(None)\n        signal.set_wakeup_fd(-1)\n\n    os.register_at_fork(after_in_child=on_fork)\n", 882], "/usr/lib/python3.13/asyncio/unix_events.py": ["\"\"\"Selector event loop for Unix with signal handling.\"\"\"\n\nimport errno\nimport io\nimport itertools\nimport os\nimport selectors\nimport signal\nimport socket\nimport stat\nimport subprocess\nimport sys\nimport threading\nimport warnings\n\nfrom . import base_events\nfrom . import base_subprocess\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import selector_events\nfrom . import tasks\nfrom . import transports\nfrom .log import logger\n\n\n__all__ = (\n    'SelectorEventLoop',\n    'AbstractChildWatcher', 'SafeChildWatcher',\n    'FastChildWatcher', 'PidfdChildWatcher',\n    'MultiLoopChildWatcher', 'ThreadedChildWatcher',\n    'DefaultEventLoopPolicy',\n    'EventLoop',\n)\n\n\nif sys.platform == 'win32':  # pragma: no cover\n    raise ImportError('Signals are not really supported on Windows')\n\n\ndef _sighandler_noop(signum, frame):\n    \"\"\"Dummy signal handler.\"\"\"\n    pass\n\n\ndef waitstatus_to_exitcode(status):\n    try:\n        return os.waitstatus_to_exitcode(status)\n    except ValueError:\n        # The child exited, but we don't understand its status.\n        # This shouldn't happen, but if it does, let's just\n        # return that status; perhaps that helps debug it.\n        return status\n\n\nclass _UnixSelectorEventLoop(selector_events.BaseSelectorEventLoop):\n    \"\"\"Unix event loop.\n\n    Adds signal handling and UNIX Domain Socket support to SelectorEventLoop.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__(selector)\n        self._signal_handlers = {}\n        self._unix_server_sockets = {}\n\n    def close(self):\n        super().close()\n        if not sys.is_finalizing():\n            for sig in list(self._signal_handlers):\n                self.remove_signal_handler(sig)\n        else:\n            if self._signal_handlers:\n                warnings.warn(f\"Closing the loop {self!r} \"\n                              f\"on interpreter shutdown \"\n                              f\"stage, skipping signal handlers removal\",\n                              ResourceWarning,\n                              source=self)\n                self._signal_handlers.clear()\n\n    def _process_self_data(self, data):\n        for signum in data:\n            if not signum:\n                # ignore null bytes written by _write_to_self()\n                continue\n            self._handle_signal(signum)\n\n    def add_signal_handler(self, sig, callback, *args):\n        \"\"\"Add a handler for a signal.  UNIX only.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\"coroutines cannot be used \"\n                            \"with add_signal_handler()\")\n        self._check_signal(sig)\n        self._check_closed()\n        try:\n            # set_wakeup_fd() raises ValueError if this is not the\n            # main thread.  By calling it early we ensure that an\n            # event loop running in another thread cannot add a signal\n            # handler.\n            signal.set_wakeup_fd(self._csock.fileno())\n        except (ValueError, OSError) as exc:\n            raise RuntimeError(str(exc))\n\n        handle = events.Handle(callback, args, self, None)\n        self._signal_handlers[sig] = handle\n\n        try:\n            # Register a dummy signal handler to ask Python to write the signal\n            # number in the wakeup file descriptor. _process_self_data() will\n            # read signal numbers from this file descriptor to handle signals.\n            signal.signal(sig, _sighandler_noop)\n\n            # Set SA_RESTART to limit EINTR occurrences.\n            signal.siginterrupt(sig, False)\n        except OSError as exc:\n            del self._signal_handlers[sig]\n            if not self._signal_handlers:\n                try:\n                    signal.set_wakeup_fd(-1)\n                except (ValueError, OSError) as nexc:\n                    logger.info('set_wakeup_fd(-1) failed: %s', nexc)\n\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n    def _handle_signal(self, sig):\n        \"\"\"Internal helper that is the actual signal handler.\"\"\"\n        handle = self._signal_handlers.get(sig)\n        if handle is None:\n            return  # Assume it's some race condition.\n        if handle._cancelled:\n            self.remove_signal_handler(sig)  # Remove it properly.\n        else:\n            self._add_callback_signalsafe(handle)\n\n    def remove_signal_handler(self, sig):\n        \"\"\"Remove a handler for a signal.  UNIX only.\n\n        Return True if a signal handler was removed, False if not.\n        \"\"\"\n        self._check_signal(sig)\n        try:\n            del self._signal_handlers[sig]\n        except KeyError:\n            return False\n\n        if sig == signal.SIGINT:\n            handler = signal.default_int_handler\n        else:\n            handler = signal.SIG_DFL\n\n        try:\n            signal.signal(sig, handler)\n        except OSError as exc:\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n        if not self._signal_handlers:\n            try:\n                signal.set_wakeup_fd(-1)\n            except (ValueError, OSError) as exc:\n                logger.info('set_wakeup_fd(-1) failed: %s', exc)\n\n        return True\n\n    def _check_signal(self, sig):\n        \"\"\"Internal helper to validate a signal.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if not isinstance(sig, int):\n            raise TypeError(f'sig must be an int, not {sig!r}')\n\n        if sig not in signal.valid_signals():\n            raise ValueError(f'invalid signal number {sig}')\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        return _UnixReadPipeTransport(self, pipe, protocol, waiter, extra)\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        return _UnixWritePipeTransport(self, pipe, protocol, waiter, extra)\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', DeprecationWarning)\n            watcher = events.get_child_watcher()\n\n        with watcher:\n            if not watcher.is_active():\n                # Check early.\n                # Raising exception before process creation\n                # prevents subprocess execution if the watcher\n                # is not ready to handle it.\n                raise RuntimeError(\"asyncio.get_child_watcher() is not activated, \"\n                                \"subprocess support is not installed.\")\n            waiter = self.create_future()\n            transp = _UnixSubprocessTransport(self, protocol, args, shell,\n                                            stdin, stdout, stderr, bufsize,\n                                            waiter=waiter, extra=extra,\n                                            **kwargs)\n            watcher.add_child_handler(transp.get_pid(),\n                                    self._child_watcher_callback, transp)\n            try:\n                await waiter\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                transp.close()\n                await transp._wait()\n                raise\n\n        return transp\n\n    def _child_watcher_callback(self, pid, returncode, transp):\n        self.call_soon_threadsafe(transp._process_exited, returncode)\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        assert server_hostname is None or isinstance(server_hostname, str)\n        if ssl:\n            if server_hostname is None:\n                raise ValueError(\n                    'you have to pass server_hostname when using ssl')\n        else:\n            if server_hostname is not None:\n                raise ValueError('server_hostname is only meaningful with ssl')\n            if ssl_handshake_timeout is not None:\n                raise ValueError(\n                    'ssl_handshake_timeout is only meaningful with ssl')\n            if ssl_shutdown_timeout is not None:\n                raise ValueError(\n                    'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM, 0)\n            try:\n                sock.setblocking(False)\n                await self.sock_connect(sock, path)\n            except:\n                sock.close()\n                raise\n\n        else:\n            if sock is None:\n                raise ValueError('no path and sock were specified')\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n            sock.setblocking(False)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        return transport, protocol\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True, cleanup_socket=True):\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n\n            # Check for abstract socket. `str` and `bytes` paths are supported.\n            if path[0] not in (0, '\\x00'):\n                try:\n                    if stat.S_ISSOCK(os.stat(path).st_mode):\n                        os.remove(path)\n                except FileNotFoundError:\n                    pass\n                except OSError as err:\n                    # Directory may have permissions only to create socket.\n                    logger.error('Unable to check or remove stale UNIX socket '\n                                 '%r: %r', path, err)\n\n            try:\n                sock.bind(path)\n            except OSError as exc:\n                sock.close()\n                if exc.errno == errno.EADDRINUSE:\n                    # Let's improve the error message by adding\n                    # with what exact address it occurs.\n                    msg = f'Address {path!r} is already in use'\n                    raise OSError(errno.EADDRINUSE, msg) from None\n                else:\n                    raise\n            except:\n                sock.close()\n                raise\n        else:\n            if sock is None:\n                raise ValueError(\n                    'path was not specified, and no sock specified')\n\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n\n        if cleanup_socket:\n            path = sock.getsockname()\n            # Check for abstract socket. `str` and `bytes` paths are supported.\n            if path[0] not in (0, '\\x00'):\n                try:\n                    self._unix_server_sockets[sock] = os.stat(path).st_ino\n                except FileNotFoundError:\n                    pass\n\n        sock.setblocking(False)\n        server = base_events.Server(self, [sock], protocol_factory,\n                                    ssl, backlog, ssl_handshake_timeout,\n                                    ssl_shutdown_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        return server\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        try:\n            os.sendfile\n        except AttributeError:\n            raise exceptions.SendfileNotAvailableError(\n                \"os.sendfile() is not available\")\n        try:\n            fileno = file.fileno()\n        except (AttributeError, io.UnsupportedOperation) as err:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        try:\n            fsize = os.fstat(fileno).st_size\n        except OSError:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        blocksize = count if count else fsize\n        if not blocksize:\n            return 0  # empty file\n\n        fut = self.create_future()\n        self._sock_sendfile_native_impl(fut, None, sock, fileno,\n                                        offset, count, blocksize, 0)\n        return await fut\n\n    def _sock_sendfile_native_impl(self, fut, registered_fd, sock, fileno,\n                                   offset, count, blocksize, total_sent):\n        fd = sock.fileno()\n        if registered_fd is not None:\n            # Remove the callback early.  It should be rare that the\n            # selector says the fd is ready but the call still returns\n            # EAGAIN, and I am willing to take a hit in that case in\n            # order to simplify the common case.\n            self.remove_writer(registered_fd)\n        if fut.cancelled():\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            return\n        if count:\n            blocksize = count - total_sent\n            if blocksize <= 0:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n                return\n\n        # On 32-bit architectures truncate to 1GiB to avoid OverflowError\n        blocksize = min(blocksize, sys.maxsize//2 + 1)\n\n        try:\n            sent = os.sendfile(fd, fileno, offset, blocksize)\n        except (BlockingIOError, InterruptedError):\n            if registered_fd is None:\n                self._sock_add_cancellation_callback(fut, sock)\n            self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                            fd, sock, fileno,\n                            offset, count, blocksize, total_sent)\n        except OSError as exc:\n            if (registered_fd is not None and\n                    exc.errno == errno.ENOTCONN and\n                    type(exc) is not ConnectionError):\n                # If we have an ENOTCONN and this isn't a first call to\n                # sendfile(), i.e. the connection was closed in the middle\n                # of the operation, normalize the error to ConnectionError\n                # to make it consistent across all Posix systems.\n                new_exc = ConnectionError(\n                    \"socket is not connected\", errno.ENOTCONN)\n                new_exc.__cause__ = exc\n                exc = new_exc\n            if total_sent == 0:\n                # We can get here for different reasons, the main\n                # one being 'file' is not a regular mmap(2)-like\n                # file, in which case we'll fall back on using\n                # plain send().\n                err = exceptions.SendfileNotAvailableError(\n                    \"os.sendfile call failed\")\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(err)\n            else:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            fut.set_exception(exc)\n        else:\n            if sent == 0:\n                # EOF\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n            else:\n                offset += sent\n                total_sent += sent\n                if registered_fd is None:\n                    self._sock_add_cancellation_callback(fut, sock)\n                self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                                fd, sock, fileno,\n                                offset, count, blocksize, total_sent)\n\n    def _sock_sendfile_update_filepos(self, fileno, offset, total_sent):\n        if total_sent > 0:\n            os.lseek(fileno, offset, os.SEEK_SET)\n\n    def _sock_add_cancellation_callback(self, fut, sock):\n        def cb(fut):\n            if fut.cancelled():\n                fd = sock.fileno()\n                if fd != -1:\n                    self.remove_writer(fd)\n        fut.add_done_callback(cb)\n\n    def _stop_serving(self, sock):\n        # Is this a unix socket that needs cleanup?\n        if sock in self._unix_server_sockets:\n            path = sock.getsockname()\n        else:\n            path = None\n\n        super()._stop_serving(sock)\n\n        if path is not None:\n            prev_ino = self._unix_server_sockets[sock]\n            del self._unix_server_sockets[sock]\n            try:\n                if os.stat(path).st_ino == prev_ino:\n                    os.unlink(path)\n            except FileNotFoundError:\n                pass\n            except OSError as err:\n                logger.error('Unable to clean up listening UNIX socket '\n                             '%r: %r', path, err)\n\n\nclass _UnixReadPipeTransport(transports.ReadTransport):\n\n    max_size = 256 * 1024  # max bytes we read in one event loop iteration\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra)\n        self._extra['pipe'] = pipe\n        self._loop = loop\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._closing = False\n        self._paused = False\n\n        mode = os.fstat(self._fileno).st_mode\n        if not (stat.S_ISFIFO(mode) or\n                stat.S_ISSOCK(mode) or\n                stat.S_ISCHR(mode)):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is for pipes/sockets only.\")\n\n        os.set_blocking(self._fileno, False)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._fileno, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def _add_reader(self, fd, callback):\n        if not self.is_reading():\n            return\n        self._loop._add_reader(fd, callback)\n\n    def is_reading(self):\n        return not self._paused and not self._closing\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_READ)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def _read_ready(self):\n        try:\n            data = os.read(self._fileno, self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._fatal_error(exc, 'Fatal read error on pipe transport')\n        else:\n            if data:\n                self._protocol.data_received(data)\n            else:\n                if self._loop.get_debug():\n                    logger.info(\"%r was closed by peer\", self)\n                self._closing = True\n                self._loop._remove_reader(self._fileno)\n                self._loop.call_soon(self._protocol.eof_received)\n                self._loop.call_soon(self._call_connection_lost, None)\n\n    def pause_reading(self):\n        if not self.is_reading():\n            return\n        self._paused = True\n        self._loop._remove_reader(self._fileno)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._loop._add_reader(self._fileno, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if not self._closing:\n            self._close(None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if (isinstance(exc, OSError) and exc.errno == errno.EIO):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc):\n        self._closing = True\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixWritePipeTransport(transports._FlowControlMixin,\n                              transports.WriteTransport):\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra, loop)\n        self._extra['pipe'] = pipe\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._buffer = bytearray()\n        self._conn_lost = 0\n        self._closing = False  # Set when close() or write_eof() called.\n\n        mode = os.fstat(self._fileno).st_mode\n        is_char = stat.S_ISCHR(mode)\n        is_fifo = stat.S_ISFIFO(mode)\n        is_socket = stat.S_ISSOCK(mode)\n        if not (is_char or is_fifo or is_socket):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is only for \"\n                             \"pipes, sockets and character devices\")\n\n        os.set_blocking(self._fileno, False)\n        self._loop.call_soon(self._protocol.connection_made, self)\n\n        # On AIX, the reader trick (to be notified when the read end of the\n        # socket is closed) only works for sockets. On other platforms it\n        # works for pipes and sockets. (Exception: OS X 10.4?  Issue #19294.)\n        if is_socket or (is_fifo and not sys.platform.startswith(\"aix\")):\n            # only start reading when connection_made() has been called\n            self._loop.call_soon(self._loop._add_reader,\n                                 self._fileno, self._read_ready)\n\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_WRITE)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'bufsize={bufsize}')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _read_ready(self):\n        # Pipe was closed by peer.\n        if self._loop.get_debug():\n            logger.info(\"%r was closed by peer\", self)\n        if self._buffer:\n            self._close(BrokenPipeError())\n        else:\n            self._close()\n\n    def write(self, data):\n        assert isinstance(data, (bytes, bytearray, memoryview)), repr(data)\n        if isinstance(data, bytearray):\n            data = memoryview(data)\n        if not data:\n            return\n\n        if self._conn_lost or self._closing:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('pipe closed by peer or '\n                               'os.write(pipe, data) raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                n = os.write(self._fileno, data)\n            except (BlockingIOError, InterruptedError):\n                n = 0\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._conn_lost += 1\n                self._fatal_error(exc, 'Fatal write error on pipe transport')\n                return\n            if n == len(data):\n                return\n            elif n > 0:\n                data = memoryview(data)[n:]\n            self._loop._add_writer(self._fileno, self._write_ready)\n\n        self._buffer += data\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        try:\n            n = os.write(self._fileno, self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._buffer.clear()\n            self._conn_lost += 1\n            # Remove writer here, _fatal_error() doesn't it\n            # because _buffer is empty.\n            self._loop._remove_writer(self._fileno)\n            self._fatal_error(exc, 'Fatal write error on pipe transport')\n        else:\n            if n == len(self._buffer):\n                self._buffer.clear()\n                self._loop._remove_writer(self._fileno)\n                self._maybe_resume_protocol()  # May append to buffer.\n                if self._closing:\n                    self._loop._remove_reader(self._fileno)\n                    self._call_connection_lost(None)\n                return\n            elif n > 0:\n                del self._buffer[:n]\n\n    def can_write_eof(self):\n        return True\n\n    def write_eof(self):\n        if self._closing:\n            return\n        assert self._pipe\n        self._closing = True\n        if not self._buffer:\n            self._loop._remove_reader(self._fileno)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._pipe is not None and not self._closing:\n            # write_eof is all what we needed to close the write pipe\n            self.write_eof()\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def abort(self):\n        self._close(None)\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc=None):\n        self._closing = True\n        if self._buffer:\n            self._loop._remove_writer(self._fileno)\n        self._buffer.clear()\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixSubprocessTransport(base_subprocess.BaseSubprocessTransport):\n\n    def _start(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs):\n        stdin_w = None\n        if stdin == subprocess.PIPE and sys.platform.startswith('aix'):\n            # Use a socket pair for stdin on AIX, since it does not\n            # support selecting read events on the write end of a\n            # socket (which we use in order to detect closing of the\n            # other end).\n            stdin, stdin_w = socket.socketpair()\n        try:\n            self._proc = subprocess.Popen(\n                args, shell=shell, stdin=stdin, stdout=stdout, stderr=stderr,\n                universal_newlines=False, bufsize=bufsize, **kwargs)\n            if stdin_w is not None:\n                stdin.close()\n                self._proc.stdin = open(stdin_w.detach(), 'wb', buffering=bufsize)\n                stdin_w = None\n        finally:\n            if stdin_w is not None:\n                stdin.close()\n                stdin_w.close()\n\n\nclass AbstractChildWatcher:\n    \"\"\"Abstract base class for monitoring child processes.\n\n    Objects derived from this class monitor a collection of subprocesses and\n    report their termination or interruption by a signal.\n\n    New callbacks are registered with .add_child_handler(). Starting a new\n    process must be done within a 'with' block to allow the watcher to suspend\n    its activity until the new process if fully registered (this is needed to\n    prevent a race condition in some implementations).\n\n    Example:\n        with watcher:\n            proc = subprocess.Popen(\"sleep 1\")\n            watcher.add_child_handler(proc.pid, callback)\n\n    Notes:\n        Implementations of this class must be thread-safe.\n\n        Since child watcher objects may catch the SIGCHLD signal and call\n        waitpid(-1), there should be only one active object per process.\n    \"\"\"\n\n    def __init_subclass__(cls) -> None:\n        if cls.__module__ != __name__:\n            warnings._deprecated(\"AbstractChildWatcher\",\n                             \"{name!r} is deprecated as of Python 3.12 and will be \"\n                             \"removed in Python {remove}.\",\n                              remove=(3, 14))\n\n    def add_child_handler(self, pid, callback, *args):\n        \"\"\"Register a new child handler.\n\n        Arrange for callback(pid, returncode, *args) to be called when\n        process 'pid' terminates. Specifying another callback for the same\n        process replaces the previous handler.\n\n        Note: callback() must be thread-safe.\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_child_handler(self, pid):\n        \"\"\"Removes the handler for process 'pid'.\n\n        The function returns True if the handler was successfully removed,\n        False if there was nothing to remove.\"\"\"\n\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        \"\"\"Attach the watcher to an event loop.\n\n        If the watcher was previously attached to an event loop, then it is\n        first detached before attaching to the new loop.\n\n        Note: loop may be None.\n        \"\"\"\n        raise NotImplementedError()\n\n    def close(self):\n        \"\"\"Close the watcher.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        raise NotImplementedError()\n\n    def is_active(self):\n        \"\"\"Return ``True`` if the watcher is active and is used by the event loop.\n\n        Return True if the watcher is installed and ready to handle process exit\n        notifications.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def __enter__(self):\n        \"\"\"Enter the watcher's context and allow starting new processes\n\n        This function must return self\"\"\"\n        raise NotImplementedError()\n\n    def __exit__(self, a, b, c):\n        \"\"\"Exit the watcher's context\"\"\"\n        raise NotImplementedError()\n\n\nclass PidfdChildWatcher(AbstractChildWatcher):\n    \"\"\"Child watcher implementation using Linux's pid file descriptors.\n\n    This child watcher polls process file descriptors (pidfds) to await child\n    process termination. In some respects, PidfdChildWatcher is a \"Goldilocks\"\n    child watcher implementation. It doesn't require signals or threads, doesn't\n    interfere with any processes launched outside the event loop, and scales\n    linearly with the number of subprocesses launched by the event loop. The\n    main disadvantage is that pidfds are specific to Linux, and only work on\n    recent (5.3+) kernels.\n    \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        pass\n\n    def is_active(self):\n        return True\n\n    def close(self):\n        pass\n\n    def attach_loop(self, loop):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        pidfd = os.pidfd_open(pid)\n        loop._add_reader(pidfd, self._do_wait, pid, pidfd, callback, args)\n\n    def _do_wait(self, pid, pidfd, callback, args):\n        loop = events.get_running_loop()\n        loop._remove_reader(pidfd)\n        try:\n            _, status = os.waitpid(pid, 0)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            returncode = 255\n            logger.warning(\n                \"child process pid %d exit status already read: \"\n                \" will report returncode 255\",\n                pid)\n        else:\n            returncode = waitstatus_to_exitcode(status)\n\n        os.close(pidfd)\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        # asyncio never calls remove_child_handler() !!!\n        # The method is no-op but is implemented because\n        # abstract base classes require it.\n        return True\n\n\nclass BaseChildWatcher(AbstractChildWatcher):\n\n    def __init__(self):\n        self._loop = None\n        self._callbacks = {}\n\n    def close(self):\n        self.attach_loop(None)\n\n    def is_active(self):\n        return self._loop is not None and self._loop.is_running()\n\n    def _do_waitpid(self, expected_pid):\n        raise NotImplementedError()\n\n    def _do_waitpid_all(self):\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        assert loop is None or isinstance(loop, events.AbstractEventLoop)\n\n        if self._loop is not None and loop is None and self._callbacks:\n            warnings.warn(\n                'A loop is being detached '\n                'from a child watcher with pending handlers',\n                RuntimeWarning)\n\n        if self._loop is not None:\n            self._loop.remove_signal_handler(signal.SIGCHLD)\n\n        self._loop = loop\n        if loop is not None:\n            loop.add_signal_handler(signal.SIGCHLD, self._sig_chld)\n\n            # Prevent a race condition in case a child terminated\n            # during the switch.\n            self._do_waitpid_all()\n\n    def _sig_chld(self):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            # self._loop should always be available here\n            # as '_sig_chld' is added as a signal handler\n            # in 'attach_loop'\n            self._loop.call_exception_handler({\n                'message': 'Unknown exception in SIGCHLD handler',\n                'exception': exc,\n            })\n\n\nclass SafeChildWatcher(BaseChildWatcher):\n    \"\"\"'Safe' child watcher implementation.\n\n    This implementation avoids disrupting other code spawning processes by\n    polling explicitly each process in the SIGCHLD handler instead of calling\n    os.waitpid(-1).\n\n    This is a safe solution but it has a significant overhead when handling a\n    big number of children (O(n) each time SIGCHLD is raised)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        warnings._deprecated(\"SafeChildWatcher\",\n                             \"{name!r} is deprecated as of Python 3.12 and will be \"\n                             \"removed in Python {remove}.\",\n                              remove=(3, 14))\n\n    def close(self):\n        self._callbacks.clear()\n        super().close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, a, b, c):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        self._callbacks[pid] = (callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = waitstatus_to_exitcode(status)\n            if self._loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        try:\n            callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            if self._loop.get_debug():\n                logger.warning(\"Child watcher got an unexpected pid: %r\",\n                               pid, exc_info=True)\n        else:\n            callback(pid, returncode, *args)\n\n\nclass FastChildWatcher(BaseChildWatcher):\n    \"\"\"'Fast' child watcher implementation.\n\n    This implementation reaps every terminated processes by calling\n    os.waitpid(-1) directly, possibly breaking other code spawning processes\n    and waiting for their termination.\n\n    There is no noticeable overhead when handling a big number of children\n    (O(1) each time a child terminates).\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = threading.Lock()\n        self._zombies = {}\n        self._forks = 0\n        warnings._deprecated(\"FastChildWatcher\",\n                             \"{name!r} is deprecated as of Python 3.12 and will be \"\n                             \"removed in Python {remove}.\",\n                              remove=(3, 14))\n\n    def close(self):\n        self._callbacks.clear()\n        self._zombies.clear()\n        super().close()\n\n    def __enter__(self):\n        with self._lock:\n            self._forks += 1\n\n            return self\n\n    def __exit__(self, a, b, c):\n        with self._lock:\n            self._forks -= 1\n\n            if self._forks or not self._zombies:\n                return\n\n            collateral_victims = str(self._zombies)\n            self._zombies.clear()\n\n        logger.warning(\n            \"Caught subprocesses termination from unknown pids: %s\",\n            collateral_victims)\n\n    def add_child_handler(self, pid, callback, *args):\n        assert self._forks, \"Must use the context manager\"\n\n        with self._lock:\n            try:\n                returncode = self._zombies.pop(pid)\n            except KeyError:\n                # The child is running.\n                self._callbacks[pid] = callback, args\n                return\n\n        # The child is dead already. We can fire the callback.\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n        # Because of signal coalescing, we must keep calling waitpid() as\n        # long as we're able to reap a child.\n        while True:\n            try:\n                pid, status = os.waitpid(-1, os.WNOHANG)\n            except ChildProcessError:\n                # No more child processes exist.\n                return\n            else:\n                if pid == 0:\n                    # A child process is still alive.\n                    return\n\n                returncode = waitstatus_to_exitcode(status)\n\n            with self._lock:\n                try:\n                    callback, args = self._callbacks.pop(pid)\n                except KeyError:\n                    # unknown child\n                    if self._forks:\n                        # It may not be registered yet.\n                        self._zombies[pid] = returncode\n                        if self._loop.get_debug():\n                            logger.debug('unknown process %s exited '\n                                         'with returncode %s',\n                                         pid, returncode)\n                        continue\n                    callback = None\n                else:\n                    if self._loop.get_debug():\n                        logger.debug('process %s exited with returncode %s',\n                                     pid, returncode)\n\n            if callback is None:\n                logger.warning(\n                    \"Caught subprocess termination from unknown pid: \"\n                    \"%d -> %d\", pid, returncode)\n            else:\n                callback(pid, returncode, *args)\n\n\nclass MultiLoopChildWatcher(AbstractChildWatcher):\n    \"\"\"A watcher that doesn't require running loop in the main thread.\n\n    This implementation registers a SIGCHLD signal handler on\n    instantiation (which may conflict with other code that\n    install own handler for this signal).\n\n    The solution is safe but it has a significant overhead when\n    handling a big number of processes (*O(n)* each time a\n    SIGCHLD is received).\n    \"\"\"\n\n    # Implementation note:\n    # The class keeps compatibility with AbstractChildWatcher ABC\n    # To achieve this it has empty attach_loop() method\n    # and doesn't accept explicit loop argument\n    # for add_child_handler()/remove_child_handler()\n    # but retrieves the current loop by get_running_loop()\n\n    def __init__(self):\n        self._callbacks = {}\n        self._saved_sighandler = None\n        warnings._deprecated(\"MultiLoopChildWatcher\",\n                             \"{name!r} is deprecated as of Python 3.12 and will be \"\n                             \"removed in Python {remove}.\",\n                              remove=(3, 14))\n\n    def is_active(self):\n        return self._saved_sighandler is not None\n\n    def close(self):\n        self._callbacks.clear()\n        if self._saved_sighandler is None:\n            return\n\n        handler = signal.getsignal(signal.SIGCHLD)\n        if handler != self._sig_chld:\n            logger.warning(\"SIGCHLD handler was changed by outside code\")\n        else:\n            signal.signal(signal.SIGCHLD, self._saved_sighandler)\n        self._saved_sighandler = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        self._callbacks[pid] = (loop, callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def attach_loop(self, loop):\n        # Don't save the loop but initialize itself if called first time\n        # The reason to do it here is that attach_loop() is called from\n        # unix policy only for the main thread.\n        # Main thread is required for subscription on SIGCHLD signal\n        if self._saved_sighandler is not None:\n            return\n\n        self._saved_sighandler = signal.signal(signal.SIGCHLD, self._sig_chld)\n        if self._saved_sighandler is None:\n            logger.warning(\"Previous SIGCHLD handler was set by non-Python code, \"\n                           \"restore to default handler on watcher close.\")\n            self._saved_sighandler = signal.SIG_DFL\n\n        # Set SA_RESTART to limit EINTR occurrences.\n        signal.siginterrupt(signal.SIGCHLD, False)\n\n    def _do_waitpid_all(self):\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n            debug_log = False\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = waitstatus_to_exitcode(status)\n            debug_log = True\n        try:\n            loop, callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            logger.warning(\"Child watcher got an unexpected pid: %r\",\n                           pid, exc_info=True)\n        else:\n            if loop.is_closed():\n                logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n            else:\n                if debug_log and loop.get_debug():\n                    logger.debug('process %s exited with returncode %s',\n                                 expected_pid, returncode)\n                loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n    def _sig_chld(self, signum, frame):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException:\n            logger.warning('Unknown exception in SIGCHLD handler', exc_info=True)\n\n\nclass ThreadedChildWatcher(AbstractChildWatcher):\n    \"\"\"Threaded child watcher implementation.\n\n    The watcher uses a thread per process\n    for waiting for the process finish.\n\n    It doesn't require subscription on POSIX signal\n    but a thread creation is not free.\n\n    The watcher has O(1) complexity, its performance doesn't depend\n    on amount of spawn processes.\n    \"\"\"\n\n    def __init__(self):\n        self._pid_counter = itertools.count(0)\n        self._threads = {}\n\n    def is_active(self):\n        return True\n\n    def close(self):\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def __del__(self, _warn=warnings.warn):\n        threads = [thread for thread in list(self._threads.values())\n                   if thread.is_alive()]\n        if threads:\n            _warn(f\"{self.__class__} has registered but not finished child processes\",\n                  ResourceWarning,\n                  source=self)\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        thread = threading.Thread(target=self._do_waitpid,\n                                  name=f\"asyncio-waitpid-{next(self._pid_counter)}\",\n                                  args=(loop, pid, callback, args),\n                                  daemon=True)\n        self._threads[pid] = thread\n        thread.start()\n\n    def remove_child_handler(self, pid):\n        # asyncio never calls remove_child_handler() !!!\n        # The method is no-op but is implemented because\n        # abstract base classes require it.\n        return True\n\n    def attach_loop(self, loop):\n        pass\n\n    def _do_waitpid(self, loop, expected_pid, callback, args):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, 0)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            returncode = waitstatus_to_exitcode(status)\n            if loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        if loop.is_closed():\n            logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n        else:\n            loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n        self._threads.pop(expected_pid)\n\ndef can_use_pidfd():\n    if not hasattr(os, 'pidfd_open'):\n        return False\n    try:\n        pid = os.getpid()\n        os.close(os.pidfd_open(pid, 0))\n    except OSError:\n        # blocked by security policy like SECCOMP\n        return False\n    return True\n\n\nclass _UnixDefaultEventLoopPolicy(events.BaseDefaultEventLoopPolicy):\n    \"\"\"UNIX event loop policy with a watcher for child processes.\"\"\"\n    _loop_factory = _UnixSelectorEventLoop\n\n    def __init__(self):\n        super().__init__()\n        self._watcher = None\n\n    def _init_watcher(self):\n        with events._lock:\n            if self._watcher is None:  # pragma: no branch\n                if can_use_pidfd():\n                    self._watcher = PidfdChildWatcher()\n                else:\n                    self._watcher = ThreadedChildWatcher()\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\n\n        As a side effect, if a child watcher was set before, then calling\n        .set_event_loop() from the main thread will call .attach_loop(loop) on\n        the child watcher.\n        \"\"\"\n\n        super().set_event_loop(loop)\n\n        if (self._watcher is not None and\n                threading.current_thread() is threading.main_thread()):\n            self._watcher.attach_loop(loop)\n\n    def get_child_watcher(self):\n        \"\"\"Get the watcher for child processes.\n\n        If not yet set, a ThreadedChildWatcher object is automatically created.\n        \"\"\"\n        if self._watcher is None:\n            self._init_watcher()\n\n        warnings._deprecated(\"get_child_watcher\",\n                            \"{name!r} is deprecated as of Python 3.12 and will be \"\n                            \"removed in Python {remove}.\", remove=(3, 14))\n        return self._watcher\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n\n        assert watcher is None or isinstance(watcher, AbstractChildWatcher)\n\n        if self._watcher is not None:\n            self._watcher.close()\n\n        self._watcher = watcher\n        warnings._deprecated(\"set_child_watcher\",\n                            \"{name!r} is deprecated as of Python 3.12 and will be \"\n                            \"removed in Python {remove}.\", remove=(3, 14))\n\n\nSelectorEventLoop = _UnixSelectorEventLoop\nDefaultEventLoopPolicy = _UnixDefaultEventLoopPolicy\nEventLoop = SelectorEventLoop\n", 1536], "/usr/lib/python3.13/asyncio/coroutines.py": ["__all__ = 'iscoroutinefunction', 'iscoroutine'\n\nimport collections.abc\nimport inspect\nimport os\nimport sys\nimport types\n\n\ndef _is_debug_mode():\n    # See: https://docs.python.org/3/library/asyncio-dev.html#asyncio-debug-mode.\n    return sys.flags.dev_mode or (not sys.flags.ignore_environment and\n                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))\n\n\n# A marker for iscoroutinefunction.\n_is_coroutine = object()\n\n\ndef iscoroutinefunction(func):\n    \"\"\"Return True if func is a decorated coroutine function.\"\"\"\n    return (inspect.iscoroutinefunction(func) or\n            getattr(func, '_is_coroutine', None) is _is_coroutine)\n\n\n# Prioritize native coroutine check to speed-up\n# asyncio.iscoroutine.\n_COROUTINE_TYPES = (types.CoroutineType, collections.abc.Coroutine)\n_iscoroutine_typecache = set()\n\n\ndef iscoroutine(obj):\n    \"\"\"Return True if obj is a coroutine object.\"\"\"\n    if type(obj) in _iscoroutine_typecache:\n        return True\n\n    if isinstance(obj, _COROUTINE_TYPES):\n        # Just in case we don't want to cache more than 100\n        # positive types.  That shouldn't ever happen, unless\n        # someone stressing the system on purpose.\n        if len(_iscoroutine_typecache) < 100:\n            _iscoroutine_typecache.add(type(obj))\n        return True\n    else:\n        return False\n\n\ndef _format_coroutine(coro):\n    assert iscoroutine(coro)\n\n    def get_name(coro):\n        # Coroutines compiled with Cython sometimes don't have\n        # proper __qualname__ or __name__.  While that is a bug\n        # in Cython, asyncio shouldn't crash with an AttributeError\n        # in its __repr__ functions.\n        if hasattr(coro, '__qualname__') and coro.__qualname__:\n            coro_name = coro.__qualname__\n        elif hasattr(coro, '__name__') and coro.__name__:\n            coro_name = coro.__name__\n        else:\n            # Stop masking Cython bugs, expose them in a friendly way.\n            coro_name = f'<{type(coro).__name__} without __name__>'\n        return f'{coro_name}()'\n\n    def is_running(coro):\n        try:\n            return coro.cr_running\n        except AttributeError:\n            try:\n                return coro.gi_running\n            except AttributeError:\n                return False\n\n    coro_code = None\n    if hasattr(coro, 'cr_code') and coro.cr_code:\n        coro_code = coro.cr_code\n    elif hasattr(coro, 'gi_code') and coro.gi_code:\n        coro_code = coro.gi_code\n\n    coro_name = get_name(coro)\n\n    if not coro_code:\n        # Built-in types might not have __qualname__ or __name__.\n        if is_running(coro):\n            return f'{coro_name} running'\n        else:\n            return coro_name\n\n    coro_frame = None\n    if hasattr(coro, 'gi_frame') and coro.gi_frame:\n        coro_frame = coro.gi_frame\n    elif hasattr(coro, 'cr_frame') and coro.cr_frame:\n        coro_frame = coro.cr_frame\n\n    # If Cython's coroutine has a fake code object without proper\n    # co_filename -- expose that.\n    filename = coro_code.co_filename or '<empty co_filename>'\n\n    lineno = 0\n\n    if coro_frame is not None:\n        lineno = coro_frame.f_lineno\n        coro_repr = f'{coro_name} running at {filename}:{lineno}'\n\n    else:\n        lineno = coro_code.co_firstlineno\n        coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n\n    return coro_repr\n", 109], "/usr/lib/python3.13/asyncio/base_events.py": ["\"\"\"Base implementation of event loop.\n\nThe event loop can be broken up into a multiplexer (the part\nresponsible for notifying us of I/O events) and the event loop proper,\nwhich wraps a multiplexer with functionality for scheduling callbacks,\nimmediately or at a given time in the future.\n\nWhenever a public API takes a callback, subsequent positional\narguments will be passed to the callback if/when it is called.  This\navoids the proliferation of trivial lambdas implementing closures.\nKeyword arguments for the callback are not supported; this is a\nconscious design decision, leaving the door open for keyword arguments\nto modify the meaning of the API call itself.\n\"\"\"\n\nimport collections\nimport collections.abc\nimport concurrent.futures\nimport errno\nimport functools\nimport heapq\nimport itertools\nimport os\nimport socket\nimport stat\nimport subprocess\nimport threading\nimport time\nimport traceback\nimport sys\nimport warnings\nimport weakref\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import staggered\nfrom . import tasks\nfrom . import timeouts\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\n__all__ = 'BaseEventLoop','Server',\n\n\n# Minimum number of _scheduled timer handles before cleanup of\n# cancelled handles is performed.\n_MIN_SCHEDULED_TIMER_HANDLES = 100\n\n# Minimum fraction of _scheduled timer handles that are cancelled\n# before cleanup of cancelled handles is performed.\n_MIN_CANCELLED_TIMER_HANDLES_FRACTION = 0.5\n\n\n_HAS_IPv6 = hasattr(socket, 'AF_INET6')\n\n# Maximum timeout passed to select to avoid OS limitations\nMAXIMUM_SELECT_TIMEOUT = 24 * 3600\n\n\ndef _format_handle(handle):\n    cb = handle._callback\n    if isinstance(getattr(cb, '__self__', None), tasks.Task):\n        # format the task\n        return repr(cb.__self__)\n    else:\n        return str(handle)\n\n\ndef _format_pipe(fd):\n    if fd == subprocess.PIPE:\n        return '<pipe>'\n    elif fd == subprocess.STDOUT:\n        return '<stdout>'\n    else:\n        return repr(fd)\n\n\ndef _set_reuseport(sock):\n    if not hasattr(socket, 'SO_REUSEPORT'):\n        raise ValueError('reuse_port not supported by socket module')\n    else:\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        except OSError:\n            raise ValueError('reuse_port not supported by socket module, '\n                             'SO_REUSEPORT defined but not implemented.')\n\n\ndef _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):\n    # Try to skip getaddrinfo if \"host\" is already an IP. Users might have\n    # handled name resolution in their own code and pass in resolved IPs.\n    if not hasattr(socket, 'inet_pton'):\n        return\n\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or \\\n            host is None:\n        return None\n\n    if type == socket.SOCK_STREAM:\n        proto = socket.IPPROTO_TCP\n    elif type == socket.SOCK_DGRAM:\n        proto = socket.IPPROTO_UDP\n    else:\n        return None\n\n    if port is None:\n        port = 0\n    elif isinstance(port, bytes) and port == b'':\n        port = 0\n    elif isinstance(port, str) and port == '':\n        port = 0\n    else:\n        # If port's a service name like \"http\", don't skip getaddrinfo.\n        try:\n            port = int(port)\n        except (TypeError, ValueError):\n            return None\n\n    if family == socket.AF_UNSPEC:\n        afs = [socket.AF_INET]\n        if _HAS_IPv6:\n            afs.append(socket.AF_INET6)\n    else:\n        afs = [family]\n\n    if isinstance(host, bytes):\n        host = host.decode('idna')\n    if '%' in host:\n        # Linux's inet_pton doesn't accept an IPv6 zone index after host,\n        # like '::1%lo0'.\n        return None\n\n    for af in afs:\n        try:\n            socket.inet_pton(af, host)\n            # The host has already been resolved.\n            if _HAS_IPv6 and af == socket.AF_INET6:\n                return af, type, proto, '', (host, port, flowinfo, scopeid)\n            else:\n                return af, type, proto, '', (host, port)\n        except OSError:\n            pass\n\n    # \"host\" is not an IP address.\n    return None\n\n\ndef _interleave_addrinfos(addrinfos, first_address_family_count=1):\n    \"\"\"Interleave list of addrinfo tuples by family.\"\"\"\n    # Group addresses by family\n    addrinfos_by_family = collections.OrderedDict()\n    for addr in addrinfos:\n        family = addr[0]\n        if family not in addrinfos_by_family:\n            addrinfos_by_family[family] = []\n        addrinfos_by_family[family].append(addr)\n    addrinfos_lists = list(addrinfos_by_family.values())\n\n    reordered = []\n    if first_address_family_count > 1:\n        reordered.extend(addrinfos_lists[0][:first_address_family_count - 1])\n        del addrinfos_lists[0][:first_address_family_count - 1]\n    reordered.extend(\n        a for a in itertools.chain.from_iterable(\n            itertools.zip_longest(*addrinfos_lists)\n        ) if a is not None)\n    return reordered\n\n\ndef _run_until_complete_cb(fut):\n    if not fut.cancelled():\n        exc = fut.exception()\n        if isinstance(exc, (SystemExit, KeyboardInterrupt)):\n            # Issue #22429: run_forever() already finished, no need to\n            # stop it.\n            return\n    futures._get_loop(fut).stop()\n\n\nif hasattr(socket, 'TCP_NODELAY'):\n    def _set_nodelay(sock):\n        if (sock.family in {socket.AF_INET, socket.AF_INET6} and\n                sock.type == socket.SOCK_STREAM and\n                sock.proto == socket.IPPROTO_TCP):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nelse:\n    def _set_nodelay(sock):\n        pass\n\n\ndef _check_ssl_socket(sock):\n    if ssl is not None and isinstance(sock, ssl.SSLSocket):\n        raise TypeError(\"Socket cannot be of type SSLSocket\")\n\n\nclass _SendfileFallbackProtocol(protocols.Protocol):\n    def __init__(self, transp):\n        if not isinstance(transp, transports._FlowControlMixin):\n            raise TypeError(\"transport should be _FlowControlMixin instance\")\n        self._transport = transp\n        self._proto = transp.get_protocol()\n        self._should_resume_reading = transp.is_reading()\n        self._should_resume_writing = transp._protocol_paused\n        transp.pause_reading()\n        transp.set_protocol(self)\n        if self._should_resume_writing:\n            self._write_ready_fut = self._transport._loop.create_future()\n        else:\n            self._write_ready_fut = None\n\n    async def drain(self):\n        if self._transport.is_closing():\n            raise ConnectionError(\"Connection closed by peer\")\n        fut = self._write_ready_fut\n        if fut is None:\n            return\n        await fut\n\n    def connection_made(self, transport):\n        raise RuntimeError(\"Invalid state: \"\n                           \"connection should have been established already.\")\n\n    def connection_lost(self, exc):\n        if self._write_ready_fut is not None:\n            # Never happens if peer disconnects after sending the whole content\n            # Thus disconnection is always an exception from user perspective\n            if exc is None:\n                self._write_ready_fut.set_exception(\n                    ConnectionError(\"Connection is closed by peer\"))\n            else:\n                self._write_ready_fut.set_exception(exc)\n        self._proto.connection_lost(exc)\n\n    def pause_writing(self):\n        if self._write_ready_fut is not None:\n            return\n        self._write_ready_fut = self._transport._loop.create_future()\n\n    def resume_writing(self):\n        if self._write_ready_fut is None:\n            return\n        self._write_ready_fut.set_result(False)\n        self._write_ready_fut = None\n\n    def data_received(self, data):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    def eof_received(self):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    async def restore(self):\n        self._transport.set_protocol(self._proto)\n        if self._should_resume_reading:\n            self._transport.resume_reading()\n        if self._write_ready_fut is not None:\n            # Cancel the future.\n            # Basically it has no effect because protocol is switched back,\n            # no code should wait for it anymore.\n            self._write_ready_fut.cancel()\n        if self._should_resume_writing:\n            self._proto.resume_writing()\n\n\nclass Server(events.AbstractServer):\n\n    def __init__(self, loop, sockets, protocol_factory, ssl_context, backlog,\n                 ssl_handshake_timeout, ssl_shutdown_timeout=None):\n        self._loop = loop\n        self._sockets = sockets\n        # Weak references so we don't break Transport's ability to\n        # detect abandoned transports\n        self._clients = weakref.WeakSet()\n        self._waiters = []\n        self._protocol_factory = protocol_factory\n        self._backlog = backlog\n        self._ssl_context = ssl_context\n        self._ssl_handshake_timeout = ssl_handshake_timeout\n        self._ssl_shutdown_timeout = ssl_shutdown_timeout\n        self._serving = False\n        self._serving_forever_fut = None\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__} sockets={self.sockets!r}>'\n\n    def _attach(self, transport):\n        assert self._sockets is not None\n        self._clients.add(transport)\n\n    def _detach(self, transport):\n        self._clients.discard(transport)\n        if len(self._clients) == 0 and self._sockets is None:\n            self._wakeup()\n\n    def _wakeup(self):\n        waiters = self._waiters\n        self._waiters = None\n        for waiter in waiters:\n            if not waiter.done():\n                waiter.set_result(None)\n\n    def _start_serving(self):\n        if self._serving:\n            return\n        self._serving = True\n        for sock in self._sockets:\n            sock.listen(self._backlog)\n            self._loop._start_serving(\n                self._protocol_factory, sock, self._ssl_context,\n                self, self._backlog, self._ssl_handshake_timeout,\n                self._ssl_shutdown_timeout)\n\n    def get_loop(self):\n        return self._loop\n\n    def is_serving(self):\n        return self._serving\n\n    @property\n    def sockets(self):\n        if self._sockets is None:\n            return ()\n        return tuple(trsock.TransportSocket(s) for s in self._sockets)\n\n    def close(self):\n        sockets = self._sockets\n        if sockets is None:\n            return\n        self._sockets = None\n\n        for sock in sockets:\n            self._loop._stop_serving(sock)\n\n        self._serving = False\n\n        if (self._serving_forever_fut is not None and\n                not self._serving_forever_fut.done()):\n            self._serving_forever_fut.cancel()\n            self._serving_forever_fut = None\n\n        if len(self._clients) == 0:\n            self._wakeup()\n\n    def close_clients(self):\n        for transport in self._clients.copy():\n            transport.close()\n\n    def abort_clients(self):\n        for transport in self._clients.copy():\n            transport.abort()\n\n    async def start_serving(self):\n        self._start_serving()\n        # Skip one loop iteration so that all 'loop.add_reader'\n        # go through.\n        await tasks.sleep(0)\n\n    async def serve_forever(self):\n        if self._serving_forever_fut is not None:\n            raise RuntimeError(\n                f'server {self!r} is already being awaited on serve_forever()')\n        if self._sockets is None:\n            raise RuntimeError(f'server {self!r} is closed')\n\n        self._start_serving()\n        self._serving_forever_fut = self._loop.create_future()\n\n        try:\n            await self._serving_forever_fut\n        except exceptions.CancelledError:\n            try:\n                self.close()\n                await self.wait_closed()\n            finally:\n                raise\n        finally:\n            self._serving_forever_fut = None\n\n    async def wait_closed(self):\n        \"\"\"Wait until server is closed and all connections are dropped.\n\n        - If the server is not closed, wait.\n        - If it is closed, but there are still active connections, wait.\n\n        Anyone waiting here will be unblocked once both conditions\n        (server is closed and all connections have been dropped)\n        have become true, in either order.\n\n        Historical note: In 3.11 and before, this was broken, returning\n        immediately if the server was already closed, even if there\n        were still active connections. An attempted fix in 3.12.0 was\n        still broken, returning immediately if the server was still\n        open and there were no active connections. Hopefully in 3.12.1\n        we have it right.\n        \"\"\"\n        # Waiters are unblocked by self._wakeup(), which is called\n        # from two places: self.close() and self._detach(), but only\n        # when both conditions have become true. To signal that this\n        # has happened, self._wakeup() sets self._waiters to None.\n        if self._waiters is None:\n            return\n        waiter = self._loop.create_future()\n        self._waiters.append(waiter)\n        await waiter\n\n\nclass BaseEventLoop(events.AbstractEventLoop):\n\n    def __init__(self):\n        self._timer_cancelled_count = 0\n        self._closed = False\n        self._stopping = False\n        self._ready = collections.deque()\n        self._scheduled = []\n        self._default_executor = None\n        self._internal_fds = 0\n        # Identifier of the thread running the event loop, or None if the\n        # event loop is not running\n        self._thread_id = None\n        self._clock_resolution = time.get_clock_info('monotonic').resolution\n        self._exception_handler = None\n        self.set_debug(coroutines._is_debug_mode())\n        # The preserved state of async generator hooks.\n        self._old_agen_hooks = None\n        # In debug mode, if the execution of a callback or a step of a task\n        # exceed this duration in seconds, the slow callback/task is logged.\n        self.slow_callback_duration = 0.1\n        self._current_handle = None\n        self._task_factory = None\n        self._coroutine_origin_tracking_enabled = False\n        self._coroutine_origin_tracking_saved_depth = None\n\n        # A weak set of all asynchronous generators that are\n        # being iterated by the loop.\n        self._asyncgens = weakref.WeakSet()\n        # Set to True when `loop.shutdown_asyncgens` is called.\n        self._asyncgens_shutdown_called = False\n        # Set to True when `loop.shutdown_default_executor` is called.\n        self._executor_shutdown_called = False\n\n    def __repr__(self):\n        return (\n            f'<{self.__class__.__name__} running={self.is_running()} '\n            f'closed={self.is_closed()} debug={self.get_debug()}>'\n        )\n\n    def create_future(self):\n        \"\"\"Create a Future object attached to the loop.\"\"\"\n        return futures.Future(loop=self)\n\n    def create_task(self, coro, *, name=None, context=None):\n        \"\"\"Schedule a coroutine object.\n\n        Return a task object.\n        \"\"\"\n        self._check_closed()\n        if self._task_factory is None:\n            task = tasks.Task(coro, loop=self, name=name, context=context)\n            if task._source_traceback:\n                del task._source_traceback[-1]\n        else:\n            if context is None:\n                # Use legacy API if context is not needed\n                task = self._task_factory(self, coro)\n            else:\n                task = self._task_factory(self, coro, context=context)\n\n            task.set_name(name)\n\n        return task\n\n    def set_task_factory(self, factory):\n        \"\"\"Set a task factory that will be used by loop.create_task().\n\n        If factory is None the default task factory will be set.\n\n        If factory is a callable, it should have a signature matching\n        '(loop, coro)', where 'loop' will be a reference to the active\n        event loop, 'coro' will be a coroutine object.  The callable\n        must return a Future.\n        \"\"\"\n        if factory is not None and not callable(factory):\n            raise TypeError('task factory must be a callable or None')\n        self._task_factory = factory\n\n    def get_task_factory(self):\n        \"\"\"Return a task factory, or None if the default one is in use.\"\"\"\n        return self._task_factory\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        \"\"\"Create socket transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            call_connection_made=True):\n        \"\"\"Create SSL transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        \"\"\"Create datagram transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        \"\"\"Create read pipe transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        \"\"\"Create write pipe transport.\"\"\"\n        raise NotImplementedError\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        \"\"\"Create subprocess transport.\"\"\"\n        raise NotImplementedError\n\n    def _write_to_self(self):\n        \"\"\"Write a byte to self-pipe, to wake up the event loop.\n\n        This may be called from a different thread.\n\n        The subclass is responsible for implementing the self-pipe.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_events(self, event_list):\n        \"\"\"Process selector events.\"\"\"\n        raise NotImplementedError\n\n    def _check_closed(self):\n        if self._closed:\n            raise RuntimeError('Event loop is closed')\n\n    def _check_default_executor(self):\n        if self._executor_shutdown_called:\n            raise RuntimeError('Executor shutdown has been called')\n\n    def _asyncgen_finalizer_hook(self, agen):\n        self._asyncgens.discard(agen)\n        if not self.is_closed():\n            self.call_soon_threadsafe(self.create_task, agen.aclose())\n\n    def _asyncgen_firstiter_hook(self, agen):\n        if self._asyncgens_shutdown_called:\n            warnings.warn(\n                f\"asynchronous generator {agen!r} was scheduled after \"\n                f\"loop.shutdown_asyncgens() call\",\n                ResourceWarning, source=self)\n\n        self._asyncgens.add(agen)\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        self._asyncgens_shutdown_called = True\n\n        if not len(self._asyncgens):\n            # If Python version is <3.6 or we don't have any asynchronous\n            # generators alive.\n            return\n\n        closing_agens = list(self._asyncgens)\n        self._asyncgens.clear()\n\n        results = await tasks.gather(\n            *[ag.aclose() for ag in closing_agens],\n            return_exceptions=True)\n\n        for result, agen in zip(results, closing_agens):\n            if isinstance(result, Exception):\n                self.call_exception_handler({\n                    'message': f'an error occurred during closing of '\n                               f'asynchronous generator {agen!r}',\n                    'exception': result,\n                    'asyncgen': agen\n                })\n\n    async def shutdown_default_executor(self, timeout=None):\n        \"\"\"Schedule the shutdown of the default executor.\n\n        The timeout parameter specifies the amount of time the executor will\n        be given to finish joining. The default value is None, which means\n        that the executor will be given an unlimited amount of time.\n        \"\"\"\n        self._executor_shutdown_called = True\n        if self._default_executor is None:\n            return\n        future = self.create_future()\n        thread = threading.Thread(target=self._do_shutdown, args=(future,))\n        thread.start()\n        try:\n            async with timeouts.timeout(timeout):\n                await future\n        except TimeoutError:\n            warnings.warn(\"The executor did not finishing joining \"\n                          f\"its threads within {timeout} seconds.\",\n                          RuntimeWarning, stacklevel=2)\n            self._default_executor.shutdown(wait=False)\n        else:\n            thread.join()\n\n    def _do_shutdown(self, future):\n        try:\n            self._default_executor.shutdown(wait=True)\n            if not self.is_closed():\n                self.call_soon_threadsafe(futures._set_result_unless_cancelled,\n                                          future, None)\n        except Exception as ex:\n            if not self.is_closed() and not future.cancelled():\n                self.call_soon_threadsafe(future.set_exception, ex)\n\n    def _check_running(self):\n        if self.is_running():\n            raise RuntimeError('This event loop is already running')\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                'Cannot run the event loop while another loop is running')\n\n    def _run_forever_setup(self):\n        \"\"\"Prepare the run loop to process events.\n\n        This method exists so that custom custom event loop subclasses (e.g., event loops\n        that integrate a GUI event loop with Python's event loop) have access to all the\n        loop setup logic.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n        self._set_coroutine_origin_tracking(self._debug)\n\n        self._old_agen_hooks = sys.get_asyncgen_hooks()\n        self._thread_id = threading.get_ident()\n        sys.set_asyncgen_hooks(\n            firstiter=self._asyncgen_firstiter_hook,\n            finalizer=self._asyncgen_finalizer_hook\n        )\n\n        events._set_running_loop(self)\n\n    def _run_forever_cleanup(self):\n        \"\"\"Clean up after an event loop finishes the looping over events.\n\n        This method exists so that custom custom event loop subclasses (e.g., event loops\n        that integrate a GUI event loop with Python's event loop) have access to all the\n        loop cleanup logic.\n        \"\"\"\n        self._stopping = False\n        self._thread_id = None\n        events._set_running_loop(None)\n        self._set_coroutine_origin_tracking(False)\n        # Restore any pre-existing async generator hooks.\n        if self._old_agen_hooks is not None:\n            sys.set_asyncgen_hooks(*self._old_agen_hooks)\n            self._old_agen_hooks = None\n\n    def run_forever(self):\n        \"\"\"Run until stop() is called.\"\"\"\n        try:\n            self._run_forever_setup()\n            while True:\n                self._run_once()\n                if self._stopping:\n                    break\n        finally:\n            self._run_forever_cleanup()\n\n    def run_until_complete(self, future):\n        \"\"\"Run until the Future is done.\n\n        If the argument is a coroutine, it is wrapped in a Task.\n\n        WARNING: It would be disastrous to call run_until_complete()\n        with the same coroutine twice -- it would wrap it in two\n        different Tasks and that can't be good.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n\n        new_task = not futures.isfuture(future)\n        future = tasks.ensure_future(future, loop=self)\n        if new_task:\n            # An exception is raised if the future didn't complete, so there\n            # is no need to log the \"destroy pending task\" message\n            future._log_destroy_pending = False\n\n        future.add_done_callback(_run_until_complete_cb)\n        try:\n            self.run_forever()\n        except:\n            if new_task and future.done() and not future.cancelled():\n                # The coroutine raised a BaseException. Consume the exception\n                # to not log a warning, the caller doesn't have access to the\n                # local task.\n                future.exception()\n            raise\n        finally:\n            future.remove_done_callback(_run_until_complete_cb)\n        if not future.done():\n            raise RuntimeError('Event loop stopped before Future completed.')\n\n        return future.result()\n\n    def stop(self):\n        \"\"\"Stop running the event loop.\n\n        Every callback already scheduled will still run.  This simply informs\n        run_forever to stop looping after a complete iteration.\n        \"\"\"\n        self._stopping = True\n\n    def close(self):\n        \"\"\"Close the event loop.\n\n        This clears the queues and shuts down the executor,\n        but does not wait for the executor to finish.\n\n        The event loop must not be running.\n        \"\"\"\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self._closed:\n            return\n        if self._debug:\n            logger.debug(\"Close %r\", self)\n        self._closed = True\n        self._ready.clear()\n        self._scheduled.clear()\n        self._executor_shutdown_called = True\n        executor = self._default_executor\n        if executor is not None:\n            self._default_executor = None\n            executor.shutdown(wait=False)\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        return self._closed\n\n    def __del__(self, _warn=warnings.warn):\n        if not self.is_closed():\n            _warn(f\"unclosed event loop {self!r}\", ResourceWarning, source=self)\n            if not self.is_running():\n                self.close()\n\n    def is_running(self):\n        \"\"\"Returns True if the event loop is running.\"\"\"\n        return (self._thread_id is not None)\n\n    def time(self):\n        \"\"\"Return the time according to the event loop's clock.\n\n        This is a float expressed in seconds since an epoch, but the\n        epoch, precision, accuracy and drift are unspecified and may\n        differ per event loop.\n        \"\"\"\n        return time.monotonic()\n\n    def call_later(self, delay, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called at a given time.\n\n        Return a Handle: an opaque object with a cancel() method that\n        can be used to cancel the call.\n\n        The delay can be an int or float, expressed in seconds.  It is\n        always relative to the current time.\n\n        Each callback will be called exactly once.  If two callbacks\n        are scheduled for exactly the same time, it is undefined which\n        will be called first.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        if delay is None:\n            raise TypeError('delay must not be None')\n        timer = self.call_at(self.time() + delay, callback, *args,\n                             context=context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        return timer\n\n    def call_at(self, when, callback, *args, context=None):\n        \"\"\"Like call_later(), but uses an absolute time.\n\n        Absolute time corresponds to the event loop's time() method.\n        \"\"\"\n        if when is None:\n            raise TypeError(\"when cannot be None\")\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_at')\n        timer = events.TimerHandle(when, callback, args, self, context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        heapq.heappush(self._scheduled, timer)\n        timer._scheduled = True\n        return timer\n\n    def call_soon(self, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called as soon as possible.\n\n        This operates as a FIFO queue: callbacks are called in the\n        order in which they are registered.  Each callback will be\n        called exactly once.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_soon')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        return handle\n\n    def _check_callback(self, callback, method):\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\n                f\"coroutines cannot be used with {method}()\")\n        if not callable(callback):\n            raise TypeError(\n                f'a callable object was expected by {method}(), '\n                f'got {callback!r}')\n\n    def _call_soon(self, callback, args, context):\n        handle = events.Handle(callback, args, self, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._ready.append(handle)\n        return handle\n\n    def _check_thread(self):\n        \"\"\"Check that the current thread is the thread running the event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.\n        \"\"\"\n        if self._thread_id is None:\n            return\n        thread_id = threading.get_ident()\n        if thread_id != self._thread_id:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        \"\"\"Like call_soon(), but thread-safe.\"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_callback(callback, 'call_soon_threadsafe')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._write_to_self()\n        return handle\n\n    def run_in_executor(self, executor, func, *args):\n        self._check_closed()\n        if self._debug:\n            self._check_callback(func, 'run_in_executor')\n        if executor is None:\n            executor = self._default_executor\n            # Only check when the default executor is being used\n            self._check_default_executor()\n            if executor is None:\n                executor = concurrent.futures.ThreadPoolExecutor(\n                    thread_name_prefix='asyncio'\n                )\n                self._default_executor = executor\n        return futures.wrap_future(\n            executor.submit(func, *args), loop=self)\n\n    def set_default_executor(self, executor):\n        if not isinstance(executor, concurrent.futures.ThreadPoolExecutor):\n            raise TypeError('executor must be ThreadPoolExecutor instance')\n        self._default_executor = executor\n\n    def _getaddrinfo_debug(self, host, port, family, type, proto, flags):\n        msg = [f\"{host}:{port!r}\"]\n        if family:\n            msg.append(f'family={family!r}')\n        if type:\n            msg.append(f'type={type!r}')\n        if proto:\n            msg.append(f'proto={proto!r}')\n        if flags:\n            msg.append(f'flags={flags!r}')\n        msg = ', '.join(msg)\n        logger.debug('Get address info %s', msg)\n\n        t0 = self.time()\n        addrinfo = socket.getaddrinfo(host, port, family, type, proto, flags)\n        dt = self.time() - t0\n\n        msg = f'Getting address info {msg} took {dt * 1e3:.3f}ms: {addrinfo!r}'\n        if dt >= self.slow_callback_duration:\n            logger.info(msg)\n        else:\n            logger.debug(msg)\n        return addrinfo\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        if self._debug:\n            getaddr_func = self._getaddrinfo_debug\n        else:\n            getaddr_func = socket.getaddrinfo\n\n        return await self.run_in_executor(\n            None, getaddr_func, host, port, family, type, proto, flags)\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        return await self.run_in_executor(\n            None, socket.getnameinfo, sockaddr, flags)\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=True):\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        _check_ssl_socket(sock)\n        self._check_sendfile_params(sock, file, offset, count)\n        try:\n            return await self._sock_sendfile_native(sock, file,\n                                                    offset, count)\n        except exceptions.SendfileNotAvailableError as exc:\n            if not fallback:\n                raise\n        return await self._sock_sendfile_fallback(sock, file,\n                                                  offset, count)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        # NB: sendfile syscall is not supported for SSL sockets and\n        # non-mmap files even if sendfile is supported by OS\n        raise exceptions.SendfileNotAvailableError(\n            f\"syscall sendfile is not available for socket {sock!r} \"\n            f\"and file {file!r} combination\")\n\n    async def _sock_sendfile_fallback(self, sock, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = (\n            min(count, constants.SENDFILE_FALLBACK_READBUFFER_SIZE)\n            if count else constants.SENDFILE_FALLBACK_READBUFFER_SIZE\n        )\n        buf = bytearray(blocksize)\n        total_sent = 0\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    break  # EOF\n                await self.sock_sendall(sock, view[:read])\n                total_sent += read\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, sock, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not sock.type == socket.SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n        if not isinstance(offset, int):\n            raise TypeError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n        if offset < 0:\n            raise ValueError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n\n    async def _connect_sock(self, exceptions, addr_info, local_addr_infos=None):\n        \"\"\"Create, bind and connect one socket.\"\"\"\n        my_exceptions = []\n        exceptions.append(my_exceptions)\n        family, type_, proto, _, address = addr_info\n        sock = None\n        try:\n            sock = socket.socket(family=family, type=type_, proto=proto)\n            sock.setblocking(False)\n            if local_addr_infos is not None:\n                for lfamily, _, _, _, laddr in local_addr_infos:\n                    # skip local addresses of different family\n                    if lfamily != family:\n                        continue\n                    try:\n                        sock.bind(laddr)\n                        break\n                    except OSError as exc:\n                        msg = (\n                            f'error while attempting to bind on '\n                            f'address {laddr!r}: {str(exc).lower()}'\n                        )\n                        exc = OSError(exc.errno, msg)\n                        my_exceptions.append(exc)\n                else:  # all bind attempts failed\n                    if my_exceptions:\n                        raise my_exceptions.pop()\n                    else:\n                        raise OSError(f\"no matching local address with {family=} found\")\n            await self.sock_connect(sock, address)\n            return sock\n        except OSError as exc:\n            my_exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n            raise\n        except:\n            if sock is not None:\n                sock.close()\n            raise\n        finally:\n            exceptions = my_exceptions = None\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0,\n            proto=0, flags=0, sock=None,\n            local_addr=None, server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            happy_eyeballs_delay=None, interleave=None,\n            all_errors=False):\n        \"\"\"Connect to a TCP server.\n\n        Create a streaming transport connection to a given internet host and\n        port: socket family AF_INET or socket.AF_INET6 depending on host (or\n        family if specified), socket type SOCK_STREAM. protocol_factory must be\n        a callable returning a protocol instance.\n\n        This method is a coroutine which will try to establish the connection\n        in the background.  When successful, the coroutine returns a\n        (transport, protocol) pair.\n        \"\"\"\n        if server_hostname is not None and not ssl:\n            raise ValueError('server_hostname is only meaningful with ssl')\n\n        if server_hostname is None and ssl:\n            # Use host as default for server_hostname.  It is an error\n            # if host is empty or not set, e.g. when an\n            # already-connected socket was passed or when only a port\n            # is given.  To avoid this error, you can pass\n            # server_hostname='' -- this will bypass the hostname\n            # check.  (This also means that if host is a numeric\n            # IP/IPv6 address, we will attempt to verify that exact\n            # address; this will probably fail, but it is possible to\n            # create a certificate for a specific IP address, so we\n            # don't judge it here.)\n            if not host:\n                raise ValueError('You must set server_hostname '\n                                 'when using ssl without a host')\n            server_hostname = host\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if happy_eyeballs_delay is not None and interleave is None:\n            # If using happy eyeballs, default to interleave addresses by family\n            interleave = 1\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            infos = await self._ensure_resolved(\n                (host, port), family=family,\n                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)\n            if not infos:\n                raise OSError('getaddrinfo() returned empty list')\n\n            if local_addr is not None:\n                laddr_infos = await self._ensure_resolved(\n                    local_addr, family=family,\n                    type=socket.SOCK_STREAM, proto=proto,\n                    flags=flags, loop=self)\n                if not laddr_infos:\n                    raise OSError('getaddrinfo() returned empty list')\n            else:\n                laddr_infos = None\n\n            if interleave:\n                infos = _interleave_addrinfos(infos, interleave)\n\n            exceptions = []\n            if happy_eyeballs_delay is None:\n                # not using happy eyeballs\n                for addrinfo in infos:\n                    try:\n                        sock = await self._connect_sock(\n                            exceptions, addrinfo, laddr_infos)\n                        break\n                    except OSError:\n                        continue\n            else:  # using happy eyeballs\n                sock, _, _ = await staggered.staggered_race(\n                    (functools.partial(self._connect_sock,\n                                       exceptions, addrinfo, laddr_infos)\n                     for addrinfo in infos),\n                    happy_eyeballs_delay, loop=self)\n\n            if sock is None:\n                exceptions = [exc for sub in exceptions for exc in sub]\n                try:\n                    if all_errors:\n                        raise ExceptionGroup(\"create_connection failed\", exceptions)\n                    if len(exceptions) == 1:\n                        raise exceptions[0]\n                    else:\n                        # If they all have the same str(), raise one.\n                        model = str(exceptions[0])\n                        if all(str(exc) == model for exc in exceptions):\n                            raise exceptions[0]\n                        # Raise a combined exception so the user can see all\n                        # the various error messages.\n                        raise OSError('Multiple exceptions: {}'.format(\n                            ', '.join(str(exc) for exc in exceptions)))\n                finally:\n                    exceptions = None\n\n        else:\n            if sock is None:\n                raise ValueError(\n                    'host and port was not specified and no sock specified')\n            if sock.type != socket.SOCK_STREAM:\n                # We allow AF_INET, AF_INET6, AF_UNIX as long as they\n                # are SOCK_STREAM.\n                # We support passing AF_UNIX sockets even though we have\n                # a dedicated API for that: create_unix_connection.\n                # Disallowing AF_UNIX in this method, breaks backwards\n                # compatibility.\n                raise ValueError(\n                    f'A Stream Socket was expected, got {sock!r}')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r connected to %s:%r: (%r, %r)\",\n                         sock, host, port, transport, protocol)\n        return transport, protocol\n\n    async def _create_connection_transport(\n            self, sock, protocol_factory, ssl,\n            server_hostname, server_side=False,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n\n        sock.setblocking(False)\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        if ssl:\n            sslcontext = None if isinstance(ssl, bool) else ssl\n            transport = self._make_ssl_transport(\n                sock, protocol, sslcontext, waiter,\n                server_side=server_side, server_hostname=server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout,\n                ssl_shutdown_timeout=ssl_shutdown_timeout)\n        else:\n            transport = self._make_socket_transport(sock, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file to transport.\n\n        Return the total number of bytes which were sent.\n\n        The method uses high-performance os.sendfile if available.\n\n        file must be a regular file object opened in binary mode.\n\n        offset tells from where to start reading the file. If specified,\n        count is the total number of bytes to transmit as opposed to\n        sending the file until EOF is reached. File position is updated on\n        return or also in case of error in which case file.tell()\n        can be used to figure out the number of bytes\n        which were sent.\n\n        fallback set to True makes asyncio to manually read and send\n        the file when the platform does not support the sendfile syscall\n        (e.g. Windows or SSL socket on Unix).\n\n        Raise SendfileNotAvailableError if the system does not support\n        sendfile syscall and fallback is False.\n        \"\"\"\n        if transport.is_closing():\n            raise RuntimeError(\"Transport is closing\")\n        mode = getattr(transport, '_sendfile_compatible',\n                       constants._SendfileMode.UNSUPPORTED)\n        if mode is constants._SendfileMode.UNSUPPORTED:\n            raise RuntimeError(\n                f\"sendfile is not supported for transport {transport!r}\")\n        if mode is constants._SendfileMode.TRY_NATIVE:\n            try:\n                return await self._sendfile_native(transport, file,\n                                                   offset, count)\n            except exceptions.SendfileNotAvailableError as exc:\n                if not fallback:\n                    raise\n\n        if not fallback:\n            raise RuntimeError(\n                f\"fallback is disabled and native sendfile is not \"\n                f\"supported for transport {transport!r}\")\n\n        return await self._sendfile_fallback(transport, file,\n                                             offset, count)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        raise exceptions.SendfileNotAvailableError(\n            \"sendfile syscall is not supported\")\n\n    async def _sendfile_fallback(self, transp, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 16384) if count else 16384\n        buf = bytearray(blocksize)\n        total_sent = 0\n        proto = _SendfileFallbackProtocol(transp)\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        return total_sent\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    return total_sent  # EOF\n                await proto.drain()\n                transp.write(view[:read])\n                total_sent += read\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n            await proto.restore()\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None,\n                        ssl_shutdown_timeout=None):\n        \"\"\"Upgrade transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        if ssl is None:\n            raise RuntimeError('Python ssl module is not available')\n\n        if not isinstance(sslcontext, ssl.SSLContext):\n            raise TypeError(\n                f'sslcontext is expected to be an instance of ssl.SSLContext, '\n                f'got {sslcontext!r}')\n\n        if not getattr(transport, '_start_tls_compatible', False):\n            raise TypeError(\n                f'transport {transport!r} is not supported by start_tls()')\n\n        waiter = self.create_future()\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout,\n            call_connection_made=False)\n\n        # Pause early so that \"ssl_protocol.data_received()\" doesn't\n        # have a chance to get called before \"ssl_protocol.connection_made()\".\n        transport.pause_reading()\n\n        transport.set_protocol(ssl_protocol)\n        conmade_cb = self.call_soon(ssl_protocol.connection_made, transport)\n        resume_cb = self.call_soon(transport.resume_reading)\n\n        try:\n            await waiter\n        except BaseException:\n            transport.close()\n            conmade_cb.cancel()\n            resume_cb.cancel()\n            raise\n\n        return ssl_protocol._app_transport\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"Create datagram connection.\"\"\"\n        if sock is not None:\n            if sock.type == socket.SOCK_STREAM:\n                raise ValueError(\n                    f'A datagram socket was expected, got {sock!r}')\n            if (local_addr or remote_addr or\n                    family or proto or flags or\n                    reuse_port or allow_broadcast):\n                # show the problematic kwargs in exception msg\n                opts = dict(local_addr=local_addr, remote_addr=remote_addr,\n                            family=family, proto=proto, flags=flags,\n                            reuse_port=reuse_port,\n                            allow_broadcast=allow_broadcast)\n                problems = ', '.join(f'{k}={v}' for k, v in opts.items() if v)\n                raise ValueError(\n                    f'socket modifier keyword arguments can not be used '\n                    f'when sock is specified. ({problems})')\n            sock.setblocking(False)\n            r_addr = None\n        else:\n            if not (local_addr or remote_addr):\n                if family == 0:\n                    raise ValueError('unexpected address family')\n                addr_pairs_info = (((family, proto), (None, None)),)\n            elif hasattr(socket, 'AF_UNIX') and family == socket.AF_UNIX:\n                for addr in (local_addr, remote_addr):\n                    if addr is not None and not isinstance(addr, str):\n                        raise TypeError('string is expected')\n\n                if local_addr and local_addr[0] not in (0, '\\x00'):\n                    try:\n                        if stat.S_ISSOCK(os.stat(local_addr).st_mode):\n                            os.remove(local_addr)\n                    except FileNotFoundError:\n                        pass\n                    except OSError as err:\n                        # Directory may have permissions only to create socket.\n                        logger.error('Unable to check or remove stale UNIX '\n                                     'socket %r: %r',\n                                     local_addr, err)\n\n                addr_pairs_info = (((family, proto),\n                                    (local_addr, remote_addr)), )\n            else:\n                # join address by (family, protocol)\n                addr_infos = {}  # Using order preserving dict\n                for idx, addr in ((0, local_addr), (1, remote_addr)):\n                    if addr is not None:\n                        if not (isinstance(addr, tuple) and len(addr) == 2):\n                            raise TypeError('2-tuple is expected')\n\n                        infos = await self._ensure_resolved(\n                            addr, family=family, type=socket.SOCK_DGRAM,\n                            proto=proto, flags=flags, loop=self)\n                        if not infos:\n                            raise OSError('getaddrinfo() returned empty list')\n\n                        for fam, _, pro, _, address in infos:\n                            key = (fam, pro)\n                            if key not in addr_infos:\n                                addr_infos[key] = [None, None]\n                            addr_infos[key][idx] = address\n\n                # each addr has to have info for each (family, proto) pair\n                addr_pairs_info = [\n                    (key, addr_pair) for key, addr_pair in addr_infos.items()\n                    if not ((local_addr and addr_pair[0] is None) or\n                            (remote_addr and addr_pair[1] is None))]\n\n                if not addr_pairs_info:\n                    raise ValueError('can not get address information')\n\n            exceptions = []\n\n            for ((family, proto),\n                 (local_address, remote_address)) in addr_pairs_info:\n                sock = None\n                r_addr = None\n                try:\n                    sock = socket.socket(\n                        family=family, type=socket.SOCK_DGRAM, proto=proto)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if allow_broadcast:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n                    sock.setblocking(False)\n\n                    if local_addr:\n                        sock.bind(local_address)\n                    if remote_addr:\n                        if not allow_broadcast:\n                            await self.sock_connect(sock, remote_address)\n                        r_addr = remote_address\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                raise exceptions[0]\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_datagram_transport(\n            sock, protocol, r_addr, waiter)\n        if self._debug:\n            if local_addr:\n                logger.info(\"Datagram endpoint local_addr=%r remote_addr=%r \"\n                            \"created: (%r, %r)\",\n                            local_addr, remote_addr, transport, protocol)\n            else:\n                logger.debug(\"Datagram endpoint remote_addr=%r created: \"\n                             \"(%r, %r)\",\n                             remote_addr, transport, protocol)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def _ensure_resolved(self, address, *,\n                               family=0, type=socket.SOCK_STREAM,\n                               proto=0, flags=0, loop):\n        host, port = address[:2]\n        info = _ipaddr_info(host, port, family, type, proto, *address[2:])\n        if info is not None:\n            # \"host\" is already a resolved IP.\n            return [info]\n        else:\n            return await loop.getaddrinfo(host, port, family=family, type=type,\n                                          proto=proto, flags=flags)\n\n    async def _create_server_getaddrinfo(self, host, port, family, flags):\n        infos = await self._ensure_resolved((host, port), family=family,\n                                            type=socket.SOCK_STREAM,\n                                            flags=flags, loop=self)\n        if not infos:\n            raise OSError(f'getaddrinfo({host!r}) returned empty list')\n        return infos\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *,\n            family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE,\n            sock=None,\n            backlog=100,\n            ssl=None,\n            reuse_address=None,\n            reuse_port=None,\n            keep_alive=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"Create a TCP server.\n\n        The host parameter can be a string, in that case the TCP server is\n        bound to host and port.\n\n        The host parameter can also be a sequence of strings and in that case\n        the TCP server is bound to all hosts of the sequence. If a host\n        appears multiple times (possibly indirectly e.g. when hostnames\n        resolve to the same IP address), the server is only bound once to that\n        host.\n\n        Return a Server object which can be used to stop the service.\n\n        This method is a coroutine.\n        \"\"\"\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            if reuse_address is None:\n                reuse_address = os.name == \"posix\" and sys.platform != \"cygwin\"\n            sockets = []\n            if host == '':\n                hosts = [None]\n            elif (isinstance(host, str) or\n                  not isinstance(host, collections.abc.Iterable)):\n                hosts = [host]\n            else:\n                hosts = host\n\n            fs = [self._create_server_getaddrinfo(host, port, family=family,\n                                                  flags=flags)\n                  for host in hosts]\n            infos = await tasks.gather(*fs)\n            infos = set(itertools.chain.from_iterable(infos))\n\n            completed = False\n            try:\n                for res in infos:\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                    except socket.error:\n                        # Assume it's a bad family/type/protocol combination.\n                        if self._debug:\n                            logger.warning('create_server() failed to create '\n                                           'socket.socket(%r, %r, %r)',\n                                           af, socktype, proto, exc_info=True)\n                        continue\n                    sockets.append(sock)\n                    if reuse_address:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if keep_alive:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_KEEPALIVE, True)\n                    # Disable IPv4/IPv6 dual stack support (enabled by\n                    # default on Linux) which makes a single socket\n                    # listen on both address families.\n                    if (_HAS_IPv6 and\n                            af == socket.AF_INET6 and\n                            hasattr(socket, 'IPPROTO_IPV6')):\n                        sock.setsockopt(socket.IPPROTO_IPV6,\n                                        socket.IPV6_V6ONLY,\n                                        True)\n                    try:\n                        sock.bind(sa)\n                    except OSError as err:\n                        msg = ('error while attempting '\n                               'to bind on address %r: %s'\n                               % (sa, str(err).lower()))\n                        if err.errno == errno.EADDRNOTAVAIL:\n                            # Assume the family is not enabled (bpo-30945)\n                            sockets.pop()\n                            sock.close()\n                            if self._debug:\n                                logger.warning(msg)\n                            continue\n                        raise OSError(err.errno, msg) from None\n\n                if not sockets:\n                    raise OSError('could not bind on any address out of %r'\n                                  % ([info[4] for info in infos],))\n\n                completed = True\n            finally:\n                if not completed:\n                    for sock in sockets:\n                        sock.close()\n        else:\n            if sock is None:\n                raise ValueError('Neither host/port nor sock were specified')\n            if sock.type != socket.SOCK_STREAM:\n                raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n            sockets = [sock]\n\n        for sock in sockets:\n            sock.setblocking(False)\n\n        server = Server(self, sockets, protocol_factory,\n                        ssl, backlog, ssl_handshake_timeout,\n                        ssl_shutdown_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        if self._debug:\n            logger.info(\"%r is serving\", server)\n        return server\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        if sock.type != socket.SOCK_STREAM:\n            raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, '', server_side=True,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r handled: (%r, %r)\", sock, transport, protocol)\n        return transport, protocol\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_read_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Read pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_write_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Write pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    def _log_subprocess(self, msg, stdin, stdout, stderr):\n        info = [msg]\n        if stdin is not None:\n            info.append(f'stdin={_format_pipe(stdin)}')\n        if stdout is not None and stderr == subprocess.STDOUT:\n            info.append(f'stdout=stderr={_format_pipe(stdout)}')\n        else:\n            if stdout is not None:\n                info.append(f'stdout={_format_pipe(stdout)}')\n            if stderr is not None:\n                info.append(f'stderr={_format_pipe(stderr)}')\n        logger.debug(' '.join(info))\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               universal_newlines=False,\n                               shell=True, bufsize=0,\n                               encoding=None, errors=None, text=None,\n                               **kwargs):\n        if not isinstance(cmd, (bytes, str)):\n            raise ValueError(\"cmd must be a string\")\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if not shell:\n            raise ValueError(\"shell must be True\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = 'run shell command %r' % cmd\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, cmd, True, stdin, stdout, stderr, bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    async def subprocess_exec(self, protocol_factory, program, *args,\n                              stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE, universal_newlines=False,\n                              shell=False, bufsize=0,\n                              encoding=None, errors=None, text=None,\n                              **kwargs):\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if shell:\n            raise ValueError(\"shell must be False\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        popen_args = (program,) + args\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = f'execute program {program!r}'\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, popen_args, False, stdin, stdout, stderr,\n            bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    def get_exception_handler(self):\n        \"\"\"Return an exception handler, or None if the default one is in use.\n        \"\"\"\n        return self._exception_handler\n\n    def set_exception_handler(self, handler):\n        \"\"\"Set handler as the new event loop exception handler.\n\n        If handler is None, the default exception handler will\n        be set.\n\n        If handler is a callable object, it should have a\n        signature matching '(loop, context)', where 'loop'\n        will be a reference to the active event loop, 'context'\n        will be a dict object (see `call_exception_handler()`\n        documentation for details about context).\n        \"\"\"\n        if handler is not None and not callable(handler):\n            raise TypeError(f'A callable object or None is expected, '\n                            f'got {handler!r}')\n        self._exception_handler = handler\n\n    def default_exception_handler(self, context):\n        \"\"\"Default exception handler.\n\n        This is called when an exception occurs and no exception\n        handler is set, and can be called by a custom exception\n        handler that wants to defer to the default behavior.\n\n        This default handler logs the error message and other\n        context-dependent information.  In debug mode, a truncated\n        stack trace is also appended showing where the given object\n        (e.g. a handle or future or task) was created, if any.\n\n        The context parameter has the same meaning as in\n        `call_exception_handler()`.\n        \"\"\"\n        message = context.get('message')\n        if not message:\n            message = 'Unhandled exception in event loop'\n\n        exception = context.get('exception')\n        if exception is not None:\n            exc_info = (type(exception), exception, exception.__traceback__)\n        else:\n            exc_info = False\n\n        if ('source_traceback' not in context and\n                self._current_handle is not None and\n                self._current_handle._source_traceback):\n            context['handle_traceback'] = \\\n                self._current_handle._source_traceback\n\n        log_lines = [message]\n        for key in sorted(context):\n            if key in {'message', 'exception'}:\n                continue\n            value = context[key]\n            if key == 'source_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Object created at (most recent call last):\\n'\n                value += tb.rstrip()\n            elif key == 'handle_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Handle created at (most recent call last):\\n'\n                value += tb.rstrip()\n            else:\n                value = repr(value)\n            log_lines.append(f'{key}: {value}')\n\n        logger.error('\\n'.join(log_lines), exc_info=exc_info)\n\n    def call_exception_handler(self, context):\n        \"\"\"Call the current event loop's exception handler.\n\n        The context argument is a dict containing the following keys:\n\n        - 'message': Error message;\n        - 'exception' (optional): Exception object;\n        - 'future' (optional): Future instance;\n        - 'task' (optional): Task instance;\n        - 'handle' (optional): Handle instance;\n        - 'protocol' (optional): Protocol instance;\n        - 'transport' (optional): Transport instance;\n        - 'socket' (optional): Socket instance;\n        - 'asyncgen' (optional): Asynchronous generator that caused\n                                 the exception.\n\n        New keys maybe introduced in the future.\n\n        Note: do not overload this method in an event loop subclass.\n        For custom exception handling, use the\n        `set_exception_handler()` method.\n        \"\"\"\n        if self._exception_handler is None:\n            try:\n                self.default_exception_handler(context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                # Second protection layer for unexpected errors\n                # in the default implementation, as well as for subclassed\n                # event loops with overloaded \"default_exception_handler\".\n                logger.error('Exception in default exception handler',\n                             exc_info=True)\n        else:\n            try:\n                ctx = None\n                thing = context.get(\"task\")\n                if thing is None:\n                    # Even though Futures don't have a context,\n                    # Task is a subclass of Future,\n                    # and sometimes the 'future' key holds a Task.\n                    thing = context.get(\"future\")\n                if thing is None:\n                    # Handles also have a context.\n                    thing = context.get(\"handle\")\n                if thing is not None and hasattr(thing, \"get_context\"):\n                    ctx = thing.get_context()\n                if ctx is not None and hasattr(ctx, \"run\"):\n                    ctx.run(self._exception_handler, self, context)\n                else:\n                    self._exception_handler(self, context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                # Exception in the user set custom exception handler.\n                try:\n                    # Let's try default handler.\n                    self.default_exception_handler({\n                        'message': 'Unhandled error in exception handler',\n                        'exception': exc,\n                        'context': context,\n                    })\n                except (SystemExit, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    # Guard 'default_exception_handler' in case it is\n                    # overloaded.\n                    logger.error('Exception in default exception handler '\n                                 'while handling an unexpected error '\n                                 'in custom exception handler',\n                                 exc_info=True)\n\n    def _add_callback(self, handle):\n        \"\"\"Add a Handle to _ready.\"\"\"\n        if not handle._cancelled:\n            self._ready.append(handle)\n\n    def _add_callback_signalsafe(self, handle):\n        \"\"\"Like _add_callback() but called from a signal handler.\"\"\"\n        self._add_callback(handle)\n        self._write_to_self()\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        if handle._scheduled:\n            self._timer_cancelled_count += 1\n\n    def _run_once(self):\n        \"\"\"Run one full iteration of the event loop.\n\n        This calls all currently ready callbacks, polls for I/O,\n        schedules the resulting callbacks, and finally schedules\n        'call_later' callbacks.\n        \"\"\"\n\n        sched_count = len(self._scheduled)\n        if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and\n            self._timer_cancelled_count / sched_count >\n                _MIN_CANCELLED_TIMER_HANDLES_FRACTION):\n            # Remove delayed calls that were cancelled if their number\n            # is too high\n            new_scheduled = []\n            for handle in self._scheduled:\n                if handle._cancelled:\n                    handle._scheduled = False\n                else:\n                    new_scheduled.append(handle)\n\n            heapq.heapify(new_scheduled)\n            self._scheduled = new_scheduled\n            self._timer_cancelled_count = 0\n        else:\n            # Remove delayed calls that were cancelled from head of queue.\n            while self._scheduled and self._scheduled[0]._cancelled:\n                self._timer_cancelled_count -= 1\n                handle = heapq.heappop(self._scheduled)\n                handle._scheduled = False\n\n        timeout = None\n        if self._ready or self._stopping:\n            timeout = 0\n        elif self._scheduled:\n            # Compute the desired timeout.\n            timeout = self._scheduled[0]._when - self.time()\n            if timeout > MAXIMUM_SELECT_TIMEOUT:\n                timeout = MAXIMUM_SELECT_TIMEOUT\n            elif timeout < 0:\n                timeout = 0\n\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n        # Needed to break cycles when an exception occurs.\n        event_list = None\n\n        # Handle 'later' callbacks that are ready.\n        end_time = self.time() + self._clock_resolution\n        while self._scheduled:\n            handle = self._scheduled[0]\n            if handle._when >= end_time:\n                break\n            handle = heapq.heappop(self._scheduled)\n            handle._scheduled = False\n            self._ready.append(handle)\n\n        # This is the only place where callbacks are actually *called*.\n        # All other places just add them to ready.\n        # Note: We run all currently scheduled callbacks, but not any\n        # callbacks scheduled by callbacks run this time around --\n        # they will be run the next time (after another I/O poll).\n        # Use an idiom that is thread-safe without using locks.\n        ntodo = len(self._ready)\n        for i in range(ntodo):\n            handle = self._ready.popleft()\n            if handle._cancelled:\n                continue\n            if self._debug:\n                try:\n                    self._current_handle = handle\n                    t0 = self.time()\n                    handle._run()\n                    dt = self.time() - t0\n                    if dt >= self.slow_callback_duration:\n                        logger.warning('Executing %s took %.3f seconds',\n                                       _format_handle(handle), dt)\n                finally:\n                    self._current_handle = None\n            else:\n                handle._run()\n        handle = None  # Needed to break cycles when an exception occurs.\n\n    def _set_coroutine_origin_tracking(self, enabled):\n        if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n            return\n\n        if enabled:\n            self._coroutine_origin_tracking_saved_depth = (\n                sys.get_coroutine_origin_tracking_depth())\n            sys.set_coroutine_origin_tracking_depth(\n                constants.DEBUG_STACK_DEPTH)\n        else:\n            sys.set_coroutine_origin_tracking_depth(\n                self._coroutine_origin_tracking_saved_depth)\n\n        self._coroutine_origin_tracking_enabled = enabled\n\n    def get_debug(self):\n        return self._debug\n\n    def set_debug(self, enabled):\n        self._debug = enabled\n\n        if self.is_running():\n            self.call_soon_threadsafe(self._set_coroutine_origin_tracking, enabled)\n", 2052], "/usr/lib/python3.13/_weakrefset.py": ["# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\nfrom types import GenericAlias\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard:\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet:\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        discard = self.data.discard\n        while True:\n            try:\n                item = pop()\n            except IndexError:\n                return\n            discard(item)\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return self.__class__, (list(self),), self.__getstate__()\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet') from None\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(map(ref, other))\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(map(ref, other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(map(ref, other))\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\n    def __repr__(self):\n        return repr(self.data)\n\n    __class_getitem__ = classmethod(GenericAlias)\n", 205], "/usr/lib/python3.13/selectors.py": ["\"\"\"Selectors module.\n\nThis module allows high-level and efficient I/O multiplexing, built upon the\n`select` module primitives.\n\"\"\"\n\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import namedtuple\nfrom collections.abc import Mapping\nimport math\nimport select\nimport sys\n\n\n# generic events, that must be mapped to implementation-specific ones\nEVENT_READ = (1 << 0)\nEVENT_WRITE = (1 << 1)\n\n\ndef _fileobj_to_fd(fileobj):\n    \"\"\"Return a file descriptor from a file object.\n\n    Parameters:\n    fileobj -- file object or file descriptor\n\n    Returns:\n    corresponding file descriptor\n\n    Raises:\n    ValueError if the object is invalid\n    \"\"\"\n    if isinstance(fileobj, int):\n        fd = fileobj\n    else:\n        try:\n            fd = int(fileobj.fileno())\n        except (AttributeError, TypeError, ValueError):\n            raise ValueError(\"Invalid file object: \"\n                             \"{!r}\".format(fileobj)) from None\n    if fd < 0:\n        raise ValueError(\"Invalid file descriptor: {}\".format(fd))\n    return fd\n\n\nSelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])\n\nSelectorKey.__doc__ = \"\"\"SelectorKey(fileobj, fd, events, data)\n\n    Object used to associate a file object to its backing\n    file descriptor, selected event mask, and attached data.\n\"\"\"\nSelectorKey.fileobj.__doc__ = 'File object registered.'\nSelectorKey.fd.__doc__ = 'Underlying file descriptor.'\nSelectorKey.events.__doc__ = 'Events that must be waited for on this file object.'\nSelectorKey.data.__doc__ = ('''Optional opaque data associated to this file object.\nFor example, this could be used to store a per-client session ID.''')\n\n\nclass _SelectorMapping(Mapping):\n    \"\"\"Mapping of file objects to selector keys.\"\"\"\n\n    def __init__(self, selector):\n        self._selector = selector\n\n    def __len__(self):\n        return len(self._selector._fd_to_key)\n\n    def get(self, fileobj, default=None):\n        fd = self._selector._fileobj_lookup(fileobj)\n        return self._selector._fd_to_key.get(fd, default)\n\n    def __getitem__(self, fileobj):\n        fd = self._selector._fileobj_lookup(fileobj)\n        key = self._selector._fd_to_key.get(fd)\n        if key is None:\n            raise KeyError(\"{!r} is not registered\".format(fileobj))\n        return key\n\n    def __iter__(self):\n        return iter(self._selector._fd_to_key)\n\n\nclass BaseSelector(metaclass=ABCMeta):\n    \"\"\"Selector abstract base class.\n\n    A selector supports registering file objects to be monitored for specific\n    I/O events.\n\n    A file object is a file descriptor or any object with a `fileno()` method.\n    An arbitrary object can be attached to the file object, which can be used\n    for example to store context information, a callback, etc.\n\n    A selector can use various implementations (select(), poll(), epoll()...)\n    depending on the platform. The default `Selector` class uses the most\n    efficient implementation on the current platform.\n    \"\"\"\n\n    @abstractmethod\n    def register(self, fileobj, events, data=None):\n        \"\"\"Register a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        ValueError if events is invalid\n        KeyError if fileobj is already registered\n        OSError if fileobj is closed or otherwise is unacceptable to\n                the underlying system call (if a system call is made)\n\n        Note:\n        OSError may or may not be raised\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def unregister(self, fileobj):\n        \"\"\"Unregister a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        KeyError if fileobj is not registered\n\n        Note:\n        If fileobj is registered but has since been closed this does\n        *not* raise OSError (even if the wrapped syscall does)\n        \"\"\"\n        raise NotImplementedError\n\n    def modify(self, fileobj, events, data=None):\n        \"\"\"Change a registered file object monitored events or attached data.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        Anything that unregister() or register() raises\n        \"\"\"\n        self.unregister(fileobj)\n        return self.register(fileobj, events, data)\n\n    @abstractmethod\n    def select(self, timeout=None):\n        \"\"\"Perform the actual selection, until some monitored file objects are\n        ready or a timeout expires.\n\n        Parameters:\n        timeout -- if timeout > 0, this specifies the maximum wait time, in\n                   seconds\n                   if timeout <= 0, the select() call won't block, and will\n                   report the currently ready file objects\n                   if timeout is None, select() will block until a monitored\n                   file object becomes ready\n\n        Returns:\n        list of (key, events) for ready file objects\n        `events` is a bitwise mask of EVENT_READ|EVENT_WRITE\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the selector.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        pass\n\n    def get_key(self, fileobj):\n        \"\"\"Return the key associated to a registered file object.\n\n        Returns:\n        SelectorKey for this file object\n        \"\"\"\n        mapping = self.get_map()\n        if mapping is None:\n            raise RuntimeError('Selector is closed')\n        try:\n            return mapping[fileobj]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    @abstractmethod\n    def get_map(self):\n        \"\"\"Return a mapping of file objects to selector keys.\"\"\"\n        raise NotImplementedError\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n\nclass _BaseSelectorImpl(BaseSelector):\n    \"\"\"Base selector implementation.\"\"\"\n\n    def __init__(self):\n        # this maps file descriptors to keys\n        self._fd_to_key = {}\n        # read-only mapping returned by get_map()\n        self._map = _SelectorMapping(self)\n\n    def _fileobj_lookup(self, fileobj):\n        \"\"\"Return a file descriptor from a file object.\n\n        This wraps _fileobj_to_fd() to do an exhaustive search in case\n        the object is invalid but we still have it in our map.  This\n        is used by unregister() so we can unregister an object that\n        was previously registered even if it is closed.  It is also\n        used by _SelectorMapping.\n        \"\"\"\n        try:\n            return _fileobj_to_fd(fileobj)\n        except ValueError:\n            # Do an exhaustive search.\n            for key in self._fd_to_key.values():\n                if key.fileobj is fileobj:\n                    return key.fd\n            # Raise ValueError after all.\n            raise\n\n    def register(self, fileobj, events, data=None):\n        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):\n            raise ValueError(\"Invalid events: {!r}\".format(events))\n\n        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)\n\n        if key.fd in self._fd_to_key:\n            raise KeyError(\"{!r} (FD {}) is already registered\"\n                           .format(fileobj, key.fd))\n\n        self._fd_to_key[key.fd] = key\n        return key\n\n    def unregister(self, fileobj):\n        try:\n            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        if events != key.events:\n            self.unregister(fileobj)\n            key = self.register(fileobj, events, data)\n        elif data != key.data:\n            # Use a shortcut to update the data.\n            key = key._replace(data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def close(self):\n        self._fd_to_key.clear()\n        self._map = None\n\n    def get_map(self):\n        return self._map\n\n\n\nclass SelectSelector(_BaseSelectorImpl):\n    \"\"\"Select-based selector.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._readers = set()\n        self._writers = set()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        if events & EVENT_READ:\n            self._readers.add(key.fd)\n        if events & EVENT_WRITE:\n            self._writers.add(key.fd)\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        self._readers.discard(key.fd)\n        self._writers.discard(key.fd)\n        return key\n\n    if sys.platform == 'win32':\n        def _select(self, r, w, _, timeout=None):\n            r, w, x = select.select(r, w, w, timeout)\n            return r, w + x, []\n    else:\n        _select = select.select\n\n    def select(self, timeout=None):\n        timeout = None if timeout is None else max(timeout, 0)\n        ready = []\n        try:\n            r, w, _ = self._select(self._readers, self._writers, [], timeout)\n        except InterruptedError:\n            return ready\n        r = frozenset(r)\n        w = frozenset(w)\n        rw = r | w\n        fd_to_key_get = self._fd_to_key.get\n        for fd in rw:\n            key = fd_to_key_get(fd)\n            if key:\n                events = ((fd in r and EVENT_READ)\n                          | (fd in w and EVENT_WRITE))\n                ready.append((key, events & key.events))\n        return ready\n\n\nclass _PollLikeSelector(_BaseSelectorImpl):\n    \"\"\"Base class shared between poll, epoll and devpoll selectors.\"\"\"\n    _selector_cls = None\n    _EVENT_READ = None\n    _EVENT_WRITE = None\n\n    def __init__(self):\n        super().__init__()\n        self._selector = self._selector_cls()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        poller_events = ((events & EVENT_READ and self._EVENT_READ)\n                         | (events & EVENT_WRITE and self._EVENT_WRITE) )\n        try:\n            self._selector.register(key.fd, poller_events)\n        except:\n            super().unregister(fileobj)\n            raise\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        try:\n            self._selector.unregister(key.fd)\n        except OSError:\n            # This can happen if the FD was closed since it\n            # was registered.\n            pass\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(f\"{fileobj!r} is not registered\") from None\n\n        changed = False\n        if events != key.events:\n            selector_events = ((events & EVENT_READ and self._EVENT_READ)\n                               | (events & EVENT_WRITE and self._EVENT_WRITE))\n            try:\n                self._selector.modify(key.fd, selector_events)\n            except:\n                super().unregister(fileobj)\n                raise\n            changed = True\n        if data != key.data:\n            changed = True\n\n        if changed:\n            key = key._replace(events=events, data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def select(self, timeout=None):\n        # This is shared between poll() and epoll().\n        # epoll() has a different signature and handling of timeout parameter.\n        if timeout is None:\n            timeout = None\n        elif timeout <= 0:\n            timeout = 0\n        else:\n            # poll() has a resolution of 1 millisecond, round away from\n            # zero to wait *at least* timeout seconds.\n            timeout = math.ceil(timeout * 1e3)\n        ready = []\n        try:\n            fd_event_list = self._selector.poll(timeout)\n        except InterruptedError:\n            return ready\n\n        fd_to_key_get = self._fd_to_key.get\n        for fd, event in fd_event_list:\n            key = fd_to_key_get(fd)\n            if key:\n                events = ((event & ~self._EVENT_READ and EVENT_WRITE)\n                           | (event & ~self._EVENT_WRITE and EVENT_READ))\n                ready.append((key, events & key.events))\n        return ready\n\n\nif hasattr(select, 'poll'):\n\n    class PollSelector(_PollLikeSelector):\n        \"\"\"Poll-based selector.\"\"\"\n        _selector_cls = select.poll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n\nif hasattr(select, 'epoll'):\n\n    _NOT_EPOLLIN = ~select.EPOLLIN\n    _NOT_EPOLLOUT = ~select.EPOLLOUT\n\n    class EpollSelector(_PollLikeSelector):\n        \"\"\"Epoll-based selector.\"\"\"\n        _selector_cls = select.epoll\n        _EVENT_READ = select.EPOLLIN\n        _EVENT_WRITE = select.EPOLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def select(self, timeout=None):\n            if timeout is None:\n                timeout = -1\n            elif timeout <= 0:\n                timeout = 0\n            else:\n                # epoll_wait() has a resolution of 1 millisecond, round away\n                # from zero to wait *at least* timeout seconds.\n                timeout = math.ceil(timeout * 1e3) * 1e-3\n\n            # epoll_wait() expects `maxevents` to be greater than zero;\n            # we want to make sure that `select()` can be called when no\n            # FD is registered.\n            max_ev = len(self._fd_to_key) or 1\n\n            ready = []\n            try:\n                fd_event_list = self._selector.poll(timeout, max_ev)\n            except InterruptedError:\n                return ready\n\n            fd_to_key = self._fd_to_key\n            for fd, event in fd_event_list:\n                key = fd_to_key.get(fd)\n                if key:\n                    events = ((event & _NOT_EPOLLIN and EVENT_WRITE)\n                              | (event & _NOT_EPOLLOUT and EVENT_READ))\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'devpoll'):\n\n    class DevpollSelector(_PollLikeSelector):\n        \"\"\"Solaris /dev/poll selector.\"\"\"\n        _selector_cls = select.devpoll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'kqueue'):\n\n    class KqueueSelector(_BaseSelectorImpl):\n        \"\"\"Kqueue-based selector.\"\"\"\n\n        def __init__(self):\n            super().__init__()\n            self._selector = select.kqueue()\n            self._max_events = 0\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def register(self, fileobj, events, data=None):\n            key = super().register(fileobj, events, data)\n            try:\n                if events & EVENT_READ:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                    self._max_events += 1\n                if events & EVENT_WRITE:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                    self._max_events += 1\n            except:\n                super().unregister(fileobj)\n                raise\n            return key\n\n        def unregister(self, fileobj):\n            key = super().unregister(fileobj)\n            if key.events & EVENT_READ:\n                kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                    select.KQ_EV_DELETE)\n                self._max_events -= 1\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # This can happen if the FD was closed since it\n                    # was registered.\n                    pass\n            if key.events & EVENT_WRITE:\n                kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                    select.KQ_EV_DELETE)\n                self._max_events -= 1\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # See comment above.\n                    pass\n            return key\n\n        def select(self, timeout=None):\n            timeout = None if timeout is None else max(timeout, 0)\n            # If max_ev is 0, kqueue will ignore the timeout. For consistent\n            # behavior with the other selector classes, we prevent that here\n            # (using max). See https://bugs.python.org/issue29255\n            max_ev = self._max_events or 1\n            ready = []\n            try:\n                kev_list = self._selector.control(None, max_ev, timeout)\n            except InterruptedError:\n                return ready\n\n            fd_to_key_get = self._fd_to_key.get\n            for kev in kev_list:\n                fd = kev.ident\n                flag = kev.filter\n                key = fd_to_key_get(fd)\n                if key:\n                    events = ((flag == select.KQ_FILTER_READ and EVENT_READ)\n                              | (flag == select.KQ_FILTER_WRITE and EVENT_WRITE))\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\ndef _can_use(method):\n    \"\"\"Check if we can use the selector depending upon the\n    operating system. \"\"\"\n    # Implementation based upon https://github.com/sethmlarson/selectors2/blob/master/selectors2.py\n    selector = getattr(select, method, None)\n    if selector is None:\n        # select module does not implement method\n        return False\n    # check if the OS and Kernel actually support the method. Call may fail with\n    # OSError: [Errno 38] Function not implemented\n    try:\n        selector_obj = selector()\n        if method == 'poll':\n            # check that poll actually works\n            selector_obj.poll(0)\n        else:\n            # close epoll, kqueue, and devpoll fd\n            selector_obj.close()\n        return True\n    except OSError:\n        return False\n\n\n# Choose the best implementation, roughly:\n#    epoll|kqueue|devpoll > poll > select.\n# select() also can't accept a FD > FD_SETSIZE (usually around 1024)\nif _can_use('kqueue'):\n    DefaultSelector = KqueueSelector\nelif _can_use('epoll'):\n    DefaultSelector = EpollSelector\nelif _can_use('devpoll'):\n    DefaultSelector = DevpollSelector\nelif _can_use('poll'):\n    DefaultSelector = PollSelector\nelse:\n    DefaultSelector = SelectSelector\n", 603], "/usr/lib/python3.13/logging/__init__.py": ["# Copyright 2001-2022 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2022 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom types import GenericAlias\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions', 'getLevelNamesMapping',\n           'getHandlerByName', 'getHandlerNames']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time_ns()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to False\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to False\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to False\n#\nlogProcesses = True\n\n#\n# If you don't want asyncio task information in the log, set this to False\n#\nlogAsyncioTasks = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelNamesMapping():\n    return _nameToLevel.copy()\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual or numeric representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    If a string representation of the level is passed in, the corresponding\n    numeric value is returned.\n\n    If no matching numeric or string value is passed in, the string\n    'Level %s' % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    with _lock:\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n\nif hasattr(sys, \"_getframe\"):\n    currentframe = lambda: sys._getframe(1)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception as exc:\n            return exc.__traceback__.tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# Setting _srcfile to None will prevent findCaller() from being called. This\n# way, you can avoid the overhead of fetching caller information.\n\n# The following is based on warnings._is_internal_frame. It makes sure that\n# frames of the import mechanism are skipped when logging at module level and\n# using a stacklevel value greater than one.\ndef _is_internal_frame(frame):\n    \"\"\"Signal whether the frame is a CPython or logging module internal.\"\"\"\n    filename = os.path.normcase(frame.f_code.co_filename)\n    return filename == _srcfile or (\n        \"importlib\" in filename and \"_bootstrap\" in filename\n    )\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\"\n                        % (level,))\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _prepareFork():\n    \"\"\"\n    Prepare to fork a new child process by acquiring the module-level lock.\n\n    This should be used in conjunction with _afterFork().\n    \"\"\"\n    # Wrap the lock acquisition in a try-except to prevent the lock from being\n    # abandoned in the event of an asynchronous exception. See gh-106238.\n    try:\n        _lock.acquire()\n    except BaseException:\n        _lock.release()\n        raise\n\ndef _afterFork():\n    \"\"\"\n    After a new child process has been forked, release the module-level lock.\n\n    This should be used in conjunction with _prepareFork().\n    \"\"\"\n    _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a _at_fork_reinit method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        with _lock:\n            _at_fork_reinit_lock_weakset.add(instance)\n\n    def _after_at_fork_child_reinit_locks():\n        for handler in _at_fork_reinit_lock_weakset:\n            handler._at_fork_reinit()\n\n        # _prepareFork() was called in the parent before forking.\n        # The lock is reinitialized to unlocked state.\n        _lock._at_fork_reinit()\n\n    os.register_at_fork(before=_prepareFork,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_afterFork)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time_ns()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct / 1e9  # ns to float seconds\n        # Get the number of whole milliseconds (0-999) in the fractional part of seconds.\n        # Eg: 1_677_903_920_999_998_503 ns --> 999_998_503 ns--> 999 ms\n        # Convert to float by adding 0.0 for historical reasons. See gh-89047\n        self.msecs = (ct % 1_000_000_000) // 1_000_000 + 0.0\n        if self.msecs == 999.0 and int(self.created) != ct // 1_000_000_000:\n            # ns -> sec conversion can round up, e.g:\n            # 1_677_903_920_999_999_900 ns --> 1_677_903_921.0 sec\n            self.msecs = 0.0\n\n        self.relativeCreated = (ct - _startTime) / 1e6\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n        self.taskName = None\n        if logAsyncioTasks:\n            asyncio = sys.modules.get('asyncio')\n            if asyncio:\n                try:\n                    self.taskName = asyncio.current_task().get_name()\n                except Exception:\n                    pass\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt, *, defaults=None):\n        self._fmt = fmt or self.default_format\n        self._defaults = defaults\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt % values\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt.format(**values)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._tpl.substitute(**values)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time_ns() / 1e9\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(taskName)s        Task name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True, *,\n                 defaults=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt, defaults=defaults)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            s = time.strftime(self.default_time_format, ct)\n            if self.default_msec_format:\n                s = self.default_msec_format % (s, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, limit=None, file=sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Returns True if the record should be logged, or False otherwise.\n        If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this by returning a false value.\n        If a filter attached to a handler returns a log record instance,\n        then that instance is used in place of the original log record in\n        any further processing of the event by that handler.\n        If a filter returns any other true value, the original log record\n        is used in any further processing of the event by that handler.\n\n        If none of the filters return false values, this method returns\n        a log record.\n        If any of the filters return a false value, this method returns\n        a false value.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n\n        .. versionchanged:: 3.12\n           Allow filters to return a LogRecord instead of\n           modifying it in place.\n        \"\"\"\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                return False\n            if isinstance(result, LogRecord):\n                record = result\n        return record\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    handlers, lock = _handlerList, _lock\n    if lock and handlers:\n        with lock:\n            try:\n                handlers.remove(wr)\n            except ValueError:\n                pass\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    with _lock:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n\n\ndef getHandlerByName(name):\n    \"\"\"\n    Get a handler with the specified *name*, or None if there isn't one with\n    that name.\n    \"\"\"\n    return _handlers.get(name)\n\n\ndef getHandlerNames():\n    \"\"\"\n    Return all known handler names as an immutable set.\n    \"\"\"\n    return frozenset(_handlers)\n\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        self._closed = False\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        with _lock:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def _at_fork_reinit(self):\n        self.lock._at_fork_reinit()\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock.\n\n        Returns an instance of the log record that was emitted\n        if it passed all filters, otherwise a false value is returned.\n        \"\"\"\n        rv = self.filter(record)\n        if isinstance(rv, LogRecord):\n            record = rv\n        if rv:\n            with self.lock:\n                self.emit(record)\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        with _lock:\n            self._closed = True\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            exc = sys.exception()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(exc, limit=None, file=sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = exc.__traceback__.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del exc\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        with self.lock:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            with self.lock:\n                self.flush()\n                self.stream = stream\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False, errors=None):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        if \"b\" not in mode:\n            self.encoding = io.text_encoding(encoding)\n        self.errors = errors\n        self.delay = delay\n        # bpo-26789: FileHandler keeps a reference to the builtin open()\n        # function to be able to open or reopen the file during Python\n        # finalization.\n        self._builtin_open = open\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        with self.lock:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                # Also see Issue #42378: we also rely on\n                # self._closed being set to True there\n                StreamHandler.close(self)\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        open_func = self._builtin_open\n        return open_func(self.baseFilename, self.mode,\n                         encoding=self.encoding, errors=self.errors)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n\n        If stream is not open, current mode is 'w' and `_closed=True`, record\n        will not be emitted (see Issue #42378).\n        \"\"\"\n        if self.stream is None:\n            if self.mode != 'w' or not self._closed:\n                self.stream = self._open()\n        if self.stream:\n            StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    @property\n    def disable(self):\n        return self._disable\n\n    @disable.setter\n    def disable(self, value):\n        self._disable = _checkLevel(value)\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        with _lock:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        with _lock:\n            for logger in self.loggerDict.values():\n                if isinstance(logger, Logger):\n                    logger._cache.clear()\n            self.root._cache.clear()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"notable problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    def fatal(self, msg, *args, **kwargs):\n        \"\"\"\n        Don't use this method, use critical() instead.\n        \"\"\"\n        self.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=True)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is None:\n            return \"(unknown file)\", 0, \"(unknown function)\", None\n        while stacklevel > 0:\n            next_f = f.f_back\n            if next_f is None:\n                ## We've got options here.\n                ## If we want to use the last (deepest) frame:\n                break\n                ## If we want to mimic the warnings module:\n                #return (\"sys\", 1, \"(unknown function)\", None)\n                ## If we want to be pedantic:\n                #raise ValueError(\"call stack is not deep enough\")\n            f = next_f\n            if not _is_internal_frame(f):\n                stacklevel -= 1\n        co = f.f_code\n        sinfo = None\n        if stack_info:\n            with io.StringIO() as sio:\n                sio.write(\"Stack (most recent call last):\\n\")\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n        return co.co_filename, f.f_lineno, co.co_name, sinfo\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if self.disabled:\n            return\n        maybe_record = self.filter(record)\n        if not maybe_record:\n            return\n        if isinstance(maybe_record, LogRecord):\n            record = maybe_record\n        self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        with _lock:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        with _lock:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            with _lock:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def getChildren(self):\n\n        def _hierlevel(logger):\n            if logger is logger.manager.root:\n                return 0\n            return 1 + logger.name.count('.')\n\n        d = self.manager.loggerDict\n        with _lock:\n            # exclude PlaceHolders - the last check is to ensure that lower-level\n            # descendants aren't returned - if there are placeholders, a logger's\n            # parent field might point to a grandparent or ancestor thereof.\n            return set(item for item in d.values()\n                       if isinstance(item, Logger) and item.parent is self and\n                       _hierlevel(item) == 1 + _hierlevel(item.parent))\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra=None, merge_extra=False):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n\n        By default, LoggerAdapter objects will drop the \"extra\" argument\n        passed on the individual log calls to use its own instead.\n\n        Initializing it with merge_extra=True will instead merge both\n        maps when logging, the individual call extra taking precedence\n        over the LoggerAdapter instance extra\n\n        .. versionchanged:: 3.13\n           The *merge_extra* argument was added.\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n        self.merge_extra = merge_extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        if self.merge_extra and \"extra\" in kwargs:\n            kwargs[\"extra\"] = {**self.extra, **kwargs[\"extra\"]}\n        else:\n            kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, **kwargs):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(level, msg, args, **kwargs)\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root logger. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    encoding  If specified together with a filename, this encoding is passed to\n              the created FileHandler, causing it to be used when the file is\n              opened.\n    errors    If specified together with a filename, this value is passed to the\n              created FileHandler, causing it to be used when the file is\n              opened in text mode. If not specified, the default value is\n              `backslashreplace`.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.9\n       Added the ``encoding`` and ``errors`` parameters.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    with _lock:\n        force = kwargs.pop('force', False)\n        encoding = kwargs.pop('encoding', None)\n        errors = kwargs.pop('errors', 'backslashreplace')\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    if 'b' in mode:\n                        errors = None\n                    else:\n                        encoding = io.text_encoding(encoding)\n                    h = FileHandler(filename, mode,\n                                    encoding=encoding, errors=errors)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == root.name:\n        return root\n    return Logger.manager.getLogger(name)\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\ndef fatal(msg, *args, **kwargs):\n    \"\"\"\n    Don't use this function, use critical() instead.\n    \"\"\"\n    critical(msg, *args, **kwargs)\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    # MemoryHandlers might not want to be flushed on close,\n                    # but circular imports prevent us scoping this to just\n                    # those handlers.  hence the default to True.\n                    if getattr(h, 'flushOnClose', True):\n                        h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n    def _at_fork_reinit(self):\n        pass\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        # bpo-46557: Log str(s) as msg instead of logger.warning(\"%s\", s)\n        # since some log aggregation tools group logs by the msg arg\n        logger.warning(str(s))\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 2324], "/usr/lib/python3.13/socket.py": ["# Wrapper module for _socket, providing some additional facilities\n# implemented in Python.\n\n\"\"\"\\\nThis module provides socket operations and some related functions.\nOn Unix, it supports IP (Internet Protocol) and Unix domain sockets.\nOn other systems, it only supports IP. Functions specific for a\nsocket are available as methods of the socket object.\n\nFunctions:\n\nsocket() -- create a new socket object\nsocketpair() -- create a pair of new socket objects [*]\nfromfd() -- create a socket object from an open file descriptor [*]\nsend_fds() -- Send file descriptor to the socket.\nrecv_fds() -- Receive file descriptors from the socket.\nfromshare() -- create a socket object from data received from socket.share() [*]\ngethostname() -- return the current hostname\ngethostbyname() -- map a hostname to its IP number\ngethostbyaddr() -- map an IP number or hostname to DNS info\ngetservbyname() -- map a service name and a protocol name to a port number\ngetprotobyname() -- map a protocol name (e.g. 'tcp') to a number\nntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order\nhtons(), htonl() -- convert 16, 32 bit int from host to network byte order\ninet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format\ninet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)\nsocket.getdefaulttimeout() -- get the default timeout value\nsocket.setdefaulttimeout() -- set the default timeout value\ncreate_connection() -- connects to an address, with an optional timeout and\n                       optional source address.\ncreate_server() -- create a TCP socket and bind it to a specified address.\n\n [*] not available on all platforms!\n\nSpecial objects:\n\nSocketType -- type object for socket objects\nerror -- exception raised for I/O errors\nhas_ipv6 -- boolean value indicating if IPv6 is supported\n\nIntEnum constants:\n\nAF_INET, AF_UNIX -- socket domains (first argument to socket() call)\nSOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)\n\nInteger constants:\n\nMany other constants may be defined; these may be used in calls to\nthe setsockopt() and getsockopt() methods.\n\"\"\"\n\nimport _socket\nfrom _socket import *\n\nimport os, sys, io, selectors\nfrom enum import IntEnum, IntFlag\n\ntry:\n    import errno\nexcept ImportError:\n    errno = None\nEBADF = getattr(errno, 'EBADF', 9)\nEAGAIN = getattr(errno, 'EAGAIN', 11)\nEWOULDBLOCK = getattr(errno, 'EWOULDBLOCK', 11)\n\n__all__ = [\"fromfd\", \"getfqdn\", \"create_connection\", \"create_server\",\n           \"has_dualstack_ipv6\", \"AddressFamily\", \"SocketKind\"]\n__all__.extend(os._get_exports_list(_socket))\n\n# Set up the socket.AF_* socket.SOCK_* constants as members of IntEnums for\n# nicer string representations.\n# Note that _socket only knows about the integer values. The public interface\n# in this module understands the enums and translates them back from integers\n# where needed (e.g. .family property of a socket object).\n\nIntEnum._convert_(\n        'AddressFamily',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AF_'))\n\nIntEnum._convert_(\n        'SocketKind',\n        __name__,\n        lambda C: C.isupper() and C.startswith('SOCK_'))\n\nIntFlag._convert_(\n        'MsgFlag',\n        __name__,\n        lambda C: C.isupper() and C.startswith('MSG_'))\n\nIntFlag._convert_(\n        'AddressInfo',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AI_'))\n\n_LOCALHOST    = '127.0.0.1'\n_LOCALHOST_V6 = '::1'\n\n\ndef _intenum_converter(value, enum_klass):\n    \"\"\"Convert a numeric family value to an IntEnum member.\n\n    If it's not a known member, return the numeric value itself.\n    \"\"\"\n    try:\n        return enum_klass(value)\n    except ValueError:\n        return value\n\n\n# WSA error codes\nif sys.platform.lower().startswith(\"win\"):\n    errorTab = {}\n    errorTab[6] = \"Specified event object handle is invalid.\"\n    errorTab[8] = \"Insufficient memory available.\"\n    errorTab[87] = \"One or more parameters are invalid.\"\n    errorTab[995] = \"Overlapped operation aborted.\"\n    errorTab[996] = \"Overlapped I/O event object not in signaled state.\"\n    errorTab[997] = \"Overlapped operation will complete later.\"\n    errorTab[10004] = \"The operation was interrupted.\"\n    errorTab[10009] = \"A bad file handle was passed.\"\n    errorTab[10013] = \"Permission denied.\"\n    errorTab[10014] = \"A fault occurred on the network??\"  # WSAEFAULT\n    errorTab[10022] = \"An invalid operation was attempted.\"\n    errorTab[10024] = \"Too many open files.\"\n    errorTab[10035] = \"The socket operation would block.\"\n    errorTab[10036] = \"A blocking operation is already in progress.\"\n    errorTab[10037] = \"Operation already in progress.\"\n    errorTab[10038] = \"Socket operation on nonsocket.\"\n    errorTab[10039] = \"Destination address required.\"\n    errorTab[10040] = \"Message too long.\"\n    errorTab[10041] = \"Protocol wrong type for socket.\"\n    errorTab[10042] = \"Bad protocol option.\"\n    errorTab[10043] = \"Protocol not supported.\"\n    errorTab[10044] = \"Socket type not supported.\"\n    errorTab[10045] = \"Operation not supported.\"\n    errorTab[10046] = \"Protocol family not supported.\"\n    errorTab[10047] = \"Address family not supported by protocol family.\"\n    errorTab[10048] = \"The network address is in use.\"\n    errorTab[10049] = \"Cannot assign requested address.\"\n    errorTab[10050] = \"Network is down.\"\n    errorTab[10051] = \"Network is unreachable.\"\n    errorTab[10052] = \"Network dropped connection on reset.\"\n    errorTab[10053] = \"Software caused connection abort.\"\n    errorTab[10054] = \"The connection has been reset.\"\n    errorTab[10055] = \"No buffer space available.\"\n    errorTab[10056] = \"Socket is already connected.\"\n    errorTab[10057] = \"Socket is not connected.\"\n    errorTab[10058] = \"The network has been shut down.\"\n    errorTab[10059] = \"Too many references.\"\n    errorTab[10060] = \"The operation timed out.\"\n    errorTab[10061] = \"Connection refused.\"\n    errorTab[10062] = \"Cannot translate name.\"\n    errorTab[10063] = \"The name is too long.\"\n    errorTab[10064] = \"The host is down.\"\n    errorTab[10065] = \"The host is unreachable.\"\n    errorTab[10066] = \"Directory not empty.\"\n    errorTab[10067] = \"Too many processes.\"\n    errorTab[10068] = \"User quota exceeded.\"\n    errorTab[10069] = \"Disk quota exceeded.\"\n    errorTab[10070] = \"Stale file handle reference.\"\n    errorTab[10071] = \"Item is remote.\"\n    errorTab[10091] = \"Network subsystem is unavailable.\"\n    errorTab[10092] = \"Winsock.dll version out of range.\"\n    errorTab[10093] = \"Successful WSAStartup not yet performed.\"\n    errorTab[10101] = \"Graceful shutdown in progress.\"\n    errorTab[10102] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10103] = \"Call has been canceled.\"\n    errorTab[10104] = \"Procedure call table is invalid.\"\n    errorTab[10105] = \"Service provider is invalid.\"\n    errorTab[10106] = \"Service provider failed to initialize.\"\n    errorTab[10107] = \"System call failure.\"\n    errorTab[10108] = \"Service not found.\"\n    errorTab[10109] = \"Class type not found.\"\n    errorTab[10110] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10111] = \"Call was canceled.\"\n    errorTab[10112] = \"Database query was refused.\"\n    errorTab[11001] = \"Host not found.\"\n    errorTab[11002] = \"Nonauthoritative host not found.\"\n    errorTab[11003] = \"This is a nonrecoverable error.\"\n    errorTab[11004] = \"Valid name, no data record requested type.\"\n    errorTab[11005] = \"QoS receivers.\"\n    errorTab[11006] = \"QoS senders.\"\n    errorTab[11007] = \"No QoS senders.\"\n    errorTab[11008] = \"QoS no receivers.\"\n    errorTab[11009] = \"QoS request confirmed.\"\n    errorTab[11010] = \"QoS admission error.\"\n    errorTab[11011] = \"QoS policy failure.\"\n    errorTab[11012] = \"QoS bad style.\"\n    errorTab[11013] = \"QoS bad object.\"\n    errorTab[11014] = \"QoS traffic control error.\"\n    errorTab[11015] = \"QoS generic error.\"\n    errorTab[11016] = \"QoS service type error.\"\n    errorTab[11017] = \"QoS flowspec error.\"\n    errorTab[11018] = \"Invalid QoS provider buffer.\"\n    errorTab[11019] = \"Invalid QoS filter style.\"\n    errorTab[11020] = \"Invalid QoS filter style.\"\n    errorTab[11021] = \"Incorrect QoS filter count.\"\n    errorTab[11022] = \"Invalid QoS object length.\"\n    errorTab[11023] = \"Incorrect QoS flow count.\"\n    errorTab[11024] = \"Unrecognized QoS object.\"\n    errorTab[11025] = \"Invalid QoS policy object.\"\n    errorTab[11026] = \"Invalid QoS flow descriptor.\"\n    errorTab[11027] = \"Invalid QoS provider-specific flowspec.\"\n    errorTab[11028] = \"Invalid QoS provider-specific filterspec.\"\n    errorTab[11029] = \"Invalid QoS shape discard mode object.\"\n    errorTab[11030] = \"Invalid QoS shaping rate object.\"\n    errorTab[11031] = \"Reserved policy QoS element type.\"\n    __all__.append(\"errorTab\")\n\n\nclass _GiveupOnSendfile(Exception): pass\n\n\nclass socket(_socket.socket):\n\n    \"\"\"A subclass of _socket.socket adding the makefile() method.\"\"\"\n\n    __slots__ = [\"__weakref__\", \"_io_refs\", \"_closed\"]\n\n    def __init__(self, family=-1, type=-1, proto=-1, fileno=None):\n        # For user code address family and type values are IntEnum members, but\n        # for the underlying _socket.socket they're just integers. The\n        # constructor of _socket.socket converts the given argument to an\n        # integer automatically.\n        if fileno is None:\n            if family == -1:\n                family = AF_INET\n            if type == -1:\n                type = SOCK_STREAM\n            if proto == -1:\n                proto = 0\n        _socket.socket.__init__(self, family, type, proto, fileno)\n        self._io_refs = 0\n        self._closed = False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        if not self._closed:\n            self.close()\n\n    def __repr__(self):\n        \"\"\"Wrap __repr__() to reveal the real class name and socket\n        address(es).\n        \"\"\"\n        closed = getattr(self, '_closed', False)\n        s = \"<%s.%s%s fd=%i, family=%s, type=%s, proto=%i\" \\\n            % (self.__class__.__module__,\n               self.__class__.__qualname__,\n               \" [closed]\" if closed else \"\",\n               self.fileno(),\n               self.family,\n               self.type,\n               self.proto)\n        if not closed:\n            # getsockname and getpeername may not be available on WASI.\n            try:\n                laddr = self.getsockname()\n                if laddr:\n                    s += \", laddr=%s\" % str(laddr)\n            except (error, AttributeError):\n                pass\n            try:\n                raddr = self.getpeername()\n                if raddr:\n                    s += \", raddr=%s\" % str(raddr)\n            except (error, AttributeError):\n                pass\n        s += '>'\n        return s\n\n    def __getstate__(self):\n        raise TypeError(f\"cannot pickle {self.__class__.__name__!r} object\")\n\n    def dup(self):\n        \"\"\"dup() -> socket object\n\n        Duplicate the socket. Return a new socket object connected to the same\n        system resource. The new socket is non-inheritable.\n        \"\"\"\n        fd = dup(self.fileno())\n        sock = self.__class__(self.family, self.type, self.proto, fileno=fd)\n        sock.settimeout(self.gettimeout())\n        return sock\n\n    def accept(self):\n        \"\"\"accept() -> (socket object, address info)\n\n        Wait for an incoming connection.  Return a new socket\n        representing the connection, and the address of the client.\n        For IP sockets, the address info is a pair (hostaddr, port).\n        \"\"\"\n        fd, addr = self._accept()\n        sock = socket(self.family, self.type, self.proto, fileno=fd)\n        # Issue #7995: if no default timeout is set and the listening\n        # socket had a (non-zero) timeout, force the new socket in blocking\n        # mode to override platform-specific socket flags inheritance.\n        if getdefaulttimeout() is None and self.gettimeout():\n            sock.setblocking(True)\n        return sock, addr\n\n    def makefile(self, mode=\"r\", buffering=None, *,\n                 encoding=None, errors=None, newline=None):\n        \"\"\"makefile(...) -> an I/O stream connected to the socket\n\n        The arguments are as for io.open() after the filename, except the only\n        supported mode values are 'r' (default), 'w', 'b', or a combination of\n        those.\n        \"\"\"\n        # XXX refactor to share code?\n        if not set(mode) <= {\"r\", \"w\", \"b\"}:\n            raise ValueError(\"invalid mode %r (only r, w, b allowed)\" % (mode,))\n        writing = \"w\" in mode\n        reading = \"r\" in mode or not writing\n        assert reading or writing\n        binary = \"b\" in mode\n        rawmode = \"\"\n        if reading:\n            rawmode += \"r\"\n        if writing:\n            rawmode += \"w\"\n        raw = SocketIO(self, rawmode)\n        self._io_refs += 1\n        if buffering is None:\n            buffering = -1\n        if buffering < 0:\n            buffering = io.DEFAULT_BUFFER_SIZE\n        if buffering == 0:\n            if not binary:\n                raise ValueError(\"unbuffered streams must be binary\")\n            return raw\n        if reading and writing:\n            buffer = io.BufferedRWPair(raw, raw, buffering)\n        elif reading:\n            buffer = io.BufferedReader(raw, buffering)\n        else:\n            assert writing\n            buffer = io.BufferedWriter(raw, buffering)\n        if binary:\n            return buffer\n        encoding = io.text_encoding(encoding)\n        text = io.TextIOWrapper(buffer, encoding, errors, newline)\n        text.mode = mode\n        return text\n\n    if hasattr(os, 'sendfile'):\n\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            self._check_sendfile_params(file, offset, count)\n            sockno = self.fileno()\n            try:\n                fileno = file.fileno()\n            except (AttributeError, io.UnsupportedOperation) as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            try:\n                fsize = os.fstat(fileno).st_size\n            except OSError as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            if not fsize:\n                return 0  # empty file\n            # Truncate to 1GiB to avoid OverflowError, see bpo-38319.\n            blocksize = min(count or fsize, 2 ** 30)\n            timeout = self.gettimeout()\n            if timeout == 0:\n                raise ValueError(\"non-blocking sockets are not supported\")\n            # poll/select have the advantage of not requiring any\n            # extra file descriptor, contrarily to epoll/kqueue\n            # (also, they require a single syscall).\n            if hasattr(selectors, 'PollSelector'):\n                selector = selectors.PollSelector()\n            else:\n                selector = selectors.SelectSelector()\n            selector.register(sockno, selectors.EVENT_WRITE)\n\n            total_sent = 0\n            # localize variable access to minimize overhead\n            selector_select = selector.select\n            os_sendfile = os.sendfile\n            try:\n                while True:\n                    if timeout and not selector_select(timeout):\n                        raise TimeoutError('timed out')\n                    if count:\n                        blocksize = min(count - total_sent, blocksize)\n                        if blocksize <= 0:\n                            break\n                    try:\n                        sent = os_sendfile(sockno, fileno, offset, blocksize)\n                    except BlockingIOError:\n                        if not timeout:\n                            # Block until the socket is ready to send some\n                            # data; avoids hogging CPU resources.\n                            selector_select()\n                        continue\n                    except OSError as err:\n                        if total_sent == 0:\n                            # We can get here for different reasons, the main\n                            # one being 'file' is not a regular mmap(2)-like\n                            # file, in which case we'll fall back on using\n                            # plain send().\n                            raise _GiveupOnSendfile(err)\n                        raise err from None\n                    else:\n                        if sent == 0:\n                            break  # EOF\n                        offset += sent\n                        total_sent += sent\n                return total_sent\n            finally:\n                if total_sent > 0 and hasattr(file, 'seek'):\n                    file.seek(offset)\n    else:\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            raise _GiveupOnSendfile(\n                \"os.sendfile() not available on this platform\")\n\n    def _sendfile_use_send(self, file, offset=0, count=None):\n        self._check_sendfile_params(file, offset, count)\n        if self.gettimeout() == 0:\n            raise ValueError(\"non-blocking sockets are not supported\")\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 8192) if count else 8192\n        total_sent = 0\n        # localize variable access to minimize overhead\n        file_read = file.read\n        sock_send = self.send\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                data = memoryview(file_read(blocksize))\n                if not data:\n                    break  # EOF\n                while True:\n                    try:\n                        sent = sock_send(data)\n                    except BlockingIOError:\n                        continue\n                    else:\n                        total_sent += sent\n                        if sent < len(data):\n                            data = data[sent:]\n                        else:\n                            break\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not self.type & SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n\n    def sendfile(self, file, offset=0, count=None):\n        \"\"\"sendfile(file[, offset[, count]]) -> sent\n\n        Send a file until EOF is reached by using high-performance\n        os.sendfile() and return the total number of bytes which\n        were sent.\n        *file* must be a regular file object opened in binary mode.\n        If os.sendfile() is not available (e.g. Windows) or file is\n        not a regular file socket.send() will be used instead.\n        *offset* tells from where to start reading the file.\n        If specified, *count* is the total number of bytes to transmit\n        as opposed to sending the file until EOF is reached.\n        File position is updated on return or also in case of error in\n        which case file.tell() can be used to figure out the number of\n        bytes which were sent.\n        The socket must be of SOCK_STREAM type.\n        Non-blocking sockets are not supported.\n        \"\"\"\n        try:\n            return self._sendfile_use_sendfile(file, offset, count)\n        except _GiveupOnSendfile:\n            return self._sendfile_use_send(file, offset, count)\n\n    def _decref_socketios(self):\n        if self._io_refs > 0:\n            self._io_refs -= 1\n        if self._closed:\n            self.close()\n\n    def _real_close(self, _ss=_socket.socket):\n        # This function should not reference any globals. See issue #808164.\n        _ss.close(self)\n\n    def close(self):\n        # This function should not reference any globals. See issue #808164.\n        self._closed = True\n        if self._io_refs <= 0:\n            self._real_close()\n\n    def detach(self):\n        \"\"\"detach() -> file descriptor\n\n        Close the socket object without closing the underlying file descriptor.\n        The object cannot be used after this call, but the file descriptor\n        can be reused for other purposes.  The file descriptor is returned.\n        \"\"\"\n        self._closed = True\n        return super().detach()\n\n    @property\n    def family(self):\n        \"\"\"Read-only access to the address family for this socket.\n        \"\"\"\n        return _intenum_converter(super().family, AddressFamily)\n\n    @property\n    def type(self):\n        \"\"\"Read-only access to the socket type.\n        \"\"\"\n        return _intenum_converter(super().type, SocketKind)\n\n    if os.name == 'nt':\n        def get_inheritable(self):\n            return os.get_handle_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_handle_inheritable(self.fileno(), inheritable)\n    else:\n        def get_inheritable(self):\n            return os.get_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_inheritable(self.fileno(), inheritable)\n    get_inheritable.__doc__ = \"Get the inheritable flag of the socket\"\n    set_inheritable.__doc__ = \"Set the inheritable flag of the socket\"\n\ndef fromfd(fd, family, type, proto=0):\n    \"\"\" fromfd(fd, family, type[, proto]) -> socket object\n\n    Create a socket object from a duplicate of the given file\n    descriptor.  The remaining arguments are the same as for socket().\n    \"\"\"\n    nfd = dup(fd)\n    return socket(family, type, proto, nfd)\n\nif hasattr(_socket.socket, \"sendmsg\"):\n    import array\n\n    def send_fds(sock, buffers, fds, flags=0, address=None):\n        \"\"\" send_fds(sock, buffers, fds[, flags[, address]]) -> integer\n\n        Send the list of file descriptors fds over an AF_UNIX socket.\n        \"\"\"\n        return sock.sendmsg(buffers, [(_socket.SOL_SOCKET,\n            _socket.SCM_RIGHTS, array.array(\"i\", fds))])\n    __all__.append(\"send_fds\")\n\nif hasattr(_socket.socket, \"recvmsg\"):\n    import array\n\n    def recv_fds(sock, bufsize, maxfds, flags=0):\n        \"\"\" recv_fds(sock, bufsize, maxfds[, flags]) -> (data, list of file\n        descriptors, msg_flags, address)\n\n        Receive up to maxfds file descriptors returning the message\n        data and a list containing the descriptors.\n        \"\"\"\n        # Array of ints\n        fds = array.array(\"i\")\n        msg, ancdata, flags, addr = sock.recvmsg(bufsize,\n            _socket.CMSG_LEN(maxfds * fds.itemsize))\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if (cmsg_level == _socket.SOL_SOCKET and cmsg_type == _socket.SCM_RIGHTS):\n                fds.frombytes(cmsg_data[:\n                        len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return msg, list(fds), flags, addr\n    __all__.append(\"recv_fds\")\n\nif hasattr(_socket.socket, \"share\"):\n    def fromshare(info):\n        \"\"\" fromshare(info) -> socket object\n\n        Create a socket object from the bytes object returned by\n        socket.share(pid).\n        \"\"\"\n        return socket(0, 0, 0, info)\n    __all__.append(\"fromshare\")\n\n# Origin: https://gist.github.com/4325783, by Geert Jansen.  Public domain.\n# This is used if _socket doesn't natively provide socketpair. It's\n# always defined so that it can be patched in for testing purposes.\ndef _fallback_socketpair(family=AF_INET, type=SOCK_STREAM, proto=0):\n    if family == AF_INET:\n        host = _LOCALHOST\n    elif family == AF_INET6:\n        host = _LOCALHOST_V6\n    else:\n        raise ValueError(\"Only AF_INET and AF_INET6 socket address families \"\n                         \"are supported\")\n    if type != SOCK_STREAM:\n        raise ValueError(\"Only SOCK_STREAM socket type is supported\")\n    if proto != 0:\n        raise ValueError(\"Only protocol zero is supported\")\n\n    # We create a connected TCP socket. Note the trick with\n    # setblocking(False) that prevents us from having to create a thread.\n    lsock = socket(family, type, proto)\n    try:\n        lsock.bind((host, 0))\n        lsock.listen()\n        # On IPv6, ignore flow_info and scope_id\n        addr, port = lsock.getsockname()[:2]\n        csock = socket(family, type, proto)\n        try:\n            csock.setblocking(False)\n            try:\n                csock.connect((addr, port))\n            except (BlockingIOError, InterruptedError):\n                pass\n            csock.setblocking(True)\n            ssock, _ = lsock.accept()\n        except:\n            csock.close()\n            raise\n    finally:\n        lsock.close()\n\n    # Authenticating avoids using a connection from something else\n    # able to connect to {host}:{port} instead of us.\n    # We expect only AF_INET and AF_INET6 families.\n    try:\n        if (\n            ssock.getsockname() != csock.getpeername()\n            or csock.getsockname() != ssock.getpeername()\n        ):\n            raise ConnectionError(\"Unexpected peer connection\")\n    except:\n        # getsockname() and getpeername() can fail\n        # if either socket isn't connected.\n        ssock.close()\n        csock.close()\n        raise\n\n    return (ssock, csock)\n\nif hasattr(_socket, \"socketpair\"):\n    def socketpair(family=None, type=SOCK_STREAM, proto=0):\n        if family is None:\n            try:\n                family = AF_UNIX\n            except NameError:\n                family = AF_INET\n        a, b = _socket.socketpair(family, type, proto)\n        a = socket(family, type, proto, a.detach())\n        b = socket(family, type, proto, b.detach())\n        return a, b\n\nelse:\n    socketpair = _fallback_socketpair\n    __all__.append(\"socketpair\")\n\nsocketpair.__doc__ = \"\"\"socketpair([family[, type[, proto]]]) -> (socket object, socket object)\nCreate a pair of socket objects from the sockets returned by the platform\nsocketpair() function.\nThe arguments are the same as for socket() except the default family is AF_UNIX\nif defined on the platform; otherwise, the default is AF_INET.\n\"\"\"\n\n_blocking_errnos = { EAGAIN, EWOULDBLOCK }\n\nclass SocketIO(io.RawIOBase):\n\n    \"\"\"Raw I/O implementation for stream sockets.\n\n    This class supports the makefile() method on sockets.  It provides\n    the raw I/O interface on top of a socket object.\n    \"\"\"\n\n    # One might wonder why not let FileIO do the job instead.  There are two\n    # main reasons why FileIO is not adapted:\n    # - it wouldn't work under Windows (where you can't used read() and\n    #   write() on a socket handle)\n    # - it wouldn't work with socket timeouts (FileIO would ignore the\n    #   timeout and consider the socket non-blocking)\n\n    # XXX More docs\n\n    def __init__(self, sock, mode):\n        if mode not in (\"r\", \"w\", \"rw\", \"rb\", \"wb\", \"rwb\"):\n            raise ValueError(\"invalid mode: %r\" % mode)\n        io.RawIOBase.__init__(self)\n        self._sock = sock\n        if \"b\" not in mode:\n            mode += \"b\"\n        self._mode = mode\n        self._reading = \"r\" in mode\n        self._writing = \"w\" in mode\n        self._timeout_occurred = False\n\n    def readinto(self, b):\n        \"\"\"Read up to len(b) bytes into the writable buffer *b* and return\n        the number of bytes read.  If the socket is non-blocking and no bytes\n        are available, None is returned.\n\n        If *b* is non-empty, a 0 return value indicates that the connection\n        was shutdown at the other end.\n        \"\"\"\n        self._checkClosed()\n        self._checkReadable()\n        if self._timeout_occurred:\n            raise OSError(\"cannot read from timed out object\")\n        try:\n            return self._sock.recv_into(b)\n        except timeout:\n            self._timeout_occurred = True\n            raise\n        except error as e:\n            if e.errno in _blocking_errnos:\n                return None\n            raise\n\n    def write(self, b):\n        \"\"\"Write the given bytes or bytearray object *b* to the socket\n        and return the number of bytes written.  This can be less than\n        len(b) if not all data could be written.  If the socket is\n        non-blocking and no bytes could be written None is returned.\n        \"\"\"\n        self._checkClosed()\n        self._checkWritable()\n        try:\n            return self._sock.send(b)\n        except error as e:\n            # XXX what about EINTR?\n            if e.errno in _blocking_errnos:\n                return None\n            raise\n\n    def readable(self):\n        \"\"\"True if the SocketIO is open for reading.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._reading\n\n    def writable(self):\n        \"\"\"True if the SocketIO is open for writing.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._writing\n\n    def seekable(self):\n        \"\"\"True if the SocketIO is open for seeking.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return super().seekable()\n\n    def fileno(self):\n        \"\"\"Return the file descriptor of the underlying socket.\n        \"\"\"\n        self._checkClosed()\n        return self._sock.fileno()\n\n    @property\n    def name(self):\n        if not self.closed:\n            return self.fileno()\n        else:\n            return -1\n\n    @property\n    def mode(self):\n        return self._mode\n\n    def close(self):\n        \"\"\"Close the SocketIO object.  This doesn't close the underlying\n        socket, except if all references to it have disappeared.\n        \"\"\"\n        if self.closed:\n            return\n        io.RawIOBase.close(self)\n        self._sock._decref_socketios()\n        self._sock = None\n\n\ndef getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available and `name`\n    was given, it is returned unchanged. If `name` was empty, '0.0.0.0' or '::',\n    hostname from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name in ('0.0.0.0', '::'):\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name\n\n\n_GLOBAL_DEFAULT_TIMEOUT = object()\n\ndef create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None, *, all_errors=False):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    A host of '' or port 0 tells the OS to use the default. When a connection\n    cannot be created, raises the last error if *all_errors* is False,\n    and an ExceptionGroup of all errors if *all_errors* is True.\n    \"\"\"\n\n    host, port = address\n    exceptions = []\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            # Break explicitly a reference cycle\n            exceptions.clear()\n            return sock\n\n        except error as exc:\n            if not all_errors:\n                exceptions.clear()  # raise only the last error\n            exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n\n    if len(exceptions):\n        try:\n            if not all_errors:\n                raise exceptions[0]\n            raise ExceptionGroup(\"create_connection failed\", exceptions)\n        finally:\n            # Break explicitly a reference cycle\n            exceptions.clear()\n    else:\n        raise error(\"getaddrinfo returns an empty list\")\n\n\ndef has_dualstack_ipv6():\n    \"\"\"Return True if the platform supports creating a SOCK_STREAM socket\n    which can handle both AF_INET and AF_INET6 (IPv4 / IPv6) connections.\n    \"\"\"\n    if not has_ipv6 \\\n            or not hasattr(_socket, 'IPPROTO_IPV6') \\\n            or not hasattr(_socket, 'IPV6_V6ONLY'):\n        return False\n    try:\n        with socket(AF_INET6, SOCK_STREAM) as sock:\n            sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            return True\n    except error:\n        return False\n\n\ndef create_server(address, *, family=AF_INET, backlog=None, reuse_port=False,\n                  dualstack_ipv6=False):\n    \"\"\"Convenience function which creates a SOCK_STREAM type socket\n    bound to *address* (a 2-tuple (host, port)) and return the socket\n    object.\n\n    *family* should be either AF_INET or AF_INET6.\n    *backlog* is the queue size passed to socket.listen().\n    *reuse_port* dictates whether to use the SO_REUSEPORT socket option.\n    *dualstack_ipv6*: if true and the platform supports it, it will\n    create an AF_INET6 socket able to accept both IPv4 or IPv6\n    connections. When false it will explicitly disable this option on\n    platforms that enable it by default (e.g. Linux).\n\n    >>> with create_server(('', 8000)) as server:\n    ...     while True:\n    ...         conn, addr = server.accept()\n    ...         # handle new connection\n    \"\"\"\n    if reuse_port and not hasattr(_socket, \"SO_REUSEPORT\"):\n        raise ValueError(\"SO_REUSEPORT not supported on this platform\")\n    if dualstack_ipv6:\n        if not has_dualstack_ipv6():\n            raise ValueError(\"dualstack_ipv6 not supported on this platform\")\n        if family != AF_INET6:\n            raise ValueError(\"dualstack_ipv6 requires AF_INET6 family\")\n    sock = socket(family, SOCK_STREAM)\n    try:\n        # Note about Windows. We don't set SO_REUSEADDR because:\n        # 1) It's unnecessary: bind() will succeed even in case of a\n        # previous closed socket on the same address and still in\n        # TIME_WAIT state.\n        # 2) If set, another socket is free to bind() on the same\n        # address, effectively preventing this one from accepting\n        # connections. Also, it may set the process in a state where\n        # it'll no longer respond to any signals or graceful kills.\n        # See: https://learn.microsoft.com/windows/win32/winsock/using-so-reuseaddr-and-so-exclusiveaddruse\n        if os.name not in ('nt', 'cygwin') and \\\n                hasattr(_socket, 'SO_REUSEADDR'):\n            try:\n                sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n            except error:\n                # Fail later on bind(), for platforms which may not\n                # support this option.\n                pass\n        if reuse_port:\n            sock.setsockopt(SOL_SOCKET, SO_REUSEPORT, 1)\n        if has_ipv6 and family == AF_INET6:\n            if dualstack_ipv6:\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            elif hasattr(_socket, \"IPV6_V6ONLY\") and \\\n                    hasattr(_socket, \"IPPROTO_IPV6\"):\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 1)\n        try:\n            sock.bind(address)\n        except error as err:\n            msg = '%s (while attempting to bind on address %r)' % \\\n                (err.strerror, address)\n            raise error(err.errno, msg) from None\n        if backlog is None:\n            sock.listen()\n        else:\n            sock.listen(backlog)\n        return sock\n    except error:\n        sock.close()\n        raise\n\n\ndef getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):\n    \"\"\"Resolve host and port into list of address info entries.\n\n    Translate the host/port argument into a sequence of 5-tuples that contain\n    all the necessary arguments for creating a socket connected to that service.\n    host is a domain name, a string representation of an IPv4/v6 address or\n    None. port is a string service name such as 'http', a numeric port number or\n    None. By passing None as the value of host and port, you can pass NULL to\n    the underlying C API.\n\n    The family, type and proto arguments can be optionally specified in order to\n    narrow the list of addresses returned. Passing zero as a value for each of\n    these arguments selects the full range of results.\n    \"\"\"\n    # We override this function since we want to translate the numeric family\n    # and socket type values to enum constants.\n    addrlist = []\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n        af, socktype, proto, canonname, sa = res\n        addrlist.append((_intenum_converter(af, AddressFamily),\n                         _intenum_converter(socktype, SocketKind),\n                         proto, canonname, sa))\n    return addrlist\n", 980], "/usr/lib/python3.13/asyncio/selector_events.py": ["\"\"\"Event loop using a selector and related classes.\n\nA selector is a \"notify-when-ready\" multiplexer.  For a subclass which\nalso includes support for signal handling, see the unix_events sub-module.\n\"\"\"\n\n__all__ = 'BaseSelectorEventLoop',\n\nimport collections\nimport errno\nimport functools\nimport itertools\nimport os\nimport selectors\nimport socket\nimport warnings\nimport weakref\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import base_events\nfrom . import constants\nfrom . import events\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n_HAS_SENDMSG = hasattr(socket.socket, 'sendmsg')\n\nif _HAS_SENDMSG:\n    try:\n        SC_IOV_MAX = os.sysconf('SC_IOV_MAX')\n    except OSError:\n        # Fallback to send\n        _HAS_SENDMSG = False\n\ndef _test_selector_event(selector, fd, event):\n    # Test if the selector is monitoring 'event' events\n    # for the file descriptor 'fd'.\n    try:\n        key = selector.get_key(fd)\n    except KeyError:\n        return False\n    else:\n        return bool(key.events & event)\n\n\nclass BaseSelectorEventLoop(base_events.BaseEventLoop):\n    \"\"\"Selector event loop.\n\n    See events.EventLoop for API specification.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__()\n\n        if selector is None:\n            selector = selectors.DefaultSelector()\n        logger.debug('Using selector: %s', selector.__class__.__name__)\n        self._selector = selector\n        self._make_self_pipe()\n        self._transports = weakref.WeakValueDictionary()\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        self._ensure_fd_no_transport(sock)\n        return _SelectorSocketTransport(self, sock, protocol, waiter,\n                                        extra, server)\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT,\n    ):\n        self._ensure_fd_no_transport(rawsock)\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout\n        )\n        _SelectorSocketTransport(self, rawsock, ssl_protocol,\n                                 extra=extra, server=server)\n        return ssl_protocol._app_transport\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        self._ensure_fd_no_transport(sock)\n        return _SelectorDatagramTransport(self, sock, protocol,\n                                          address, waiter, extra)\n\n    def close(self):\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self.is_closed():\n            return\n        self._close_self_pipe()\n        super().close()\n        if self._selector is not None:\n            self._selector.close()\n            self._selector = None\n\n    def _close_self_pipe(self):\n        self._remove_reader(self._ssock.fileno())\n        self._ssock.close()\n        self._ssock = None\n        self._csock.close()\n        self._csock = None\n        self._internal_fds -= 1\n\n    def _make_self_pipe(self):\n        # A self-socket, really. :-)\n        self._ssock, self._csock = socket.socketpair()\n        self._ssock.setblocking(False)\n        self._csock.setblocking(False)\n        self._internal_fds += 1\n        self._add_reader(self._ssock.fileno(), self._read_from_self)\n\n    def _process_self_data(self, data):\n        pass\n\n    def _read_from_self(self):\n        while True:\n            try:\n                data = self._ssock.recv(4096)\n                if not data:\n                    break\n                self._process_self_data(data)\n            except InterruptedError:\n                continue\n            except BlockingIOError:\n                break\n\n    def _write_to_self(self):\n        # This may be called from a different thread, possibly after\n        # _close_self_pipe() has been called or even while it is\n        # running.  Guard for self._csock being None or closed.  When\n        # a socket is closed, send() raises OSError (with errno set to\n        # EBADF, but let's not rely on the exact error code).\n        csock = self._csock\n        if csock is None:\n            return\n\n        try:\n            csock.send(b'\\0')\n        except OSError:\n            if self._debug:\n                logger.debug(\"Fail to write a null byte into the \"\n                             \"self-pipe socket\",\n                             exc_info=True)\n\n    def _start_serving(self, protocol_factory, sock,\n                       sslcontext=None, server=None, backlog=100,\n                       ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n                       ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        self._add_reader(sock.fileno(), self._accept_connection,\n                         protocol_factory, sock, sslcontext, server, backlog,\n                         ssl_handshake_timeout, ssl_shutdown_timeout)\n\n    def _accept_connection(\n            self, protocol_factory, sock,\n            sslcontext=None, server=None, backlog=100,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        # This method is only called once for each event loop tick where the\n        # listening socket has triggered an EVENT_READ. There may be multiple\n        # connections waiting for an .accept() so it is called in a loop.\n        # See https://bugs.python.org/issue27906 for more details.\n        for _ in range(backlog):\n            try:\n                conn, addr = sock.accept()\n                if self._debug:\n                    logger.debug(\"%r got a new connection from %r: %r\",\n                                 server, addr, conn)\n                conn.setblocking(False)\n            except (BlockingIOError, InterruptedError, ConnectionAbortedError):\n                # Early exit because the socket accept buffer is empty.\n                return None\n            except OSError as exc:\n                # There's nowhere to send the error, so just log it.\n                if exc.errno in (errno.EMFILE, errno.ENFILE,\n                                 errno.ENOBUFS, errno.ENOMEM):\n                    # Some platforms (e.g. Linux keep reporting the FD as\n                    # ready, so we remove the read handler temporarily.\n                    # We'll try again in a while.\n                    self.call_exception_handler({\n                        'message': 'socket.accept() out of system resource',\n                        'exception': exc,\n                        'socket': trsock.TransportSocket(sock),\n                    })\n                    self._remove_reader(sock.fileno())\n                    self.call_later(constants.ACCEPT_RETRY_DELAY,\n                                    self._start_serving,\n                                    protocol_factory, sock, sslcontext, server,\n                                    backlog, ssl_handshake_timeout,\n                                    ssl_shutdown_timeout)\n                else:\n                    raise  # The event loop will catch, log and ignore it.\n            else:\n                extra = {'peername': addr}\n                accept = self._accept_connection2(\n                    protocol_factory, conn, extra, sslcontext, server,\n                    ssl_handshake_timeout, ssl_shutdown_timeout)\n                self.create_task(accept)\n\n    async def _accept_connection2(\n            self, protocol_factory, conn, extra,\n            sslcontext=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        protocol = None\n        transport = None\n        try:\n            protocol = protocol_factory()\n            waiter = self.create_future()\n            if sslcontext:\n                transport = self._make_ssl_transport(\n                    conn, protocol, sslcontext, waiter=waiter,\n                    server_side=True, extra=extra, server=server,\n                    ssl_handshake_timeout=ssl_handshake_timeout,\n                    ssl_shutdown_timeout=ssl_shutdown_timeout)\n            else:\n                transport = self._make_socket_transport(\n                    conn, protocol, waiter=waiter, extra=extra,\n                    server=server)\n\n            try:\n                await waiter\n            except BaseException:\n                transport.close()\n                # gh-109534: When an exception is raised by the SSLProtocol object the\n                # exception set in this future can keep the protocol object alive and\n                # cause a reference cycle.\n                waiter = None\n                raise\n                # It's now up to the protocol to handle the connection.\n\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if self._debug:\n                context = {\n                    'message':\n                        'Error on transport creation for incoming connection',\n                    'exception': exc,\n                }\n                if protocol is not None:\n                    context['protocol'] = protocol\n                if transport is not None:\n                    context['transport'] = transport\n                self.call_exception_handler(context)\n\n    def _ensure_fd_no_transport(self, fd):\n        fileno = fd\n        if not isinstance(fileno, int):\n            try:\n                fileno = int(fileno.fileno())\n            except (AttributeError, TypeError, ValueError):\n                # This code matches selectors._fileobj_to_fd function.\n                raise ValueError(f\"Invalid file object: {fd!r}\") from None\n        transport = self._transports.get(fileno)\n        if transport and not transport.is_closing():\n            raise RuntimeError(\n                f'File descriptor {fd!r} is used by transport '\n                f'{transport!r}')\n\n    def _add_reader(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            self._selector.register(fd, selectors.EVENT_READ,\n                                    (handle, None))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_READ,\n                                  (handle, writer))\n            if reader is not None:\n                reader.cancel()\n        return handle\n\n    def _remove_reader(self, fd):\n        if self.is_closed():\n            return False\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            return False\n        mask, (reader, writer) = key.events, key.data\n        mask &= ~selectors.EVENT_READ\n        if not mask:\n            self._selector.unregister(fd)\n        else:\n            self._selector.modify(fd, mask, (None, writer))\n\n        if reader is not None:\n            reader.cancel()\n            return True\n        else:\n            return False\n\n    def _add_writer(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            self._selector.register(fd, selectors.EVENT_WRITE,\n                                    (None, handle))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_WRITE,\n                                  (reader, handle))\n            if writer is not None:\n                writer.cancel()\n        return handle\n\n    def _remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        if self.is_closed():\n            return False\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            return False\n        mask, (reader, writer) = key.events, key.data\n        # Remove both writer and connector.\n        mask &= ~selectors.EVENT_WRITE\n        if not mask:\n            self._selector.unregister(fd)\n        else:\n            self._selector.modify(fd, mask, (reader, None))\n\n        if writer is not None:\n            writer.cancel()\n            return True\n        else:\n            return False\n\n    def add_reader(self, fd, callback, *args):\n        \"\"\"Add a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_reader(fd, callback, *args)\n\n    def remove_reader(self, fd):\n        \"\"\"Remove a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_reader(fd)\n\n    def add_writer(self, fd, callback, *args):\n        \"\"\"Add a writer callback..\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_writer(fd, callback, *args)\n\n    def remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_writer(fd)\n\n    async def sock_recv(self, sock, n):\n        \"\"\"Receive data from the socket.\n\n        The return value is a bytes object representing the data received.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv, fut, sock, n)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_read_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_reader(fd)\n\n    def _sock_recv(self, fut, sock, n):\n        # _sock_recv() can add itself as an I/O callback if the operation can't\n        # be done immediately. Don't use it directly, call sock_recv().\n        if fut.done():\n            return\n        try:\n            data = sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(data)\n\n    async def sock_recv_into(self, sock, buf):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is the number of bytes written.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv_into, fut, sock, buf)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recv_into(self, fut, sock, buf):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            nbytes = sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(nbytes)\n\n    async def sock_recvfrom(self, sock, bufsize):\n        \"\"\"Receive a datagram from a datagram socket.\n\n        The return value is a tuple of (bytes, address) representing the\n        datagram received and the address it came from.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recvfrom(bufsize)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recvfrom, fut, sock, bufsize)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recvfrom(self, fut, sock, bufsize):\n        # _sock_recvfrom() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recvfrom().\n        if fut.done():\n            return\n        try:\n            result = sock.recvfrom(bufsize)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(result)\n\n    async def sock_recvfrom_into(self, sock, buf, nbytes=0):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is a tuple of (number of bytes written, address).\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        if not nbytes:\n            nbytes = len(buf)\n\n        try:\n            return sock.recvfrom_into(buf, nbytes)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recvfrom_into, fut, sock, buf,\n                                  nbytes)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recvfrom_into(self, fut, sock, buf, bufsize):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            result = sock.recvfrom_into(buf, bufsize)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(result)\n\n    async def sock_sendall(self, sock, data):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            n = sock.send(data)\n        except (BlockingIOError, InterruptedError):\n            n = 0\n\n        if n == len(data):\n            # all data sent\n            return\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        # use a trick with a list in closure to store a mutable state\n        handle = self._add_writer(fd, self._sock_sendall, fut, sock,\n                                  memoryview(data), [n])\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd, handle=handle))\n        return await fut\n\n    def _sock_sendall(self, fut, sock, view, pos):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        start = pos[0]\n        try:\n            n = sock.send(view[start:])\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n            return\n\n        start += n\n\n        if start == len(view):\n            fut.set_result(None)\n        else:\n            pos[0] = start\n\n    async def sock_sendto(self, sock, data, address):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.sendto(data, address)\n        except (BlockingIOError, InterruptedError):\n            pass\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        # use a trick with a list in closure to store a mutable state\n        handle = self._add_writer(fd, self._sock_sendto, fut, sock, data,\n                                  address)\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd, handle=handle))\n        return await fut\n\n    def _sock_sendto(self, fut, sock, data, address):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        try:\n            n = sock.sendto(data, 0, address)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(n)\n\n    async def sock_connect(self, sock, address):\n        \"\"\"Connect to a remote socket at address.\n\n        This method is a coroutine.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n\n        if sock.family == socket.AF_INET or (\n                base_events._HAS_IPv6 and sock.family == socket.AF_INET6):\n            resolved = await self._ensure_resolved(\n                address, family=sock.family, type=sock.type, proto=sock.proto,\n                loop=self,\n            )\n            _, _, _, _, address = resolved[0]\n\n        fut = self.create_future()\n        self._sock_connect(fut, sock, address)\n        try:\n            return await fut\n        finally:\n            # Needed to break cycles when an exception occurs.\n            fut = None\n\n    def _sock_connect(self, fut, sock, address):\n        fd = sock.fileno()\n        try:\n            sock.connect(address)\n        except (BlockingIOError, InterruptedError):\n            # Issue #23618: When the C function connect() fails with EINTR, the\n            # connection runs in background. We have to wait until the socket\n            # becomes writable to be notified when the connection succeed or\n            # fails.\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_writer(\n                fd, self._sock_connect_cb, fut, sock, address)\n            fut.add_done_callback(\n                functools.partial(self._sock_write_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n        finally:\n            fut = None\n\n    def _sock_write_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_writer(fd)\n\n    def _sock_connect_cb(self, fut, sock, address):\n        if fut.done():\n            return\n\n        try:\n            err = sock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n            if err != 0:\n                # Jump to any except clause below.\n                raise OSError(err, f'Connect call failed {address}')\n        except (BlockingIOError, InterruptedError):\n            # socket is still registered, the callback will be retried later\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n        finally:\n            fut = None\n\n    async def sock_accept(self, sock):\n        \"\"\"Accept a connection.\n\n        The socket must be bound to an address and listening for connections.\n        The return value is a pair (conn, address) where conn is a new socket\n        object usable to send and receive data on the connection, and address\n        is the address bound to the socket on the other end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_accept(fut, sock)\n        return await fut\n\n    def _sock_accept(self, fut, sock):\n        fd = sock.fileno()\n        try:\n            conn, address = sock.accept()\n            conn.setblocking(False)\n        except (BlockingIOError, InterruptedError):\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_reader(fd, self._sock_accept, fut, sock)\n            fut.add_done_callback(\n                functools.partial(self._sock_read_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result((conn, address))\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        del self._transports[transp._sock_fd]\n        resume_reading = transp.is_reading()\n        transp.pause_reading()\n        await transp._make_empty_waiter()\n        try:\n            return await self.sock_sendfile(transp._sock, file, offset, count,\n                                            fallback=False)\n        finally:\n            transp._reset_empty_waiter()\n            if resume_reading:\n                transp.resume_reading()\n            self._transports[transp._sock_fd] = transp\n\n    def _process_events(self, event_list):\n        for key, mask in event_list:\n            fileobj, (reader, writer) = key.fileobj, key.data\n            if mask & selectors.EVENT_READ and reader is not None:\n                if reader._cancelled:\n                    self._remove_reader(fileobj)\n                else:\n                    self._add_callback(reader)\n            if mask & selectors.EVENT_WRITE and writer is not None:\n                if writer._cancelled:\n                    self._remove_writer(fileobj)\n                else:\n                    self._add_callback(writer)\n\n    def _stop_serving(self, sock):\n        self._remove_reader(sock.fileno())\n        sock.close()\n\n\nclass _SelectorTransport(transports._FlowControlMixin,\n                         transports.Transport):\n\n    max_size = 256 * 1024  # Buffer size passed to recv().\n\n    # Attribute used in the destructor: it must be set even if the constructor\n    # is not called (see _SelectorSslTransport which may start by raising an\n    # exception)\n    _sock = None\n\n    def __init__(self, loop, sock, protocol, extra=None, server=None):\n        super().__init__(extra, loop)\n        self._extra['socket'] = trsock.TransportSocket(sock)\n        try:\n            self._extra['sockname'] = sock.getsockname()\n        except OSError:\n            self._extra['sockname'] = None\n        if 'peername' not in self._extra:\n            try:\n                self._extra['peername'] = sock.getpeername()\n            except socket.error:\n                self._extra['peername'] = None\n        self._sock = sock\n        self._sock_fd = sock.fileno()\n\n        self._protocol_connected = False\n        self.set_protocol(protocol)\n\n        self._server = server\n        self._buffer = collections.deque()\n        self._conn_lost = 0  # Set when call to connection_lost scheduled.\n        self._closing = False  # Set when close() called.\n        self._paused = False  # Set when pause_reading() called\n\n        if self._server is not None:\n            self._server._attach(self)\n        loop._transports[self._sock_fd] = self\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._sock is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._sock_fd}')\n        # test if the transport was closed\n        if self._loop is not None and not self._loop.is_closed():\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd, selectors.EVENT_READ)\n            if polling:\n                info.append('read=polling')\n            else:\n                info.append('read=idle')\n\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd,\n                                           selectors.EVENT_WRITE)\n            if polling:\n                state = 'polling'\n            else:\n                state = 'idle'\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'write=<{state}, bufsize={bufsize}>')\n        return '<{}>'.format(' '.join(info))\n\n    def abort(self):\n        self._force_close(None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n        self._protocol_connected = True\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def is_reading(self):\n        return not self.is_closing() and not self._paused\n\n    def pause_reading(self):\n        if not self.is_reading():\n            return\n        self._paused = True\n        self._loop._remove_reader(self._sock_fd)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._add_reader(self._sock_fd, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def close(self):\n        if self._closing:\n            return\n        self._closing = True\n        self._loop._remove_reader(self._sock_fd)\n        if not self._buffer:\n            self._conn_lost += 1\n            self._loop._remove_writer(self._sock_fd)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._sock is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._sock.close()\n            if self._server is not None:\n                self._server._detach(self)\n\n    def _fatal_error(self, exc, message='Fatal error on transport'):\n        # Should be called from exception handler only.\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._force_close(exc)\n\n    def _force_close(self, exc):\n        if self._conn_lost:\n            return\n        if self._buffer:\n            self._buffer.clear()\n            self._loop._remove_writer(self._sock_fd)\n        if not self._closing:\n            self._closing = True\n            self._loop._remove_reader(self._sock_fd)\n        self._conn_lost += 1\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            if self._protocol_connected:\n                self._protocol.connection_lost(exc)\n        finally:\n            self._sock.close()\n            self._sock = None\n            self._protocol = None\n            self._loop = None\n            server = self._server\n            if server is not None:\n                server._detach(self)\n                self._server = None\n\n    def get_write_buffer_size(self):\n        return sum(map(len, self._buffer))\n\n    def _add_reader(self, fd, callback, *args):\n        if not self.is_reading():\n            return\n        self._loop._add_reader(fd, callback, *args)\n\n\nclass _SelectorSocketTransport(_SelectorTransport):\n\n    _start_tls_compatible = True\n    _sendfile_compatible = constants._SendfileMode.TRY_NATIVE\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n\n        self._read_ready_cb = None\n        super().__init__(loop, sock, protocol, extra, server)\n        self._eof = False\n        self._empty_waiter = None\n        if _HAS_SENDMSG:\n            self._write_ready = self._write_sendmsg\n        else:\n            self._write_ready = self._write_send\n        # Disable the Nagle algorithm -- small writes will be\n        # sent without waiting for the TCP ACK.  This generally\n        # decreases the latency (in some cases significantly.)\n        base_events._set_nodelay(self._sock)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def set_protocol(self, protocol):\n        if isinstance(protocol, protocols.BufferedProtocol):\n            self._read_ready_cb = self._read_ready__get_buffer\n        else:\n            self._read_ready_cb = self._read_ready__data_received\n\n        super().set_protocol(protocol)\n\n    def _read_ready(self):\n        self._read_ready_cb()\n\n    def _read_ready__get_buffer(self):\n        if self._conn_lost:\n            return\n\n        try:\n            buf = self._protocol.get_buffer(-1)\n            if not len(buf):\n                raise RuntimeError('get_buffer() returned an empty buffer')\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.get_buffer() call failed.')\n            return\n\n        try:\n            nbytes = self._sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not nbytes:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.buffer_updated(nbytes)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.buffer_updated() call failed.')\n\n    def _read_ready__data_received(self):\n        if self._conn_lost:\n            return\n        try:\n            data = self._sock.recv(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not data:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.data_received(data)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.data_received() call failed.')\n\n    def _read_ready__on_eof(self):\n        if self._loop.get_debug():\n            logger.debug(\"%r received EOF\", self)\n\n        try:\n            keep_open = self._protocol.eof_received()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.eof_received() call failed.')\n            return\n\n        if keep_open:\n            # We're keeping the connection open so the\n            # protocol can write more, but we still can't\n            # receive more, so remove the reader callback.\n            self._loop._remove_reader(self._sock_fd)\n        else:\n            self.close()\n\n    def write(self, data):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if self._eof:\n            raise RuntimeError('Cannot call write() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to write; sendfile is in progress')\n        if not data:\n            return\n\n        if self._conn_lost:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Optimization: try to send now.\n            try:\n                n = self._sock.send(data)\n            except (BlockingIOError, InterruptedError):\n                pass\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(exc, 'Fatal write error on socket transport')\n                return\n            else:\n                data = memoryview(data)[n:]\n                if not data:\n                    return\n            # Not all was written; register write handler.\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n        # Add it to the buffer.\n        self._buffer.append(data)\n        self._maybe_pause_protocol()\n\n    def _get_sendmsg_buffer(self):\n        return itertools.islice(self._buffer, SC_IOV_MAX)\n\n    def _write_sendmsg(self):\n        assert self._buffer, 'Data should not be empty'\n        if self._conn_lost:\n            return\n        try:\n            nbytes = self._sock.sendmsg(self._get_sendmsg_buffer())\n            self._adjust_leftover_buffer(nbytes)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def _adjust_leftover_buffer(self, nbytes: int) -> None:\n        buffer = self._buffer\n        while nbytes:\n            b = buffer.popleft()\n            b_len = len(b)\n            if b_len <= nbytes:\n                nbytes -= b_len\n            else:\n                buffer.appendleft(b[nbytes:])\n                break\n\n    def _write_send(self):\n        assert self._buffer, 'Data should not be empty'\n        if self._conn_lost:\n            return\n        try:\n            buffer = self._buffer.popleft()\n            n = self._sock.send(buffer)\n            if n != len(buffer):\n                # Not all data was written\n                self._buffer.appendleft(buffer[n:])\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def write_eof(self):\n        if self._closing or self._eof:\n            return\n        self._eof = True\n        if not self._buffer:\n            self._sock.shutdown(socket.SHUT_WR)\n\n    def writelines(self, list_of_data):\n        if self._eof:\n            raise RuntimeError('Cannot call writelines() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to writelines; sendfile is in progress')\n        if not list_of_data:\n            return\n        self._buffer.extend([memoryview(data) for data in list_of_data])\n        self._write_ready()\n        # If the entire buffer couldn't be written, register a write handler\n        if self._buffer:\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n    def can_write_eof(self):\n        return True\n\n    def _call_connection_lost(self, exc):\n        super()._call_connection_lost(exc)\n        if self._empty_waiter is not None:\n            self._empty_waiter.set_exception(\n                ConnectionError(\"Connection is closed by peer\"))\n\n    def _make_empty_waiter(self):\n        if self._empty_waiter is not None:\n            raise RuntimeError(\"Empty waiter is already set\")\n        self._empty_waiter = self._loop.create_future()\n        if not self._buffer:\n            self._empty_waiter.set_result(None)\n        return self._empty_waiter\n\n    def _reset_empty_waiter(self):\n        self._empty_waiter = None\n\n    def close(self):\n        self._read_ready_cb = None\n        self._write_ready = None\n        super().close()\n\n\nclass _SelectorDatagramTransport(_SelectorTransport, transports.DatagramTransport):\n\n    _buffer_factory = collections.deque\n\n    def __init__(self, loop, sock, protocol, address=None,\n                 waiter=None, extra=None):\n        super().__init__(loop, sock, protocol, extra)\n        self._address = address\n        self._buffer_size = 0\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def get_write_buffer_size(self):\n        return self._buffer_size\n\n    def _read_ready(self):\n        if self._conn_lost:\n            return\n        try:\n            data, addr = self._sock.recvfrom(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on datagram transport')\n        else:\n            self._protocol.datagram_received(data, addr)\n\n    def sendto(self, data, addr=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n\n        if self._address:\n            if addr not in (None, self._address):\n                raise ValueError(\n                    f'Invalid address: must be None or {self._address}')\n            addr = self._address\n\n        if self._conn_lost and self._address:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n                return\n            except (BlockingIOError, InterruptedError):\n                self._loop._add_writer(self._sock_fd, self._sendto_ready)\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        # Ensure that what we buffer is immutable.\n        self._buffer.append((bytes(data), addr))\n        self._buffer_size += len(data) + 8  # include header bytes\n        self._maybe_pause_protocol()\n\n    def _sendto_ready(self):\n        while self._buffer:\n            data, addr = self._buffer.popleft()\n            self._buffer_size -= len(data)\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n            except (BlockingIOError, InterruptedError):\n                self._buffer.appendleft((data, addr))  # Try again later.\n                self._buffer_size += len(data)\n                break\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        self._maybe_resume_protocol()  # May append to buffer.\n        if not self._buffer:\n            self._loop._remove_writer(self._sock_fd)\n            if self._closing:\n                self._call_connection_lost(None)\n", 1311], "/usr/lib/python3.13/weakref.py": ["\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttps://peps.python.org/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType,\n     _remove_dead_weakref)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nimport _collections_abc  # Import after _weakref to avoid circular import.\nimport sys\nimport itertools\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\",\n           \"WeakSet\", \"WeakMethod\", \"finalize\"]\n\n\n_collections_abc.MutableSet.register(WeakSet)\n\nclass WeakMethod(ref):\n    \"\"\"\n    A custom `weakref.ref` subclass which simulates a weak reference to\n    a bound method, working around the lifetime problem of bound methods.\n    \"\"\"\n\n    __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\n\n    def __new__(cls, meth, callback=None):\n        try:\n            obj = meth.__self__\n            func = meth.__func__\n        except AttributeError:\n            raise TypeError(\"argument should be a bound method, not {}\"\n                            .format(type(meth))) from None\n        def _cb(arg):\n            # The self-weakref trick is needed to avoid creating a reference\n            # cycle.\n            self = self_wr()\n            if self._alive:\n                self._alive = False\n                if callback is not None:\n                    callback(self)\n        self = ref.__new__(cls, obj, _cb)\n        self._func_ref = ref(func, _cb)\n        self._meth_type = type(meth)\n        self._alive = True\n        self_wr = ref(self)\n        return self\n\n    def __call__(self):\n        obj = super().__call__()\n        func = self._func_ref()\n        if obj is None or func is None:\n            return None\n        return self._meth_type(func, obj)\n\n    def __eq__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is other\n            return ref.__eq__(self, other) and self._func_ref == other._func_ref\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is not other\n            return ref.__ne__(self, other) or self._func_ref != other._func_ref\n        return NotImplemented\n\n    __hash__ = ref.__hash__\n\n\nclass WeakValueDictionary(_collections_abc.MutableMapping):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, other=(), /, **kw):\n        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Atomic removal is necessary since this function\n                    # can be called asynchronously by the GC\n                    _atomic_removal(self.data, wr.key)\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n        self.update(other, **kw)\n\n    def _commit_removals(self, _atomic_removal=_remove_dead_weakref):\n        pop = self._pending_removals.pop\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _atomic_removal(d, key)\n\n    def __getitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        o = self.data[key]()\n        if o is None:\n            raise KeyError(key)\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __len__(self):\n        if self._pending_removals:\n            self._commit_removals()\n        return len(self.data)\n\n    def __contains__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def copy(self):\n        if self._pending_removals:\n            self._commit_removals()\n        new = WeakValueDictionary()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        if self._pending_removals:\n            self._commit_removals()\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                v = wr()\n                if v is not None:\n                    yield k, v\n\n    def keys(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                if wr() is not None:\n                    yield k\n\n    __iter__ = keys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            yield from self.data.values()\n\n    def values(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for wr in self.data.values():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            else:\n                raise KeyError(key)\n        else:\n            return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n\n    def update(self, other=None, /, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if other is not None:\n            if not hasattr(other, \"items\"):\n                other = dict(other)\n            for key, o in other.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        for key, o in kwargs.items():\n            d[key] = KeyedRef(o, self._remove, key)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        return list(self.data.values())\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super().__init__(ob, callback)\n\n\nclass WeakKeyDictionary(_collections_abc.MutableMapping):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    try:\n                        del self.data[k]\n                    except KeyError:\n                        pass\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        self._dirty_len = False\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        pop = self._pending_removals.pop\n        d = self.data\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n\n            try:\n                del d[key]\n            except KeyError:\n                pass\n\n    def _scrub_removals(self):\n        d = self.data\n        self._pending_removals = [k for k in self._pending_removals if k in d]\n        self._dirty_len = False\n\n    def __delitem__(self, key):\n        self._dirty_len = True\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __len__(self):\n        if self._dirty_len and self._pending_removals:\n            # self._pending_removals may still contain keys which were\n            # explicitly removed, we have to scrub them (see issue #21173).\n            self._scrub_removals()\n        return len(self.data) - len(self._pending_removals)\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def items(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def keys(self):\n        with _IterationGuard(self):\n            for wr in self.data:\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = keys\n\n    def values(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                if wr() is not None:\n                    yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return list(self.data)\n\n    def popitem(self):\n        self._dirty_len = True\n        while True:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        self._dirty_len = True\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, /, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass finalize:\n    \"\"\"Class for finalization of weakrefable objects\n\n    finalize(obj, func, *args, **kwargs) returns a callable finalizer\n    object which will be called when obj is garbage collected. The\n    first time the finalizer is called it evaluates func(*arg, **kwargs)\n    and returns the result. After this the finalizer is dead, and\n    calling it just returns None.\n\n    When the program exits any remaining finalizers for which the\n    atexit attribute is true will be run in reverse order of creation.\n    By default atexit is true.\n    \"\"\"\n\n    # Finalizer objects don't have any state of their own.  They are\n    # just used as keys to lookup _Info objects in the registry.  This\n    # ensures that they cannot be part of a ref-cycle.\n\n    __slots__ = ()\n    _registry = {}\n    _shutdown = False\n    _index_iter = itertools.count()\n    _dirty = False\n    _registered_with_atexit = False\n\n    class _Info:\n        __slots__ = (\"weakref\", \"func\", \"args\", \"kwargs\", \"atexit\", \"index\")\n\n    def __init__(self, obj, func, /, *args, **kwargs):\n        if not self._registered_with_atexit:\n            # We may register the exit function more than once because\n            # of a thread race, but that is harmless\n            import atexit\n            atexit.register(self._exitfunc)\n            finalize._registered_with_atexit = True\n        info = self._Info()\n        info.weakref = ref(obj, self)\n        info.func = func\n        info.args = args\n        info.kwargs = kwargs or None\n        info.atexit = True\n        info.index = next(self._index_iter)\n        self._registry[self] = info\n        finalize._dirty = True\n\n    def __call__(self, _=None):\n        \"\"\"If alive then mark as dead and return func(*args, **kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.pop(self, None)\n        if info and not self._shutdown:\n            return info.func(*info.args, **(info.kwargs or {}))\n\n    def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    def peek(self):\n        \"\"\"If alive then return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None:\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    @property\n    def alive(self):\n        \"\"\"Whether finalizer is alive\"\"\"\n        return self in self._registry\n\n    @property\n    def atexit(self):\n        \"\"\"Whether finalizer should be called at exit\"\"\"\n        info = self._registry.get(self)\n        return bool(info) and info.atexit\n\n    @atexit.setter\n    def atexit(self, value):\n        info = self._registry.get(self)\n        if info:\n            info.atexit = bool(value)\n\n    def __repr__(self):\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is None:\n            return '<%s object at %#x; dead>' % (type(self).__name__, id(self))\n        else:\n            return '<%s object at %#x; for %r at %#x>' % \\\n                (type(self).__name__, id(self), type(obj).__name__, id(obj))\n\n    @classmethod\n    def _select_for_exit(cls):\n        # Return live finalizers marked for exit, oldest first\n        L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n        L.sort(key=lambda item:item[1].index)\n        return [f for (f,i) in L]\n\n    @classmethod\n    def _exitfunc(cls):\n        # At shutdown invoke finalizers for which atexit is true.\n        # This is called once all other non-daemonic threads have been\n        # joined.\n        reenable_gc = False\n        try:\n            if cls._registry:\n                import gc\n                if gc.isenabled():\n                    reenable_gc = True\n                    gc.disable()\n                pending = None\n                while True:\n                    if pending is None or finalize._dirty:\n                        pending = cls._select_for_exit()\n                        finalize._dirty = False\n                    if not pending:\n                        break\n                    f = pending.pop()\n                    try:\n                        # gc is disabled, so (assuming no daemonic\n                        # threads) the following is the only line in\n                        # this function which might trigger creation\n                        # of a new finalizer\n                        f()\n                    except Exception:\n                        sys.excepthook(*sys.exc_info())\n                    assert f not in cls._registry\n        finally:\n            # prevent any more finalizers from executing during shutdown\n            finalize._shutdown = True\n            if reenable_gc:\n                gc.enable()\n", 674], "/usr/lib/python3.13/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport warnings\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs', 'gettrace', 'getprofile',\n           'setprofile_all_threads','settrace_all_threads']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_joinable_thread = _thread.start_joinable_thread\n_daemon_threads_allowed = _thread.daemon_threads_allowed\n_allocate_lock = _thread.allocate_lock\n_LockType = _thread.LockType\n_thread_shutdown = _thread._shutdown\n_make_thread_handle = _thread._make_thread_handle\n_ThreadHandle = _thread._ThreadHandle\nget_ident = _thread.get_ident\n_get_main_thread_ident = _thread._get_main_thread_ident\n_is_main_interpreter = _thread._is_main_interpreter\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef setprofile_all_threads(func):\n    \"\"\"Set a profile function for all threads started from the threading module\n    and all Python threads that are currently executing.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n    \"\"\"\n    setprofile(func)\n    _sys._setprofileallthreads(func)\n\ndef getprofile():\n    \"\"\"Get the profiler function as set by threading.setprofile().\"\"\"\n    return _profile_hook\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\ndef settrace_all_threads(func):\n    \"\"\"Set a trace function for all threads started from the threading module\n    and all Python threads that are currently executing.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n    \"\"\"\n    settrace(func)\n    _sys._settraceallthreads(func)\n\ndef gettrace():\n    \"\"\"Get the trace function as set by threading.settrace().\"\"\"\n    return _trace_hook\n\n# Synchronization classes\n\nLock = _LockType\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if args or kwargs:\n        warnings.warn(\n            'Passing arguments to RLock is deprecated and will be removed in 3.15',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n    # Internal method used for reentrancy checks\n\n    def _recursion_count(self):\n        if self._owner != get_ident():\n            return 0\n        return self._count\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        if hasattr(lock, '_release_save'):\n            self._release_save = lock._release_save\n        if hasattr(lock, '_acquire_restore'):\n            self._acquire_restore = lock._acquire_restore\n        if hasattr(lock, '_is_owned'):\n            self._is_owned = lock._is_owned\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating-point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        This method is deprecated, use notify_all() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('notifyAll() is deprecated, use notify_all() instead',\n                      DeprecationWarning, stacklevel=2)\n        self.notify_all()\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}>\")\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            self._cond.notify(n)\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        super().__init__(value)\n        self._initial_value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}/{self._initial_value}>\")\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            self._cond.notify(n)\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def __repr__(self):\n        cls = self.__class__\n        status = 'set' if self._flag else 'unset'\n        return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: {status}>\"\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._after_fork()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    def isSet(self):\n        \"\"\"Return true if and only if the internal flag is true.\n\n        This method is deprecated, use is_set() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isSet() is deprecated, use is_set() instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.is_set()\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating-point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        if parties < 1:\n            raise ValueError(\"parties must be > 0\")\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def __repr__(self):\n        cls = self.__class__\n        if self.broken:\n            return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: broken>\"\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" waiters={self.n_waiting}/{self.parties}>\")\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count(1).__next__\ndef _newname(name_template):\n    return name_template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        if name:\n            name = str(name)\n        else:\n            name = _newname(\"Thread-%d\")\n            if target is not None:\n                try:\n                    target_name = target.__name__\n                    name += f\" ({target_name})\"\n                except AttributeError:\n                    pass\n\n        self._target = target\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            if daemon and not _daemon_threads_allowed():\n                raise RuntimeError('daemon threads are disabled in this (sub)interpreter')\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._handle = _ThreadHandle()\n        self._started = Event()\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _after_fork(self, new_ident=None):\n        # Private!  Called by threading._after_fork().\n        self._started._at_fork_reinit()\n        if new_ident is not None:\n            # This thread is alive.\n            self._ident = new_ident\n            assert self._handle.ident == new_ident\n        else:\n            # Otherwise, the thread is dead, Jim.  _PyThread_AfterFork()\n            # already marked our handle done.\n            pass\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        if self._handle.is_done():\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            # Start joinable thread\n            _start_joinable_thread(self._bootstrap, handle=self._handle,\n                                   daemon=self.daemon)\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()  # Will set ident and native_id\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            self._delete()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating-point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        # the behavior of a negative timeout isn't documented, but\n        # historically .join(timeout=x) for x<0 has acted as if timeout=0\n        if timeout is not None:\n            timeout = max(timeout, 0)\n\n        self._handle.join(timeout)\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._started.is_set() and not self._handle.is_done()\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if daemonic and not _daemon_threads_allowed():\n            raise RuntimeError('daemon threads are disabled in this interpreter')\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        \"\"\"Return whether this thread is a daemon.\n\n        This method is deprecated, use the daemon attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        \"\"\"Set whether this thread is a daemon.\n\n        This method is deprecated, use the .daemon property instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.daemon = daemonic\n\n    def getName(self):\n        \"\"\"Return a string used for identification purposes only.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('getName() is deprecated, get the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.name\n\n    def setName(self, name):\n        \"\"\"Set the name string for this thread.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setName() is deprecated, set the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\n# Original value of threading.excepthook\n__excepthook__ = excepthook\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._started.set()\n        self._ident = _get_main_thread_ident()\n        self._handle = _make_thread_handle(self._ident)\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Helper thread-local instance to detect when a _DummyThread\n# is collected. Not a part of the public API.\n_thread_local_info = local()\n\n\nclass _DeleteDummyThreadOnDel:\n    '''\n    Helper class to remove a dummy thread from threading._active on __del__.\n    '''\n\n    def __init__(self, dummy_thread):\n        self._dummy_thread = dummy_thread\n        self._tident = dummy_thread.ident\n        # Put the thread on a thread local variable so that when\n        # the related thread finishes this instance is collected.\n        #\n        # Note: no other references to this instance may be created.\n        # If any client code creates a reference to this instance,\n        # the related _DummyThread will be kept forever!\n        _thread_local_info._track_dummy_thread_ref = self\n\n    def __del__(self):\n        with _active_limbo_lock:\n            if _active.get(self._tident) is self._dummy_thread:\n                _active.pop(self._tident, None)\n\n\n# Dummy thread class to represent threads not started here.\n# These should be added to `_active` and removed automatically\n# when they die, although they can't be waited for.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"),\n                        daemon=_daemon_threads_allowed())\n        self._started.set()\n        self._set_ident()\n        self._handle = _make_thread_handle(self._ident)\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n        _DeleteDummyThreadOnDel(self)\n\n    def is_alive(self):\n        if not self._handle.is_done() and self._started.is_set():\n            return True\n        raise RuntimeError(\"thread is not alive\")\n\n    def join(self, timeout=None):\n        raise RuntimeError(\"cannot join a dummy thread\")\n\n    def _after_fork(self, new_ident=None):\n        if new_ident is not None:\n            self.__class__ = _MainThread\n            self._name = 'MainThread'\n            self._daemonic = False\n        Thread._after_fork(self, new_ident=new_ident)\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    This function is deprecated, use current_thread() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('currentThread() is deprecated, use current_thread() instead',\n                  DeprecationWarning, stacklevel=2)\n    return current_thread()\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    # NOTE: if the logic in here ever changes, update Modules/posixmodule.c\n    # warn_about_fork_with_threads() to match.\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    This function is deprecated, use active_count() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('activeCount() is deprecated, use active_count() instead',\n                  DeprecationWarning, stacklevel=2)\n    return active_count()\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    _threading_atexits.append(lambda: func(*arg, **kwargs))\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure: other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it. We can't wait for it to be marked as done\n    # normally - that won't happen until the interpreter is nearly dead. So\n    # mark it done here.\n    if _main_thread._handle.is_done() and _is_main_interpreter():\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    if _is_main_interpreter():\n        _main_thread._handle._set_done()\n\n    # Wait for all non-daemon threads to exit.\n    _thread_shutdown()\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    # XXX Figure this out for subinterpreters.  (See gh-75698.)\n    return _main_thread\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # This is the one and only active thread.\n                ident = get_ident()\n                thread._after_fork(new_ident=ident)\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._after_fork()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1599], "/usr/lib/python3.13/signal.py": ["import _signal\nfrom _signal import *\nfrom enum import IntEnum as _IntEnum\n\n_globals = globals()\n\n_IntEnum._convert_(\n        'Signals', __name__,\n        lambda name:\n            name.isupper()\n            and (name.startswith('SIG') and not name.startswith('SIG_'))\n            or name.startswith('CTRL_'))\n\n_IntEnum._convert_(\n        'Handlers', __name__,\n        lambda name: name in ('SIG_DFL', 'SIG_IGN'))\n\nif 'pthread_sigmask' in _globals:\n    _IntEnum._convert_(\n            'Sigmasks', __name__,\n            lambda name: name in ('SIG_BLOCK', 'SIG_UNBLOCK', 'SIG_SETMASK'))\n\n\ndef _int_to_enum(value, enum_klass):\n    \"\"\"Convert a possible numeric value to an IntEnum member.\n    If it's not a known member, return the value itself.\n    \"\"\"\n    if not isinstance(value, int):\n        return value\n    try:\n        return enum_klass(value)\n    except ValueError:\n        return value\n\n\ndef _enum_to_int(value):\n    \"\"\"Convert an IntEnum member to a numeric value.\n    If it's not an IntEnum member return the value itself.\n    \"\"\"\n    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return value\n\n\n# Similar to functools.wraps(), but only assign __doc__.\n# __module__ should be preserved,\n# __name__ and __qualname__ are already fine,\n# __annotations__ is not set.\ndef _wraps(wrapped):\n    def decorator(wrapper):\n        wrapper.__doc__ = wrapped.__doc__\n        return wrapper\n    return decorator\n\n@_wraps(_signal.signal)\ndef signal(signalnum, handler):\n    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n    return _int_to_enum(handler, Handlers)\n\n\n@_wraps(_signal.getsignal)\ndef getsignal(signalnum):\n    handler = _signal.getsignal(signalnum)\n    return _int_to_enum(handler, Handlers)\n\n\nif 'pthread_sigmask' in _globals:\n    @_wraps(_signal.pthread_sigmask)\n    def pthread_sigmask(how, mask):\n        sigs_set = _signal.pthread_sigmask(how, mask)\n        return set(_int_to_enum(x, Signals) for x in sigs_set)\n\n\nif 'sigpending' in _globals:\n    @_wraps(_signal.sigpending)\n    def sigpending():\n        return {_int_to_enum(x, Signals) for x in _signal.sigpending()}\n\n\nif 'sigwait' in _globals:\n    @_wraps(_signal.sigwait)\n    def sigwait(sigset):\n        retsig = _signal.sigwait(sigset)\n        return _int_to_enum(retsig, Signals)\n\n\nif 'valid_signals' in _globals:\n    @_wraps(_signal.valid_signals)\n    def valid_signals():\n        return {_int_to_enum(x, Signals) for x in _signal.valid_signals()}\n\n\ndel _globals, _wraps\n", 94], "/usr/lib/python3.13/asyncio/base_futures.py": ["__all__ = ()\n\nimport reprlib\n\nfrom . import format_helpers\n\n# States for Future.\n_PENDING = 'PENDING'\n_CANCELLED = 'CANCELLED'\n_FINISHED = 'FINISHED'\n\n\ndef isfuture(obj):\n    \"\"\"Check for a Future.\n\n    This returns True when obj is a Future instance or is advertising\n    itself as duck-type compatible by setting _asyncio_future_blocking.\n    See comment in Future for more details.\n    \"\"\"\n    return (hasattr(obj.__class__, '_asyncio_future_blocking') and\n            obj._asyncio_future_blocking is not None)\n\n\ndef _format_callbacks(cb):\n    \"\"\"helper function for Future.__repr__\"\"\"\n    size = len(cb)\n    if not size:\n        cb = ''\n\n    def format_cb(callback):\n        return format_helpers._format_callback_source(callback, ())\n\n    if size == 1:\n        cb = format_cb(cb[0][0])\n    elif size == 2:\n        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))\n    elif size > 2:\n        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),\n                                        size - 2,\n                                        format_cb(cb[-1][0]))\n    return f'cb=[{cb}]'\n\n\ndef _future_repr_info(future):\n    # (Future) -> str\n    \"\"\"helper function for Future.__repr__\"\"\"\n    info = [future._state.lower()]\n    if future._state == _FINISHED:\n        if future._exception is not None:\n            info.append(f'exception={future._exception!r}')\n        else:\n            # use reprlib to limit the length of the output, especially\n            # for very long strings\n            result = reprlib.repr(future._result)\n            info.append(f'result={result}')\n    if future._callbacks:\n        info.append(_format_callbacks(future._callbacks))\n    if future._source_traceback:\n        frame = future._source_traceback[-1]\n        info.append(f'created at {frame[0]}:{frame[1]}')\n    return info\n\n\n@reprlib.recursive_repr()\ndef _future_repr(future):\n    info = ' '.join(_future_repr_info(future))\n    return f'<{future.__class__.__name__} {info}>'\n", 67], "/usr/lib/python3.13/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\nfrom types import GenericAlias\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n    _cancel_message = None\n    # A saved CancelledError for later chaining as an exception context.\n    _cancelled_exc = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    def __repr__(self):\n        return base_futures._future_repr(self)\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if val:\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def _make_cancelled_error(self):\n        \"\"\"Create the CancelledError to raise if the Future is cancelled.\n\n        This should only be called once when handling a cancellation since\n        it erases the saved context exception value.\n        \"\"\"\n        if self._cancelled_exc is not None:\n            exc = self._cancelled_exc\n            self._cancelled_exc = None\n            return exc\n\n        if self._cancel_message is None:\n            exc = exceptions.CancelledError()\n        else:\n            exc = exceptions.CancelledError(self._cancel_message)\n        return exc\n\n    def cancel(self, msg=None):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self._cancel_message = msg\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception.with_traceback(self._exception_tb)\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if isinstance(exception, StopIteration):\n            new_exc = RuntimeError(\"StopIteration interacts badly with \"\n                                   \"generators and cannot be raised into a \"\n                                   \"Future\")\n            new_exc.__cause__ = exception\n            new_exc.__context__ = exception\n            exception = new_exc\n        self._exception = exception\n        self._exception_tb = exception.__traceback__\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args).with_traceback(exc.__traceback__)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args).with_traceback(exc.__traceback__)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            if dest_loop.is_closed():\n                return\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events.get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 427], "/usr/lib/python3.13/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    'create_eager_task_factory', 'eager_task_factory',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport math\nimport types\nimport weakref\nfrom types import GenericAlias\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import queues\nfrom . import timeouts\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # capturing the set of eager tasks first, so if an eager task \"graduates\"\n    # to a regular task in another thread, we don't risk missing it.\n    eager_tasks = list(_eager_tasks)\n    # Looping over the WeakSet isn't safe as it can be updated from another\n    # thread, therefore we cast it to list prior to filtering. The list cast\n    # itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur).\n    # See issues 34970 and 36607 for details.\n    scheduled_tasks = None\n    i = 0\n    while True:\n        try:\n            scheduled_tasks = list(_scheduled_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in itertools.chain(scheduled_tasks, eager_tasks)\n            if futures._get_loop(t) is loop and not t.done()}\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    # _fut_waiter is either None or a Future.  The Future\n    # can be either done() or not done().\n    # The task can be in any of 3 states:\n    #\n    # - 1: _fut_waiter is not None and not _fut_waiter.done():\n    #      __step() is *not* scheduled and the Task is waiting for _fut_waiter.\n    # - 2: (_fut_waiter is None or _fut_waiter.done()) and __step() is scheduled:\n    #       the Task is waiting for __step() to be executed.\n    # - 3:  _fut_waiter is None and __step() is *not* scheduled:\n    #       the Task is currently executing (in __step()).\n    #\n    # * In state 1, one of the callbacks of __fut_waiter must be __wakeup().\n    # * The transition from 1 to 2 happens when _fut_waiter becomes done(),\n    #   as it schedules __wakeup() to be called (which calls __step() so\n    #   we way that __step() is scheduled).\n    # * It transitions from 2 to 3 when __step() is executed, and it clears\n    #   _fut_waiter to None.\n\n    # If False, don't log a message if the task is destroyed while its\n    # status is still pending\n    _log_destroy_pending = True\n\n    def __init__(self, coro, *, loop=None, name=None, context=None,\n                 eager_start=False):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._num_cancels_requested = 0\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        if context is None:\n            self._context = contextvars.copy_context()\n        else:\n            self._context = context\n\n        if eager_start and self._loop.is_running():\n            self.__eager_start()\n        else:\n            self._loop.call_soon(self.__step, context=self._context)\n            _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __repr__(self):\n        return base_tasks._task_repr(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_context(self):\n        return self._context\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self, msg=None):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n\n        This also increases the task's count of cancellation requests.\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        self._num_cancels_requested += 1\n        # These two lines are controversial.  See discussion starting at\n        # https://github.com/python/cpython/pull/31394#issuecomment-1053545331\n        # Also remember that this is duplicated in _asynciomodule.c.\n        # if self._num_cancels_requested > 1:\n        #     return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel(msg=msg):\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        self._cancel_message = msg\n        return True\n\n    def cancelling(self):\n        \"\"\"Return the count of the task's cancellation requests.\n\n        This count is incremented when .cancel() is called\n        and may be decremented using .uncancel().\n        \"\"\"\n        return self._num_cancels_requested\n\n    def uncancel(self):\n        \"\"\"Decrement the task's count of cancellation requests.\n\n        This should be called by the party that called `cancel()` on the task\n        beforehand.\n\n        Returns the remaining number of cancellation requests.\n        \"\"\"\n        if self._num_cancels_requested > 0:\n            self._num_cancels_requested -= 1\n            if self._num_cancels_requested == 0:\n                self._must_cancel = False\n        return self._num_cancels_requested\n\n    def __eager_start(self):\n        prev_task = _swap_current_task(self._loop, self)\n        try:\n            _register_eager_task(self)\n            try:\n                self._context.run(self.__step_run_and_handle_result, None)\n            finally:\n                _unregister_eager_task(self)\n        finally:\n            try:\n                curtask = _swap_current_task(self._loop, prev_task)\n                assert curtask is self\n            finally:\n                if self.done():\n                    self._coro = None\n                    self = None  # Needed to break cycles when an exception occurs.\n                else:\n                    _register_task(self)\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = self._make_cancelled_error()\n            self._must_cancel = False\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        try:\n            self.__step_run_and_handle_result(exc)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __step_run_and_handle_result(self, exc):\n        coro = self._coro\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel(msg=self._cancel_message)\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError as exc:\n            # Save the original exception so we can chain it later.\n            self._cancelled_exc = exc\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel(\n                                    msg=self._cancel_message):\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None, context=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    if context is None:\n        # Use legacy API if context is not needed\n        task = loop.create_task(coro, name=name)\n    else:\n        task = loop.create_task(coro, name=name, context=context)\n\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures or Tasks given by fs to complete.\n\n    The fs iterable must not be empty.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of Tasks/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    fs = set(fs)\n\n    if any(coroutines.iscoroutine(f) for f in fs):\n        raise TypeError(\"Passing coroutines is forbidden, use tasks explicitly.\")\n\n    loop = events.get_running_loop()\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    If the task suppresses the cancellation and returns a value instead,\n    that value is returned.\n\n    This function is a coroutine.\n    \"\"\"\n    # The special case for timeout <= 0 is for the following case:\n    #\n    # async def test_waitfor():\n    #     func_started = False\n    #\n    #     async def func():\n    #         nonlocal func_started\n    #         func_started = True\n    #\n    #     try:\n    #         await asyncio.wait_for(func(), 0)\n    #     except asyncio.TimeoutError:\n    #         assert not func_started\n    #     else:\n    #         assert False\n    #\n    # asyncio.run(test_waitfor())\n\n\n    if timeout is not None and timeout <= 0:\n        fut = ensure_future(fut)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut)\n        try:\n            return fut.result()\n        except exceptions.CancelledError as exc:\n            raise TimeoutError from exc\n\n    async with timeouts.timeout(timeout):\n        return await fut\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    loop = events.get_running_loop()\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\nclass _AsCompletedIterator:\n    \"\"\"Iterator of awaitables representing tasks of asyncio.as_completed.\n\n    As an asynchronous iterator, iteration yields futures as they finish. As a\n    plain iterator, new coroutines are yielded that will return or raise the\n    result of the next underlying future to complete.\n    \"\"\"\n    def __init__(self, aws, timeout):\n        self._done = queues.Queue()\n        self._timeout_handle = None\n\n        loop = events.get_event_loop()\n        todo = {ensure_future(aw, loop=loop) for aw in set(aws)}\n        for f in todo:\n            f.add_done_callback(self._handle_completion)\n        if todo and timeout is not None:\n            self._timeout_handle = (\n                loop.call_later(timeout, self._handle_timeout)\n            )\n        self._todo = todo\n        self._todo_left = len(todo)\n\n    def __aiter__(self):\n        return self\n\n    def __iter__(self):\n        return self\n\n    async def __anext__(self):\n        if not self._todo_left:\n            raise StopAsyncIteration\n        assert self._todo_left > 0\n        self._todo_left -= 1\n        return await self._wait_for_one()\n\n    def __next__(self):\n        if not self._todo_left:\n            raise StopIteration\n        assert self._todo_left > 0\n        self._todo_left -= 1\n        return self._wait_for_one(resolve=True)\n\n    def _handle_timeout(self):\n        for f in self._todo:\n            f.remove_done_callback(self._handle_completion)\n            self._done.put_nowait(None)  # Sentinel for _wait_for_one().\n        self._todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _handle_completion(self, f):\n        if not self._todo:\n            return  # _handle_timeout() was here first.\n        self._todo.remove(f)\n        self._done.put_nowait(f)\n        if not self._todo and self._timeout_handle is not None:\n            self._timeout_handle.cancel()\n\n    async def _wait_for_one(self, resolve=False):\n        # Wait for the next future to be done and return it unless resolve is\n        # set, in which case return either the result of the future or raise\n        # an exception.\n        f = await self._done.get()\n        if f is None:\n            # Dummy value from _handle_timeout().\n            raise exceptions.TimeoutError\n        return f.result() if resolve else f\n\n\ndef as_completed(fs, *, timeout=None):\n    \"\"\"Create an iterator of awaitables or their results in completion order.\n\n    Run the supplied awaitables concurrently. The returned object can be\n    iterated to obtain the results of the awaitables as they finish.\n\n    The object returned can be iterated as an asynchronous iterator or a plain\n    iterator. When asynchronous iteration is used, the originally-supplied\n    awaitables are yielded if they are tasks or futures. This makes it easy to\n    correlate previously-scheduled tasks with their results:\n\n        ipv4_connect = create_task(open_connection(\"127.0.0.1\", 80))\n        ipv6_connect = create_task(open_connection(\"::1\", 80))\n        tasks = [ipv4_connect, ipv6_connect]\n\n        async for earliest_connect in as_completed(tasks):\n            # earliest_connect is done. The result can be obtained by\n            # awaiting it or calling earliest_connect.result()\n            reader, writer = await earliest_connect\n\n            if earliest_connect is ipv6_connect:\n                print(\"IPv6 connection established.\")\n            else:\n                print(\"IPv4 connection established.\")\n\n    During asynchronous iteration, implicitly-created tasks will be yielded for\n    supplied awaitables that aren't tasks or futures.\n\n    When used as a plain iterator, each iteration yields a new coroutine that\n    returns the result or raises the exception of the next completed awaitable.\n    This pattern is compatible with Python versions older than 3.13:\n\n        ipv4_connect = create_task(open_connection(\"127.0.0.1\", 80))\n        ipv6_connect = create_task(open_connection(\"::1\", 80))\n        tasks = [ipv4_connect, ipv6_connect]\n\n        for next_connect in as_completed(tasks):\n            # next_connect is not one of the original task objects. It must be\n            # awaited to obtain the result value or raise the exception of the\n            # awaitable that finishes next.\n            reader, writer = await next_connect\n\n    A TimeoutError is raised if the timeout occurs before all awaitables are\n    done. This is raised by the async for loop during asynchronous iteration or\n    by the coroutines yielded during plain iteration.\n    \"\"\"\n    if inspect.isawaitable(fs):\n        raise TypeError(\n            f\"expects an iterable of awaitables, not {type(fs).__name__}\"\n        )\n\n    return _AsCompletedIterator(fs, timeout)\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    if math.isnan(delay):\n        raise ValueError(\"Invalid delay: NaN (not a number)\")\n\n    loop = events.get_running_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    if futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                            'the one specified as the loop argument')\n        return coro_or_future\n    should_close = True\n    if not coroutines.iscoroutine(coro_or_future):\n        if inspect.isawaitable(coro_or_future):\n            async def _wrap_awaitable(awaitable):\n                return await awaitable\n\n            coro_or_future = _wrap_awaitable(coro_or_future)\n            should_close = False\n        else:\n            raise TypeError('An asyncio.Future, a coroutine or an awaitable '\n                            'is required')\n\n    if loop is None:\n        loop = events.get_event_loop()\n    try:\n        return loop.create_task(coro_or_future)\n    except RuntimeError:\n        if should_close:\n            coro_or_future.close()\n        raise\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop):\n        assert loop is not None\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self, msg=None):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel(msg=msg):\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if not coros_or_futures:\n        loop = events.get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer is None or outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\n                    # will *raise* a CancelledError instead of returning it.\n                    # Also, since we're adding the exception return value\n                    # to 'results' instead of raising it, don't bother\n                    # setting __context__.  This also lets us preserve\n                    # calling '_make_cancelled_error()' at most once.\n                    res = exceptions.CancelledError(\n                        '' if fut._cancel_message is None else\n                        fut._cancel_message)\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    done_futs = []\n    loop = None\n    outer = None  # bpo-46672\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            if fut.done():\n                done_futs.append(fut)\n            else:\n                fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    # Run done callbacks after GatheringFuture created so any post-processing\n    # can be performed at this point\n    # optimization: in the special case that *all* futures finished eagerly,\n    # this will effectively complete the gather eagerly, with the last\n    # callback setting the result (or exception) on outer before returning it\n    for fut in done_futs:\n        _done_callback(fut)\n    return outer\n\n\ndef shield(arg):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        task = asyncio.create_task(something())\n        res = await shield(task)\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        task = asyncio.create_task(something())\n        try:\n            res = await shield(task)\n        except CancelledError:\n            res = None\n\n    Save a reference to tasks passed to this function, to avoid\n    a task disappearing mid-execution. The event loop only keeps\n    weak references to tasks. A task that isn't referenced elsewhere\n    may get garbage collected at any time, even before it's done.\n    \"\"\"\n    inner = ensure_future(arg)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\ndef create_eager_task_factory(custom_task_constructor):\n    \"\"\"Create a function suitable for use as a task factory on an event-loop.\n\n        Example usage:\n\n            loop.set_task_factory(\n                asyncio.create_eager_task_factory(my_task_constructor))\n\n        Now, tasks created will be started immediately (rather than being first\n        scheduled to an event loop). The constructor argument can be any callable\n        that returns a Task-compatible object and has a signature compatible\n        with `Task.__init__`; it must have the `eager_start` keyword argument.\n\n        Most applications will use `Task` for `custom_task_constructor` and in\n        this case there's no need to call `create_eager_task_factory()`\n        directly. Instead the  global `eager_task_factory` instance can be\n        used. E.g. `loop.set_task_factory(asyncio.eager_task_factory)`.\n        \"\"\"\n\n    def factory(loop, coro, *, name=None, context=None):\n        return custom_task_constructor(\n            coro, loop=loop, name=name, context=context, eager_start=True)\n\n    return factory\n\n\neager_task_factory = create_eager_task_factory(Task)\n\n\n# Collectively these two sets hold references to the complete set of active\n# tasks. Eagerly executed tasks use a faster regular set as an optimization\n# but may graduate to a WeakSet if the task blocks on IO.\n_scheduled_tasks = weakref.WeakSet()\n_eager_tasks = set()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register an asyncio Task scheduled to run on an event loop.\"\"\"\n    _scheduled_tasks.add(task)\n\n\ndef _register_eager_task(task):\n    \"\"\"Register an asyncio Task about to be eagerly executed.\"\"\"\n    _eager_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _swap_current_task(loop, task):\n    prev_task = _current_tasks.get(loop)\n    if task is None:\n        del _current_tasks[loop]\n    else:\n        _current_tasks[loop] = task\n    return prev_task\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a completed, scheduled Task.\"\"\"\n    _scheduled_tasks.discard(task)\n\n\ndef _unregister_eager_task(task):\n    \"\"\"Unregister a task which finished its first eager step.\"\"\"\n    _eager_tasks.discard(task)\n\n\n_py_current_task = current_task\n_py_register_task = _register_task\n_py_register_eager_task = _register_eager_task\n_py_unregister_task = _unregister_task\n_py_unregister_eager_task = _unregister_eager_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n_py_swap_current_task = _swap_current_task\n\n\ntry:\n    from _asyncio import (_register_task, _register_eager_task,\n                          _unregister_task, _unregister_eager_task,\n                          _enter_task, _leave_task, _swap_current_task,\n                          _scheduled_tasks, _eager_tasks, _current_tasks,\n                          current_task)\nexcept ImportError:\n    pass\nelse:\n    _c_current_task = current_task\n    _c_register_task = _register_task\n    _c_register_eager_task = _register_eager_task\n    _c_unregister_task = _unregister_task\n    _c_unregister_eager_task = _unregister_eager_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n    _c_swap_current_task = _swap_current_task\n", 1118], "/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py": ["import asyncio\n\n\nasync def io_task():\n    await asyncio.sleep(0.01)\n\n\nasync def main():\n    t1 = asyncio.create_task(io_task())\n    t2 = asyncio.create_task(io_task())\n    t3 = asyncio.create_task(io_task())\n\n    await t1\n    await t2\n    await t3\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", 19]}, "functions": {"Runner.__init__ (/usr/lib/python3.13/asyncio/runners.py:48)": ["/usr/lib/python3.13/asyncio/runners.py", 48], "BaseDefaultEventLoopPolicy.__init__ (/usr/lib/python3.13/asyncio/events.py:685)": ["/usr/lib/python3.13/asyncio/events.py", 685], "_UnixDefaultEventLoopPolicy.__init__ (/usr/lib/python3.13/asyncio/unix_events.py:1481)": ["/usr/lib/python3.13/asyncio/unix_events.py", 1481], "_init_event_loop_policy (/usr/lib/python3.13/asyncio/events.py:789)": ["/usr/lib/python3.13/asyncio/events.py", 789], "get_event_loop_policy (/usr/lib/python3.13/asyncio/events.py:797)": ["/usr/lib/python3.13/asyncio/events.py", 797], "_is_debug_mode (/usr/lib/python3.13/asyncio/coroutines.py:10)": ["/usr/lib/python3.13/asyncio/coroutines.py", 10], "BaseEventLoop.is_running (/usr/lib/python3.13/asyncio/base_events.py:764)": ["/usr/lib/python3.13/asyncio/base_events.py", 764], "BaseEventLoop.set_debug (/usr/lib/python3.13/asyncio/base_events.py:2048)": ["/usr/lib/python3.13/asyncio/base_events.py", 2048], "WeakSet.__init__ (/usr/lib/python3.13/_weakrefset.py:37)": ["/usr/lib/python3.13/_weakrefset.py", 37], "BaseEventLoop.__init__ (/usr/lib/python3.13/asyncio/base_events.py:420)": ["/usr/lib/python3.13/asyncio/base_events.py", 420], "_SelectorMapping.__init__ (/usr/lib/python3.13/selectors.py:63)": ["/usr/lib/python3.13/selectors.py", 63], "_BaseSelectorImpl.__init__ (/usr/lib/python3.13/selectors.py:213)": ["/usr/lib/python3.13/selectors.py", 213], "_PollLikeSelector.__init__ (/usr/lib/python3.13/selectors.py:336)": ["/usr/lib/python3.13/selectors.py", 336], "Manager.disable (/usr/lib/python3.13/logging/__init__.py:1353)": ["/usr/lib/python3.13/logging/__init__.py", 1353], "Logger.getEffectiveLevel (/usr/lib/python3.13/logging/__init__.py:1750)": ["/usr/lib/python3.13/logging/__init__.py", 1750], "Logger.isEnabledFor (/usr/lib/python3.13/logging/__init__.py:1764)": ["/usr/lib/python3.13/logging/__init__.py", 1764], "Logger.debug (/usr/lib/python3.13/logging/__init__.py:1497)": ["/usr/lib/python3.13/logging/__init__.py", 1497], "socket.__init__ (/usr/lib/python3.13/socket.py:221)": ["/usr/lib/python3.13/socket.py", 221], "socketpair (/usr/lib/python3.13/socket.py:653)": ["/usr/lib/python3.13/socket.py", 653], "BaseEventLoop._check_closed (/usr/lib/python3.13/asyncio/base_events.py:550)": ["/usr/lib/python3.13/asyncio/base_events.py", 550], "BaseEventLoop.get_debug (/usr/lib/python3.13/asyncio/base_events.py:2045)": ["/usr/lib/python3.13/asyncio/base_events.py", 2045], "Handle.__init__ (/usr/lib/python3.13/asyncio/events.py:36)": ["/usr/lib/python3.13/asyncio/events.py", 36], "_BaseSelectorImpl.get_map (/usr/lib/python3.13/selectors.py:276)": ["/usr/lib/python3.13/selectors.py", 276], "_fileobj_to_fd (/usr/lib/python3.13/selectors.py:21)": ["/usr/lib/python3.13/selectors.py", 21], "_BaseSelectorImpl._fileobj_lookup (/usr/lib/python3.13/selectors.py:219)": ["/usr/lib/python3.13/selectors.py", 219], "_SelectorMapping.get (/usr/lib/python3.13/selectors.py:69)": ["/usr/lib/python3.13/selectors.py", 69], "_BaseSelectorImpl.register (/usr/lib/python3.13/selectors.py:238)": ["/usr/lib/python3.13/selectors.py", 238], "_PollLikeSelector.register (/usr/lib/python3.13/selectors.py:340)": ["/usr/lib/python3.13/selectors.py", 340], "BaseSelectorEventLoop._add_reader (/usr/lib/python3.13/asyncio/selector_events.py:274)": ["/usr/lib/python3.13/asyncio/selector_events.py", 274], "BaseSelectorEventLoop._make_self_pipe (/usr/lib/python3.13/asyncio/selector_events.py:118)": ["/usr/lib/python3.13/asyncio/selector_events.py", 118], "WeakValueDictionary.update (/usr/lib/python3.13/weakref.py:289)": ["/usr/lib/python3.13/weakref.py", 289], "WeakValueDictionary.__init__ (/usr/lib/python3.13/weakref.py:104)": ["/usr/lib/python3.13/weakref.py", 104], "BaseSelectorEventLoop.__init__ (/usr/lib/python3.13/asyncio/selector_events.py:59)": ["/usr/lib/python3.13/asyncio/selector_events.py", 59], "_UnixSelectorEventLoop.__init__ (/usr/lib/python3.13/asyncio/unix_events.py:64)": ["/usr/lib/python3.13/asyncio/unix_events.py", 64], "BaseDefaultEventLoopPolicy.new_event_loop (/usr/lib/python3.13/asyncio/events.py:728)": ["/usr/lib/python3.13/asyncio/events.py", 728], "new_event_loop (/usr/lib/python3.13/asyncio/events.py:835)": ["/usr/lib/python3.13/asyncio/events.py", 835], "BaseDefaultEventLoopPolicy.set_event_loop (/usr/lib/python3.13/asyncio/events.py:721)": ["/usr/lib/python3.13/asyncio/events.py", 721], "_UnixDefaultEventLoopPolicy.set_event_loop (/usr/lib/python3.13/asyncio/unix_events.py:1493)": ["/usr/lib/python3.13/asyncio/unix_events.py", 1493], "set_event_loop (/usr/lib/python3.13/asyncio/events.py:830)": ["/usr/lib/python3.13/asyncio/events.py", 830], "Runner._lazy_init (/usr/lib/python3.13/asyncio/runners.py:131)": ["/usr/lib/python3.13/asyncio/runners.py", 131], "Runner.__enter__ (/usr/lib/python3.13/asyncio/runners.py:57)": ["/usr/lib/python3.13/asyncio/runners.py", 57], "iscoroutine (/usr/lib/python3.13/asyncio/coroutines.py:32)": ["/usr/lib/python3.13/asyncio/coroutines.py", 32], "BaseEventLoop._call_soon (/usr/lib/python3.13/asyncio/base_events.py:848)": ["/usr/lib/python3.13/asyncio/base_events.py", 848], "BaseEventLoop.call_soon (/usr/lib/python3.13/asyncio/base_events.py:819)": ["/usr/lib/python3.13/asyncio/base_events.py", 819], "WeakSet.add (/usr/lib/python3.13/_weakrefset.py:85)": ["/usr/lib/python3.13/_weakrefset.py", 85], "BaseEventLoop.create_task (/usr/lib/python3.13/asyncio/base_events.py:462)": ["/usr/lib/python3.13/asyncio/base_events.py", 462], "current_thread (/usr/lib/python3.13/threading.py:1427)": ["/usr/lib/python3.13/threading.py", 1427], "main_thread (/usr/lib/python3.13/threading.py:1543)": ["/usr/lib/python3.13/threading.py", 1543], "_int_to_enum (/usr/lib/python3.13/signal.py:24)": ["/usr/lib/python3.13/signal.py", 24], "getsignal (/usr/lib/python3.13/signal.py:62)": ["/usr/lib/python3.13/signal.py", 62], "_enum_to_int (/usr/lib/python3.13/signal.py:36)": ["/usr/lib/python3.13/signal.py", 36], "signal (/usr/lib/python3.13/signal.py:56)": ["/usr/lib/python3.13/signal.py", 56], "BaseEventLoop._check_running (/usr/lib/python3.13/asyncio/base_events.py:631)": ["/usr/lib/python3.13/asyncio/base_events.py", 631], "isfuture (/usr/lib/python3.13/asyncio/base_futures.py:13)": ["/usr/lib/python3.13/asyncio/base_futures.py", 13], "_get_loop (/usr/lib/python3.13/asyncio/futures.py:300)": ["/usr/lib/python3.13/asyncio/futures.py", 300], "ensure_future (/usr/lib/python3.13/asyncio/tasks.py:723)": ["/usr/lib/python3.13/asyncio/tasks.py", 723], "BaseEventLoop._set_coroutine_origin_tracking (/usr/lib/python3.13/asyncio/base_events.py:2030)": ["/usr/lib/python3.13/asyncio/base_events.py", 2030], "BaseEventLoop._run_forever_setup (/usr/lib/python3.13/asyncio/base_events.py:638)": ["/usr/lib/python3.13/asyncio/base_events.py", 638], "EpollSelector.select (/usr/lib/python3.13/selectors.py:435)": ["/usr/lib/python3.13/selectors.py", 435], "BaseSelectorEventLoop._process_events (/usr/lib/python3.13/asyncio/selector_events.py:740)": ["/usr/lib/python3.13/asyncio/selector_events.py", 740], "BaseEventLoop.time (/usr/lib/python3.13/asyncio/base_events.py:768)": ["/usr/lib/python3.13/asyncio/base_events.py", 768], "create_task (/usr/lib/python3.13/asyncio/tasks.py:402)": ["/usr/lib/python3.13/asyncio/tasks.py", 402], "main (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:8)": ["/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py", 8], "Handle._run (/usr/lib/python3.13/asyncio/events.py:87)": ["/usr/lib/python3.13/asyncio/events.py", 87], "BaseEventLoop._run_once (/usr/lib/python3.13/asyncio/base_events.py:1947)": ["/usr/lib/python3.13/asyncio/base_events.py", 1947], "BaseEventLoop.create_future (/usr/lib/python3.13/asyncio/base_events.py:458)": ["/usr/lib/python3.13/asyncio/base_events.py", 458], "TimerHandle.__init__ (/usr/lib/python3.13/asyncio/events.py:113)": ["/usr/lib/python3.13/asyncio/events.py", 113], "BaseEventLoop.call_at (/usr/lib/python3.13/asyncio/base_events.py:801)": ["/usr/lib/python3.13/asyncio/base_events.py", 801], "BaseEventLoop.call_later (/usr/lib/python3.13/asyncio/base_events.py:777)": ["/usr/lib/python3.13/asyncio/base_events.py", 777], "sleep (/usr/lib/python3.13/asyncio/tasks.py:703)": ["/usr/lib/python3.13/asyncio/tasks.py", 703], "io_task (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:4)": ["/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py", 4], "TimerHandle.__lt__ (/usr/lib/python3.13/asyncio/events.py:129)": ["/usr/lib/python3.13/asyncio/events.py", 129], "_set_result_unless_cancelled (/usr/lib/python3.13/asyncio/futures.py:312)": ["/usr/lib/python3.13/asyncio/futures.py", 312], "BaseEventLoop._timer_handle_cancelled (/usr/lib/python3.13/asyncio/base_events.py:1942)": ["/usr/lib/python3.13/asyncio/base_events.py", 1942], "Handle.cancel (/usr/lib/python3.13/asyncio/events.py:73)": ["/usr/lib/python3.13/asyncio/events.py", 73], "TimerHandle.cancel (/usr/lib/python3.13/asyncio/events.py:157)": ["/usr/lib/python3.13/asyncio/events.py", 157], "BaseEventLoop.stop (/usr/lib/python3.13/asyncio/base_events.py:723)": ["/usr/lib/python3.13/asyncio/base_events.py", 723], "_run_until_complete_cb (/usr/lib/python3.13/asyncio/base_events.py:182)": ["/usr/lib/python3.13/asyncio/base_events.py", 182], "BaseEventLoop._run_forever_cleanup (/usr/lib/python3.13/asyncio/base_events.py:658)": ["/usr/lib/python3.13/asyncio/base_events.py", 658], "BaseEventLoop.run_forever (/usr/lib/python3.13/asyncio/base_events.py:674)": ["/usr/lib/python3.13/asyncio/base_events.py", 674], "BaseEventLoop.run_until_complete (/usr/lib/python3.13/asyncio/base_events.py:685)": ["/usr/lib/python3.13/asyncio/base_events.py", 685], "Runner.run (/usr/lib/python3.13/asyncio/runners.py:86)": ["/usr/lib/python3.13/asyncio/runners.py", 86], "WeakSet.__len__ (/usr/lib/python3.13/_weakrefset.py:72)": ["/usr/lib/python3.13/_weakrefset.py", 72], "_IterationGuard.__init__ (/usr/lib/python3.13/_weakrefset.py:17)": ["/usr/lib/python3.13/_weakrefset.py", 17], "_IterationGuard.__enter__ (/usr/lib/python3.13/_weakrefset.py:21)": ["/usr/lib/python3.13/_weakrefset.py", 21], "WeakSet.__iter__ (/usr/lib/python3.13/_weakrefset.py:63)": ["/usr/lib/python3.13/_weakrefset.py", 63], "WeakSet._commit_removals (/usr/lib/python3.13/_weakrefset.py:53)": ["/usr/lib/python3.13/_weakrefset.py", 53], "_IterationGuard.__exit__ (/usr/lib/python3.13/_weakrefset.py:27)": ["/usr/lib/python3.13/_weakrefset.py", 27], "all_tasks (/usr/lib/python3.13/asyncio/tasks.py:44)": ["/usr/lib/python3.13/asyncio/tasks.py", 44], "_cancel_all_tasks (/usr/lib/python3.13/asyncio/runners.py:197)": ["/usr/lib/python3.13/asyncio/runners.py", 197], "BaseEventLoop.shutdown_asyncgens (/usr/lib/python3.13/asyncio/base_events.py:572)": ["/usr/lib/python3.13/asyncio/base_events.py", 572], "BaseEventLoop.shutdown_default_executor (/usr/lib/python3.13/asyncio/base_events.py:597)": ["/usr/lib/python3.13/asyncio/base_events.py", 597], "BaseEventLoop.is_closed (/usr/lib/python3.13/asyncio/base_events.py:754)": ["/usr/lib/python3.13/asyncio/base_events.py", 754], "_BaseSelectorImpl.unregister (/usr/lib/python3.13/selectors.py:251)": ["/usr/lib/python3.13/selectors.py", 251], "_PollLikeSelector.unregister (/usr/lib/python3.13/selectors.py:351)": ["/usr/lib/python3.13/selectors.py", 351], "BaseSelectorEventLoop._remove_reader (/usr/lib/python3.13/asyncio/selector_events.py:289)": ["/usr/lib/python3.13/asyncio/selector_events.py", 289], "socket._real_close (/usr/lib/python3.13/socket.py:497)": ["/usr/lib/python3.13/socket.py", 497], "socket.close (/usr/lib/python3.13/socket.py:501)": ["/usr/lib/python3.13/socket.py", 501], "BaseSelectorEventLoop._close_self_pipe (/usr/lib/python3.13/asyncio/selector_events.py:110)": ["/usr/lib/python3.13/asyncio/selector_events.py", 110], "BaseEventLoop.close (/usr/lib/python3.13/asyncio/base_events.py:731)": ["/usr/lib/python3.13/asyncio/base_events.py", 731], "_BaseSelectorImpl.close (/usr/lib/python3.13/selectors.py:272)": ["/usr/lib/python3.13/selectors.py", 272], "EpollSelector.close (/usr/lib/python3.13/selectors.py:465)": ["/usr/lib/python3.13/selectors.py", 465], "BaseSelectorEventLoop.close (/usr/lib/python3.13/asyncio/selector_events.py:99)": ["/usr/lib/python3.13/asyncio/selector_events.py", 99], "_UnixSelectorEventLoop.close (/usr/lib/python3.13/asyncio/unix_events.py:69)": ["/usr/lib/python3.13/asyncio/unix_events.py", 69], "Runner.close (/usr/lib/python3.13/asyncio/runners.py:64)": ["/usr/lib/python3.13/asyncio/runners.py", 64], "Runner.__exit__ (/usr/lib/python3.13/asyncio/runners.py:61)": ["/usr/lib/python3.13/asyncio/runners.py", 61], "run (/usr/lib/python3.13/asyncio/runners.py:160)": ["/usr/lib/python3.13/asyncio/runners.py", 160], "<module> (/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py:1)": ["/home/gaogaotiantian/programs/viztracer/example/src/async_simple.py", 1]}}}